{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0b1d1371-e9f4-4dda-8dc6-e7882e362b20"
    }
   },
   "source": [
    "# Artificial Intelligence Nanodegree\n",
    "\n",
    "## Convolutional Neural Networks\n",
    "\n",
    "## Project: Write an Algorithm for a Dog Identification App \n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, some template code has already been provided for you, and you will need to implement additional functionality to successfully complete this project. You will not need to modify the included code beyond what is requested. Sections that begin with **'(IMPLEMENTATION)'** in the header indicate that the following block of code will require additional functionality which you must provide. Instructions will be provided for each section, and the specifics of the implementation are marked in the code block with a 'TODO' statement. Please be sure to read the instructions carefully! \n",
    "\n",
    "> **Note**: Once you have completed all of the code implementations, you need to finalize your work by exporting the iPython Notebook as an HTML document. Before exporting the notebook to html, all of the code cells need to have been run so that reviewers can see the final implementation and output. You can then export the notebook by using the menu above and navigating to  \\n\",\n",
    "    \"**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission.\n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question X'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut.  Markdown cells can be edited by double-clicking the cell to enter edit mode.\n",
    "\n",
    "The rubric contains _optional_ \"Stand Out Suggestions\" for enhancing the project beyond the minimum requirements. If you decide to pursue the \"Stand Out Suggestions\", you should include the code in this IPython notebook.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "### Why We're Here \n",
    "\n",
    "In this notebook, you will make the first steps towards developing an algorithm that could be used as part of a mobile or web app.  At the end of this project, your code will accept any user-supplied image as input.  If a dog is detected in the image, it will provide an estimate of the dog's breed.  If a human is detected, it will provide an estimate of the dog breed that is most resembling.  The image below displays potential sample output of your finished project (... but we expect that each student's algorithm will behave differently!). \n",
    "\n",
    "![Sample Dog Output](images/sample_dog_output.png)\n",
    "\n",
    "In this real-world setting, you will need to piece together a series of models to perform different tasks; for instance, the algorithm that detects humans in an image will be different from the CNN that infers dog breed.  There are many points of possible failure, and no perfect algorithm exists.  Your imperfect solution will nonetheless create a fun user experience!\n",
    "\n",
    "### The Road Ahead\n",
    "\n",
    "We break the notebook into separate steps.  Feel free to use the links below to navigate the notebook.\n",
    "\n",
    "* [Step 0](#step0): Import Datasets\n",
    "* [Step 1](#step1): Detect Humans\n",
    "* [Step 2](#step2): Detect Dogs\n",
    "* [Step 3](#step3): Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "* [Step 4](#step4): Use a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "* [Step 5](#step5): Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "* [Step 6](#step6): Write your Algorithm\n",
    "* [Step 7](#step7): Test Your Algorithm\n",
    "\n",
    "---\n",
    "<a id='step0'></a>\n",
    "## Step 0: Import Datasets\n",
    "\n",
    "### Import Dog Dataset\n",
    "\n",
    "In the code cell below, we import a dataset of dog images.  We populate a few variables through the use of the `load_files` function from the scikit-learn library:\n",
    "- `train_files`, `valid_files`, `test_files` - numpy arrays containing file paths to images\n",
    "- `train_targets`, `valid_targets`, `test_targets` - numpy arrays containing onehot-encoded classification labels \n",
    "- `dog_names` - list of string-valued dog breed names for translating labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "be414f28-6d27-4e52-8786-1d0d51575532"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 133 total dog categories.\n",
      "There are 8351 total dog images.\n",
      "\n",
      "There are 6680 training dog images.\n",
      "There are 835 validation dog images.\n",
      "There are 836 test dog images.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "    return dog_files, dog_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('dogImages/train')\n",
    "valid_files, valid_targets = load_dataset('dogImages/valid')\n",
    "test_files, test_targets = load_dataset('dogImages/test')\n",
    "\n",
    "# load list of dog names\n",
    "dog_names = [item[20:-1] for item in sorted(glob(\"dogImages/train/*/\"))]\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total dog categories.' % len(dog_names))\n",
    "print('There are %s total dog images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training dog images.' % len(train_files))\n",
    "print('There are %d validation dog images.' % len(valid_files))\n",
    "print('There are %d test dog images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5f2f0863-3365-4d0f-b188-bfb017d7c9c2"
    }
   },
   "source": [
    "### Import Human Dataset\n",
    "\n",
    "In the code cell below, we import a dataset of human images, where the file paths are stored in the numpy array `human_files`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "aa49ee33-9985-43e4-b7bd-e502ac28ab64"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13233 total human images.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(8675309)\n",
    "\n",
    "# load filenames in shuffled human dataset\n",
    "human_files = np.array(glob(\"lfw/lfw/*/*\"))\n",
    "random.shuffle(human_files)\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total human images.' % len(human_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "9e2f1553-8581-4fac-9402-ce0510458c7d"
    }
   },
   "source": [
    "---\n",
    "<a id='step1'></a>\n",
    "## Step 1: Detect Humans\n",
    "\n",
    "We use OpenCV's implementation of [Haar feature-based cascade classifiers](http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html) to detect human faces in images.  OpenCV provides many pre-trained face detectors, stored as XML files on [github](https://github.com/opencv/opencv/tree/master/data/haarcascades).  We have downloaded one of these detectors and stored it in the `haarcascades` directory.\n",
    "\n",
    "In the next code cell, we demonstrate how to use this detector to find human faces in a sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "fd8fd949-ab08-4b19-83e4-5058d6494b44"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of faces detected: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQsAAAD8CAYAAABgtYFHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8b68591e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2                \n",
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline                               \n",
    "\n",
    "# extract pre-trained face detector\n",
    "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt.xml')\n",
    "\n",
    "# load color (BGR) image\n",
    "img = cv2.imread(human_files[3])\n",
    "# convert BGR image to grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# find faces in image\n",
    "faces = face_cascade.detectMultiScale(gray)\n",
    "\n",
    "# print number of faces detected in the image\n",
    "print('Number of faces detected:', len(faces))\n",
    "\n",
    "# get bounding box for each detected face\n",
    "for (x,y,w,h) in faces:\n",
    "    # add bounding box to color image\n",
    "    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    \n",
    "# convert BGR image to RGB for plotting\n",
    "cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# display the image, along with bounding box\n",
    "plt.imshow(cv_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "82321456-bdf7-429f-87a5-80bbb875ed4b"
    }
   },
   "source": [
    "Before using any of the face detectors, it is standard procedure to convert the images to grayscale.  The `detectMultiScale` function executes the classifier stored in `face_cascade` and takes the grayscale image as a parameter.  \n",
    "\n",
    "In the above code, `faces` is a numpy array of detected faces, where each row corresponds to a detected face.  Each detected face is a 1D array with four entries that specifies the bounding box of the detected face.  The first two entries in the array (extracted in the above code as `x` and `y`) specify the horizontal and vertical positions of the top left corner of the bounding box.  The last two entries in the array (extracted here as `w` and `h`) specify the width and height of the box.\n",
    "\n",
    "### Write a Human Face Detector\n",
    "\n",
    "We can use this procedure to write a function that returns `True` if a human face is detected in an image and `False` otherwise.  This function, aptly named `face_detector`, takes a string-valued file path to an image as input and appears in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "9785470a-3226-4b28-bb31-b408e246f471"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# returns \"True\" if face is detected in image stored at img_path\n",
    "def face_detector(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray)\n",
    "    return len(faces) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "922e2623-1236-4d28-8bdc-df63b756ab9a"
    }
   },
   "source": [
    "### (IMPLEMENTATION) Assess the Human Face Detector\n",
    "\n",
    "__Question 1:__ Use the code cell below to test the performance of the `face_detector` function.  \n",
    "- What percentage of the first 100 images in `human_files` have a detected human face?  \n",
    "- What percentage of the first 100 images in `dog_files` have a detected human face? \n",
    "\n",
    "Ideally, we would like 100% of human images with a detected face and 0% of dog images with a detected face.  You will see that our algorithm falls short of this goal, but still gives acceptable performance.  We extract the file paths for the first 100 images from each of the datasets and store them in the numpy arrays `human_files_short` and `dog_files_short`.\n",
    "\n",
    "__Answer:__ \n",
    "99% of the human images have detected a human face, and 11% of the dog images have a detected human face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbpresent": {
     "id": "9d7fcb7a-fc73-48e1-825a-ebd5af3ed167"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 human faces were detected in the first 100 human images.\n",
      "11 human faces were detected in the first 100 dog images.\n"
     ]
    }
   ],
   "source": [
    "human_files_short = human_files[:100]\n",
    "dog_files_short = train_files[:100]\n",
    "# Do NOT modify the code above this line.\n",
    "\n",
    "## TODO: Test the performance of the face_detector algorithm \n",
    "## on the images in human_files_short and dog_files_short.\n",
    "num_human_faces = 0\n",
    "num_dog_faces = 0\n",
    "\n",
    "for x in human_files_short:\n",
    "    if face_detector(x):\n",
    "        num_human_faces += 1\n",
    "        \n",
    "for x in dog_files_short:\n",
    "    if face_detector(x):\n",
    "        num_dog_faces += 1\n",
    "\n",
    "print('%d human faces were detected in the first 100 human images.' % num_human_faces)\n",
    "print('%d human faces were detected in the first 100 dog images.' % num_dog_faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "197df124-a496-4dc9-a10b-80de67d20ded"
    }
   },
   "source": [
    "__Question 2:__ This algorithmic choice necessitates that we communicate to the user that we accept human images only when they provide a clear view of a face (otherwise, we risk having unneccessarily frustrated users!). In your opinion, is this a reasonable expectation to pose on the user? If not, can you think of a way to detect humans in images that does not necessitate an image with a clearly presented face?\n",
    "\n",
    "__Answer:__\n",
    "In my opinion, accepting human images only when they contain a clear view of a face is a reasonable expectation for this type of application. Most people looking to use an app like this are taking shots straight on with a phone, similar to taking a selfie or using a program like Skype. I think people are already used to using apps that have this type of requirement. If it did turn into an issue though, the same type of deep learning network that is being trained in this project to detect dogs could be used to create a pipeline for detecting humans without a clear image of their face.\n",
    "\n",
    "We suggest the face detector from OpenCV as a potential way to detect human images in your algorithm, but you are free to explore other approaches, especially approaches that make use of deep learning :).  Please use the code cell below to design and test your own face detection algorithm.  If you decide to pursue this _optional_ task, report performance on each of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "95ca45c9-4659-4be3-9fc7-a674b91ae7db"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## (Optional) TODO: Report the performance of another  \n",
    "## face detection algorithm on the LFW dataset\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a7e03bb9-f910-403a-89be-ccb580273907"
    }
   },
   "source": [
    "---\n",
    "<a id='step2'></a>\n",
    "## Step 2: Detect Dogs\n",
    "\n",
    "In this section, we use a pre-trained [ResNet-50](http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006) model to detect dogs in images.  Our first line of code downloads the ResNet-50 model, along with weights that have been trained on [ImageNet](http://www.image-net.org/), a very large, very popular dataset used for image classification and other vision tasks.  ImageNet contains over 10 million URLs, each linking to an image containing an object from one of [1000 categories](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a).  Given an image, this pre-trained ResNet-50 model returns a prediction (derived from the available categories in ImageNet) for the object that is contained in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "8a856d1b-97f7-42a3-8f4b-fb1020d47ef0"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "# define ResNet50 model\n",
    "ResNet50_model = ResNet50(weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6a5d04a8-c85c-4573-8da8-1671348f5182"
    }
   },
   "source": [
    "### Pre-process the Data\n",
    "\n",
    "When using TensorFlow as backend, Keras CNNs require a 4D array (which we'll also refer to as a 4D tensor) as input, with shape\n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, \\text{rows}, \\text{columns}, \\text{channels}),\n",
    "$$\n",
    "\n",
    "where `nb_samples` corresponds to the total number of images (or samples), and `rows`, `columns`, and `channels` correspond to the number of rows, columns, and channels for each image, respectively.  \n",
    "\n",
    "The `path_to_tensor` function below takes a string-valued file path to a color image as input and returns a 4D tensor suitable for supplying to a Keras CNN.  The function first loads the image and resizes it to a square image that is $224 \\times 224$ pixels.  Next, the image is converted to an array, which is then resized to a 4D tensor.  In this case, since we are working with color images, each image has three channels.  Likewise, since we are processing a single image (or sample), the returned tensor will always have shape\n",
    "\n",
    "$$\n",
    "(1, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "The `paths_to_tensor` function takes a numpy array of string-valued image paths as input and returns a 4D tensor with shape \n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "Here, `nb_samples` is the number of samples, or number of images, in the supplied array of image paths.  It is best to think of `nb_samples` as the number of 3D tensors (where each 3D tensor corresponds to a different image) in your dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "0d5079c3-d8b4-47c0-86ea-2f30433c2556"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2f21f4bf-ca21-41b5-b2e8-177d2c85a319"
    }
   },
   "source": [
    "### Making Predictions with ResNet-50\n",
    "\n",
    "Getting the 4D tensor ready for ResNet-50, and for any other pre-trained model in Keras, requires some additional processing.  First, the RGB image is converted to BGR by reordering the channels.  All pre-trained models have the additional normalization step that the mean pixel (expressed in RGB as $[103.939, 116.779, 123.68]$ and calculated from all pixels in all images in ImageNet) must be subtracted from every pixel in each image.  This is implemented in the imported function `preprocess_input`.  If you're curious, you can check the code for `preprocess_input` [here](https://github.com/fchollet/keras/blob/master/keras/applications/imagenet_utils.py).\n",
    "\n",
    "Now that we have a way to format our image for supplying to ResNet-50, we are now ready to use the model to extract the predictions.  This is accomplished with the `predict` method, which returns an array whose $i$-th entry is the model's predicted probability that the image belongs to the $i$-th ImageNet category.  This is implemented in the `ResNet50_predict_labels` function below.\n",
    "\n",
    "By taking the argmax of the predicted probability vector, we obtain an integer corresponding to the model's predicted object class, which we can identify with an object category through the use of this [dictionary](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "b0ec57f9-5d06-4e12-b598-3366a1bd4a53"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "def ResNet50_predict_labels(img_path):\n",
    "    # returns prediction vector for image located at img_path\n",
    "    img = preprocess_input(path_to_tensor(img_path))\n",
    "    return np.argmax(ResNet50_model.predict(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "11f53b71-e368-45e5-a900-fd3eb2e13587"
    }
   },
   "source": [
    "### Write a Dog Detector\n",
    "\n",
    "While looking at the [dictionary](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a), you will notice that the categories corresponding to dogs appear in an uninterrupted sequence and correspond to dictionary keys 151-268, inclusive, to include all categories from `'Chihuahua'` to `'Mexican hairless'`.  Thus, in order to check to see if an image is predicted to contain a dog by the pre-trained ResNet-50 model, we need only check if the `ResNet50_predict_labels` function above returns a value between 151 and 268 (inclusive).\n",
    "\n",
    "We use these ideas to complete the `dog_detector` function below, which returns `True` if a dog is detected in an image (and `False` if not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "b17f501c-85c8-48c1-8bb1-01673ef453de"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### returns \"True\" if a dog is detected in the image stored at img_path\n",
    "def dog_detector(img_path):\n",
    "    prediction = ResNet50_predict_labels(img_path)\n",
    "    return ((prediction <= 268) & (prediction >= 151)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "14326f0d-2f7f-4ca0-9609-37d3d1311666"
    }
   },
   "source": [
    "### (IMPLEMENTATION) Assess the Dog Detector\n",
    "\n",
    "__Question 3:__ Use the code cell below to test the performance of your `dog_detector` function.  \n",
    "- What percentage of the images in `human_files_short` have a detected dog?  \n",
    "- What percentage of the images in `dog_files_short` have a detected dog?\n",
    "\n",
    "__Answer:__ \n",
    "0% of the first 100 human files have a detected dog. 100% of the first 100 dog files had a detected dog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nbpresent": {
     "id": "40bfc0ac-b4b1-43ff-8b6f-c4094c24b437"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 dogs were detected in the first 100 human images.\n",
      "100 dogs were detected in the first 100 dog images.\n"
     ]
    }
   ],
   "source": [
    "### TODO: Test the performance of the dog_detector function\n",
    "### on the images in human_files_short and dog_files_short.\n",
    "num_human_faces = 0\n",
    "num_dog_faces = 0\n",
    "\n",
    "for x in human_files_short:\n",
    "    if dog_detector(x):\n",
    "        num_human_faces += 1\n",
    "        \n",
    "for x in dog_files_short:\n",
    "    if dog_detector(x):\n",
    "        num_dog_faces += 1\n",
    "        \n",
    "print('%d dogs were detected in the first 100 human images.' % num_human_faces)\n",
    "print('%d dogs were detected in the first 100 dog images.' % num_dog_faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c1ae29c7-3b8b-47e1-b6cc-b3855b660fc0"
    }
   },
   "source": [
    "---\n",
    "<a id='step3'></a>\n",
    "## Step 3: Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "\n",
    "Now that we have functions for detecting humans and dogs in images, we need a way to predict breed from images.  In this step, you will create a CNN that classifies dog breeds.  You must create your CNN _from scratch_ (so, you can't use transfer learning _yet_!), and you must attain a test accuracy of at least 1%.  In Step 5 of this notebook, you will have the opportunity to use transfer learning to create a CNN that attains greatly improved accuracy.\n",
    "\n",
    "Be careful with adding too many trainable layers!  More parameters means longer training, which means you are more likely to need a GPU to accelerate the training process.  Thankfully, Keras provides a handy estimate of the time that each epoch is likely to take; you can extrapolate this estimate to figure out how long it will take for your algorithm to train. \n",
    "\n",
    "We mention that the task of assigning breed to dogs from images is considered exceptionally challenging.  To see why, consider that *even a human* would have great difficulty in distinguishing between a Brittany and a Welsh Springer Spaniel.  \n",
    "\n",
    "Brittany | Welsh Springer Spaniel\n",
    "- | - \n",
    "<img src=\"images/Brittany_02625.jpg\" width=\"100\"> | <img src=\"images/Welsh_springer_spaniel_08203.jpg\" width=\"200\">\n",
    "\n",
    "It is not difficult to find other dog breed pairs with minimal inter-class variation (for instance, Curly-Coated Retrievers and American Water Spaniels).  \n",
    "\n",
    "Curly-Coated Retriever | American Water Spaniel\n",
    "- | -\n",
    "<img src=\"images/Curly-coated_retriever_03896.jpg\" width=\"200\"> | <img src=\"images/American_water_spaniel_00648.jpg\" width=\"200\">\n",
    "\n",
    "\n",
    "Likewise, recall that labradors come in yellow, chocolate, and black.  Your vision-based algorithm will have to conquer this high intra-class variation to determine how to classify all of these different shades as the same breed.  \n",
    "\n",
    "Yellow Labrador | Chocolate Labrador | Black Labrador\n",
    "- | -\n",
    "<img src=\"images/Labrador_retriever_06457.jpg\" width=\"150\"> | <img src=\"images/Labrador_retriever_06455.jpg\" width=\"240\"> | <img src=\"images/Labrador_retriever_06449.jpg\" width=\"220\">\n",
    "\n",
    "We also mention that random chance presents an exceptionally low bar: setting aside the fact that the classes are slightly imabalanced, a random guess will provide a correct answer roughly 1 in 133 times, which corresponds to an accuracy of less than 1%.  \n",
    "\n",
    "Remember that the practice is far ahead of the theory in deep learning.  Experiment with many different architectures, and trust your intuition.  And, of course, have fun! \n",
    "\n",
    "### Pre-process the Data\n",
    "\n",
    "We rescale the images by dividing every pixel in every image by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "nbpresent": {
     "id": "5aded55d-a7be-4955-9622-3a8c84f13340"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6680/6680 [00:54<00:00, 123.12it/s]\n",
      "100%|██████████| 835/835 [00:06<00:00, 137.52it/s]\n",
      "100%|██████████| 836/836 [00:06<00:00, 138.20it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "76dd4630-e672-44fc-b2c9-694de96229be"
    }
   },
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Create a CNN to classify dog breed.  At the end of your code cell block, summarize the layers of your model by executing the line:\n",
    "    \n",
    "        model.summary()\n",
    "\n",
    "We have imported some Python modules to get you started, but feel free to import as many modules as you need.  If you end up getting stuck, here's a hint that specifies a model that trains relatively fast on CPU and attains >1% test accuracy in 5 epochs:\n",
    "\n",
    "![Sample CNN](images/sample_cnn.png)\n",
    "           \n",
    "__Question 4:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  If you chose to use the hinted architecture above, describe why you think that CNN architecture should work well for the image classification task.\n",
    "\n",
    "__Answer:__\n",
    "\n",
    "I tried the base model that is recommended in the image above and got approximately 2.5% accuracy, so I decided to experiment with other methods.\n",
    "\n",
    "Based on the lecture information and research into common CCN architectures, I found that a repeated set of convolution and pooling followed by fully connected layers is the recommended layering pattern. I choose to do three blocks of CONV RELU POOl with the last CONV layer having 2 additional pooling layers in order to reduce the dimensionality of the data.\n",
    "\n",
    "My choice of parameters in the CONV layers was aimed at trying to keep the training time within a realistic range, so I started with 16 filters and doubled it for each CONV layer. 2x2 and 3x3 sized filters are the most common in the research I read, so I selected the 3x3 arbitrarily. Padding='same' was chosen so that the output size of each layer wouldn't change.\n",
    "\n",
    "After the CONV blocks, the data is sent to a Flattening layer (which was recommended in the keras blog to reduce the data from 3-d feature maps to 1-d feature vectors) and then passed to a dense layer with 50% dropout and a final dense layer for class prediction. The dropout was chosen to reduce overfitting. The number of nodes on the first dense layer was set mostly to speed up processing time.\n",
    "\n",
    "This architecture achieves approximately 13% accuracy with 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "nbpresent": {
     "id": "e3421c6c-f71c-4a21-9c46-2e1485e97d36"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 16)      448       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 224, 224, 16)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 112, 112, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 112, 112, 32)      4640      \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 56, 56, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 56, 56, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 133)               68229     \n",
      "=================================================================\n",
      "Total params: 1,697,957.0\n",
      "Trainable params: 1,697,957.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense, Activation\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "### TODO: Define your architecture.\n",
    "\n",
    "model.add(Conv2D(16, (3,3), padding='same', input_shape=(224, 224, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(32, (3,3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(64, (3,3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d1cab638-b77b-4d26-ac61-2d085f8b7d9b"
    }
   },
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "96880e8c-d72e-49f9-8cde-132a57731919"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5b6391e4-183c-4e6f-b556-073ea09dadf7"
    }
   },
   "source": [
    "### (IMPLEMENTATION) Train the Model\n",
    "\n",
    "Train your model in the code cell below.  Use model checkpointing to save the model that attains the best validation loss.\n",
    "\n",
    "You are welcome to [augment the training data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), but this is not a requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "nbpresent": {
     "id": "8a6b0228-99a0-4c33-b0ce-535c5b56102d"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/5\n",
      "6660/6680 [============================>.] - ETA: 1s - loss: 4.8387 - acc: 0.0141Epoch 00000: val_loss improved from inf to 4.69595, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 371s - loss: 4.8382 - acc: 0.0141 - val_loss: 4.6959 - val_acc: 0.0240\n",
      "Epoch 2/5\n",
      "6660/6680 [============================>.] - ETA: 1s - loss: 4.4943 - acc: 0.0459Epoch 00001: val_loss improved from 4.69595 to 4.27469, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 372s - loss: 4.4928 - acc: 0.0461 - val_loss: 4.2747 - val_acc: 0.0539\n",
      "Epoch 3/5\n",
      "6660/6680 [============================>.] - ETA: 1s - loss: 4.1409 - acc: 0.0754Epoch 00002: val_loss improved from 4.27469 to 4.05896, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 374s - loss: 4.1404 - acc: 0.0753 - val_loss: 4.0590 - val_acc: 0.0934\n",
      "Epoch 4/5\n",
      "6660/6680 [============================>.] - ETA: 1s - loss: 3.8945 - acc: 0.1078Epoch 00003: val_loss improved from 4.05896 to 3.95080, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 375s - loss: 3.8950 - acc: 0.1078 - val_loss: 3.9508 - val_acc: 0.1090\n",
      "Epoch 5/5\n",
      "6660/6680 [============================>.] - ETA: 1s - loss: 3.6467 - acc: 0.1498Epoch 00004: val_loss improved from 3.95080 to 3.92367, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 375s - loss: 3.6462 - acc: 0.1500 - val_loss: 3.9237 - val_acc: 0.1090\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8af185cc18>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "### TODO: specify the number of epochs that you would like to use to train the model.\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "### Do NOT modify the code below this line.\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=epochs, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "298d0910-21a9-4261-962c-ee93bb6ac592"
    }
   },
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "6a23eac4-242b-4e8e-9d94-ef7fe32824cb"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.load_weights('saved_models/weights.best.from_scratch.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1d337b93-57f5-498c-8ba2-fc350be7eae8"
    }
   },
   "source": [
    "### Test the Model\n",
    "\n",
    "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "nbpresent": {
     "id": "617cc09a-6a6f-4746-8e28-d9c627135a64"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 13.2775%\n"
     ]
    }
   ],
   "source": [
    "# get index of predicted dog breed for each image in test set\n",
    "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a0e528df-c5d3-4f4f-ac21-22c6cd102e56"
    }
   },
   "source": [
    "---\n",
    "<a id='step4'></a>\n",
    "## Step 4: Use a CNN to Classify Dog Breeds\n",
    "\n",
    "To reduce training time without sacrificing accuracy, we show you how to train a CNN using transfer learning.  In the following step, you will get a chance to use transfer learning to train your own CNN.\n",
    "\n",
    "### Obtain Bottleneck Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "c71e9ce4-4a39-4979-b695-dfbcf05e465a"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bottleneck_features = np.load('bottleneck_features/DogVGG16Data.npz')\n",
    "train_VGG16 = bottleneck_features['train']\n",
    "valid_VGG16 = bottleneck_features['valid']\n",
    "test_VGG16 = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4dcd32d1-22c0-4c4c-a175-18dbd73de3ca"
    }
   },
   "source": [
    "### Model Architecture\n",
    "\n",
    "The model uses the the pre-trained VGG-16 model as a fixed feature extractor, where the last convolutional output of VGG-16 is fed as input to our model.  We only add a global average pooling layer and a fully connected layer, where the latter contains one node for each dog category and is equipped with a softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "nbpresent": {
     "id": "6369770f-aac6-4921-80e6-947191449268"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_1 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 133)               68229     \n",
      "=================================================================\n",
      "Total params: 68,229.0\n",
      "Trainable params: 68,229.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "VGG16_model = Sequential()\n",
    "VGG16_model.add(GlobalAveragePooling2D(input_shape=train_VGG16.shape[1:]))\n",
    "VGG16_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "VGG16_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c792a73e-81cc-475e-be24-8f429a75b5b3"
    }
   },
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "d6ff3093-83d7-4596-a317-0eb378733877"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "VGG16_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "68a866d5-b1dd-4fd1-a515-1fcf15c15172"
    }
   },
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "nbpresent": {
     "id": "f9d05385-5750-493f-a8a6-676a2a2a29f5"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      "6400/6680 [===========================>..] - ETA: 0s - loss: 11.9445 - acc: 0.1336Epoch 00000: val_loss improved from inf to 10.46697, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 11.8859 - acc: 0.1371 - val_loss: 10.4670 - val_acc: 0.2347\n",
      "Epoch 2/20\n",
      "6420/6680 [===========================>..] - ETA: 0s - loss: 9.9581 - acc: 0.2958Epoch 00001: val_loss improved from 10.46697 to 10.04209, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 9.9733 - acc: 0.2960 - val_loss: 10.0421 - val_acc: 0.2970\n",
      "Epoch 3/20\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 9.5659 - acc: 0.3506Epoch 00002: val_loss improved from 10.04209 to 9.89794, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 9.5755 - acc: 0.3501 - val_loss: 9.8979 - val_acc: 0.3150\n",
      "Epoch 4/20\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 9.3682 - acc: 0.3767Epoch 00003: val_loss improved from 9.89794 to 9.72437, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 9.3525 - acc: 0.3777 - val_loss: 9.7244 - val_acc: 0.3210\n",
      "Epoch 5/20\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 9.2127 - acc: 0.3969Epoch 00004: val_loss improved from 9.72437 to 9.61567, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 9.2015 - acc: 0.3975 - val_loss: 9.6157 - val_acc: 0.3437\n",
      "Epoch 6/20\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 9.0432 - acc: 0.4134Epoch 00005: val_loss improved from 9.61567 to 9.58766, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 9.0557 - acc: 0.4130 - val_loss: 9.5877 - val_acc: 0.3485\n",
      "Epoch 7/20\n",
      "6480/6680 [============================>.] - ETA: 0s - loss: 8.9906 - acc: 0.4247Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 8.9870 - acc: 0.4251 - val_loss: 9.5931 - val_acc: 0.3449\n",
      "Epoch 8/20\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 8.8968 - acc: 0.4326Epoch 00007: val_loss improved from 9.58766 to 9.47255, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 8.9255 - acc: 0.4310 - val_loss: 9.4726 - val_acc: 0.3485\n",
      "Epoch 9/20\n",
      "6440/6680 [===========================>..] - ETA: 0s - loss: 8.8072 - acc: 0.4411Epoch 00008: val_loss improved from 9.47255 to 9.40026, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 8.8183 - acc: 0.4404 - val_loss: 9.4003 - val_acc: 0.3641\n",
      "Epoch 10/20\n",
      "6540/6680 [============================>.] - ETA: 0s - loss: 8.7804 - acc: 0.4454Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 8.7802 - acc: 0.4454 - val_loss: 9.4708 - val_acc: 0.3593\n",
      "Epoch 11/20\n",
      "6580/6680 [============================>.] - ETA: 0s - loss: 8.6742 - acc: 0.4468Epoch 00010: val_loss improved from 9.40026 to 9.32106, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 8.6633 - acc: 0.4472 - val_loss: 9.3211 - val_acc: 0.3605\n",
      "Epoch 12/20\n",
      "6560/6680 [============================>.] - ETA: 0s - loss: 8.4953 - acc: 0.4623Epoch 00011: val_loss improved from 9.32106 to 9.15668, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 8.4941 - acc: 0.4624 - val_loss: 9.1567 - val_acc: 0.3749\n",
      "Epoch 13/20\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 8.4277 - acc: 0.4686Epoch 00012: val_loss improved from 9.15668 to 9.12534, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 8.4480 - acc: 0.4674 - val_loss: 9.1253 - val_acc: 0.3749\n",
      "Epoch 14/20\n",
      "6520/6680 [============================>.] - ETA: 0s - loss: 8.3472 - acc: 0.4727Epoch 00013: val_loss improved from 9.12534 to 8.92231, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 8.3307 - acc: 0.4737 - val_loss: 8.9223 - val_acc: 0.3892\n",
      "Epoch 15/20\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 8.2496 - acc: 0.4796Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 8.2601 - acc: 0.4789 - val_loss: 9.0062 - val_acc: 0.3784\n",
      "Epoch 16/20\n",
      "6540/6680 [============================>.] - ETA: 0s - loss: 8.1269 - acc: 0.4818Epoch 00015: val_loss improved from 8.92231 to 8.84494, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 8.1414 - acc: 0.4808 - val_loss: 8.8449 - val_acc: 0.3904\n",
      "Epoch 17/20\n",
      "6440/6680 [===========================>..] - ETA: 0s - loss: 8.0570 - acc: 0.4908Epoch 00016: val_loss improved from 8.84494 to 8.79727, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 8.0411 - acc: 0.4918 - val_loss: 8.7973 - val_acc: 0.3844\n",
      "Epoch 18/20\n",
      "6520/6680 [============================>.] - ETA: 0s - loss: 7.9938 - acc: 0.4963Epoch 00017: val_loss improved from 8.79727 to 8.79224, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 8.0195 - acc: 0.4949 - val_loss: 8.7922 - val_acc: 0.3916\n",
      "Epoch 19/20\n",
      "6560/6680 [============================>.] - ETA: 0s - loss: 8.0135 - acc: 0.4968Epoch 00018: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 8.0032 - acc: 0.4975 - val_loss: 8.9006 - val_acc: 0.3904\n",
      "Epoch 20/20\n",
      "6540/6680 [============================>.] - ETA: 0s - loss: 7.9349 - acc: 0.4995Epoch 00019: val_loss improved from 8.79224 to 8.66626, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 7.9254 - acc: 0.4999 - val_loss: 8.6663 - val_acc: 0.3988\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8af05f7c88>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.VGG16.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "VGG16_model.fit(train_VGG16, train_targets, \n",
    "          validation_data=(valid_VGG16, valid_targets),\n",
    "          epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "8ce78060-7823-4180-b1de-7480070928cc"
    }
   },
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "93e6e66b-1c4d-4078-b5b5-5f20c4a6d696"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "VGG16_model.load_weights('saved_models/weights.best.VGG16.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5a55d640-1bda-43cb-926c-3cfcd7c461c4"
    }
   },
   "source": [
    "### Test the Model\n",
    "\n",
    "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "nbpresent": {
     "id": "4a10d074-285c-41c8-9878-4b0ddba05d03"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 41.8660%\n"
     ]
    }
   ],
   "source": [
    "# get index of predicted dog breed for each image in test set\n",
    "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "29c791f6-bf10-4a84-9230-8e73ce1e5d56"
    }
   },
   "source": [
    "### Predict Dog Breed with the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "9e57ec93-eaa0-4f5a-803f-fb2c3cc34496"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from extract_bottleneck_features import *\n",
    "\n",
    "def VGG16_predict_breed(img_path):\n",
    "    # extract bottleneck features\n",
    "    bottleneck_feature = extract_VGG16(path_to_tensor(img_path))\n",
    "    # obtain predicted vector\n",
    "    predicted_vector = VGG16_model.predict(bottleneck_feature)\n",
    "    # return dog breed that is predicted by the model\n",
    "    return dog_names[np.argmax(predicted_vector)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "285d10bf-cef5-4093-9e21-808a09f3fceb"
    }
   },
   "source": [
    "---\n",
    "<a id='step5'></a>\n",
    "## Step 5: Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "\n",
    "You will now use transfer learning to create a CNN that can identify dog breed from images.  Your CNN must attain at least 60% accuracy on the test set.\n",
    "\n",
    "In Step 4, we used transfer learning to create a CNN using VGG-16 bottleneck features.  In this section, you must use the bottleneck features from a different pre-trained model.  To make things easier for you, we have pre-computed the features for all of the networks that are currently available in Keras:\n",
    "- [VGG-19](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogVGG19Data.npz) bottleneck features\n",
    "- [ResNet-50](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogResnet50Data.npz) bottleneck features\n",
    "- [Inception](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogInceptionV3Data.npz) bottleneck features\n",
    "- [Xception](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogXceptionData.npz) bottleneck features\n",
    "\n",
    "The files are encoded as such:\n",
    "\n",
    "    Dog{network}Data.npz\n",
    "    \n",
    "where `{network}`, in the above filename, can be one of `VGG19`, `Resnet50`, `InceptionV3`, or `Xception`.  Pick one of the above architectures, download the corresponding bottleneck features, and store the downloaded file in the `bottleneck_features/` folder in the repository.\n",
    "\n",
    "### (IMPLEMENTATION) Obtain Bottleneck Features\n",
    "\n",
    "In the code block below, extract the bottleneck features corresponding to the train, test, and validation sets by running the following:\n",
    "\n",
    "    bottleneck_features = np.load('bottleneck_features/Dog{network}Data.npz')\n",
    "    train_{network} = bottleneck_features['train']\n",
    "    valid_{network} = bottleneck_features['valid']\n",
    "    test_{network} = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "efa4dcdf-25c5-4f39-b482-2b20c04bd396"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### TODO: Obtain bottleneck features from another pre-trained CNN.\n",
    "bottleneck_features = np.load('bottleneck_features/DogVGG19Data.npz')\n",
    "train_VGG19 = bottleneck_features['train']\n",
    "valid_VGG19 = bottleneck_features['valid']\n",
    "test_VGG19 = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "69c0df3d-d0f3-4020-8c67-40a24bd8e97b"
    }
   },
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Create a CNN to classify dog breed.  At the end of your code cell block, summarize the layers of your model by executing the line:\n",
    "    \n",
    "        <your model's name>.summary()\n",
    "   \n",
    "__Question 5:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  Describe why you think the architecture is suitable for the current problem.\n",
    "\n",
    "__Answer:__ \n",
    "\n",
    "I used a lot of trial and error to select the transfer model and the layers and parameters for my final model. I tried the VGG19, Resnet50, and Inception models, all of which feed into my first layer, which is a GAP2D layer. I also tried a flatten layer for the input, but that scored lower than the GAP, so I reverted back to the GAP.\n",
    "\n",
    "Below the GAP I placed a dense layer with 50% dropout to help alleviate overfitting. The number of nodes selected at that layer ranged from 256 up to 2048. My best results were achieved with 512 nodes. I tried dropout rates from 40% to 75%. 50% seemed the most consistent. The last layer consists of a fully connected dense layer used to perform the softmax predictions.\n",
    "\n",
    "The suggestion in the keras blog article (that is linked to from this project) to change the optimizer to SGD with a slow learner rate made a noticeable difference in the accuracy score. I tried different learner rates, but settled on the number suggested in the article since it produced slightly better results.\n",
    "\n",
    "I experimented with adding more dense layers of varying node sizes with varying dropout rates, but didn't manage to increase accuracy. The final model that I selected uses GAP for input from the Resnet50 and 2 FC layers. It trains in around 2 seconds per epoch on an AWS GPU instance. I selected 500 epochs since the training time was short.\n",
    "\n",
    "Several models are depicted below, with the final model displayed in code cells 35 and 36 of the notebook.\n",
    "\n",
    "This architecture works better than the one depicted in step 4 becuase the addition of another fully connected layer helps to establish more meaning or relevance for the patterns coming in from the pretrained transfer model. Also, the other transfer models are probably more effective than the VGG16 used in step4, with everything else held equal.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "nbpresent": {
     "id": "eb16b6c2-d84d-4b5c-b7fa-1fb637df84d9"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_2 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 133)               68229     \n",
      "=================================================================\n",
      "Total params: 330,885.0\n",
      "Trainable params: 330,885.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### TODO: Define your architecture.\n",
    "VGG19_model = Sequential()\n",
    "VGG19_model.add(GlobalAveragePooling2D(input_shape=train_VGG19.shape[1:]))\n",
    "VGG19_model.add(Dense(512, activation='relu'))\n",
    "VGG19_model.add(Dropout(0.5))\n",
    "VGG19_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "VGG19_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "fe9034c9-227f-4ec2-8e1a-2201ed102401"
    }
   },
   "source": [
    "### (IMPLEMENTATION) Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "bb35f6e3-d2c1-4e70-ba54-938ee9eaefe1"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### TODO: Compile the model.\n",
    "from keras import optimizers\n",
    "VGG19_model.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=1e-4, momentum=0.9), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "bbc8a30c-aa10-4579-973f-f991c9014dc4"
    }
   },
   "source": [
    "### (IMPLEMENTATION) Train the Model\n",
    "\n",
    "Train your model in the code cell below.  Use model checkpointing to save the model that attains the best validation loss.  \n",
    "\n",
    "You are welcome to [augment the training data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), but this is not a requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "nbpresent": {
     "id": "fe585b02-0648-4bed-9a10-d7fbd5c084ff"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 11.0655 - acc: 0.0456Epoch 00000: val_loss improved from inf to 4.29692, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 11.0025 - acc: 0.0463 - val_loss: 4.2969 - val_acc: 0.1892\n",
      "Epoch 2/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 4.6670 - acc: 0.1372Epoch 00001: val_loss improved from 4.29692 to 3.13379, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 4.6614 - acc: 0.1382 - val_loss: 3.1338 - val_acc: 0.3066\n",
      "Epoch 3/100\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 3.5778 - acc: 0.2318Epoch 00002: val_loss improved from 3.13379 to 2.63327, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 3.5717 - acc: 0.2322 - val_loss: 2.6333 - val_acc: 0.4060\n",
      "Epoch 4/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 3.0169 - acc: 0.3213Epoch 00003: val_loss improved from 2.63327 to 2.25301, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 3.0121 - acc: 0.3216 - val_loss: 2.2530 - val_acc: 0.4707\n",
      "Epoch 5/100\n",
      "6520/6680 [============================>.] - ETA: 0s - loss: 2.6206 - acc: 0.3831Epoch 00004: val_loss improved from 2.25301 to 1.95357, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 2.6186 - acc: 0.3834 - val_loss: 1.9536 - val_acc: 0.5269\n",
      "Epoch 6/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 2.3524 - acc: 0.4342Epoch 00005: val_loss improved from 1.95357 to 1.74026, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 2.3517 - acc: 0.4340 - val_loss: 1.7403 - val_acc: 0.5677\n",
      "Epoch 7/100\n",
      "6540/6680 [============================>.] - ETA: 0s - loss: 2.1157 - acc: 0.4725Epoch 00006: val_loss improved from 1.74026 to 1.57222, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 2.1124 - acc: 0.4734 - val_loss: 1.5722 - val_acc: 0.5988\n",
      "Epoch 8/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.9296 - acc: 0.5102Epoch 00007: val_loss improved from 1.57222 to 1.45522, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 1.9283 - acc: 0.5109 - val_loss: 1.4552 - val_acc: 0.6419\n",
      "Epoch 9/100\n",
      "6540/6680 [============================>.] - ETA: 0s - loss: 1.7655 - acc: 0.5466Epoch 00008: val_loss improved from 1.45522 to 1.36713, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 1.7613 - acc: 0.5487 - val_loss: 1.3671 - val_acc: 0.6383\n",
      "Epoch 10/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.6262 - acc: 0.5715Epoch 00009: val_loss improved from 1.36713 to 1.27609, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 1.6224 - acc: 0.5723 - val_loss: 1.2761 - val_acc: 0.6551\n",
      "Epoch 11/100\n",
      "6540/6680 [============================>.] - ETA: 0s - loss: 1.5501 - acc: 0.5896Epoch 00010: val_loss improved from 1.27609 to 1.20642, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 1.5490 - acc: 0.5894 - val_loss: 1.2064 - val_acc: 0.6790\n",
      "Epoch 12/100\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 1.4406 - acc: 0.6175Epoch 00011: val_loss improved from 1.20642 to 1.16473, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 1.4413 - acc: 0.6174 - val_loss: 1.1647 - val_acc: 0.6886\n",
      "Epoch 13/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 1.3304 - acc: 0.6417Epoch 00012: val_loss improved from 1.16473 to 1.12260, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 1.3310 - acc: 0.6422 - val_loss: 1.1226 - val_acc: 0.6970\n",
      "Epoch 14/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.2593 - acc: 0.6589Epoch 00013: val_loss improved from 1.12260 to 1.08365, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 1.2635 - acc: 0.6587 - val_loss: 1.0837 - val_acc: 0.7078\n",
      "Epoch 15/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 1.2108 - acc: 0.6680Epoch 00014: val_loss improved from 1.08365 to 1.04911, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 1.2102 - acc: 0.6681 - val_loss: 1.0491 - val_acc: 0.7138\n",
      "Epoch 16/100\n",
      "6520/6680 [============================>.] - ETA: 0s - loss: 1.1660 - acc: 0.6738Epoch 00015: val_loss improved from 1.04911 to 1.01407, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 1.1658 - acc: 0.6743 - val_loss: 1.0141 - val_acc: 0.7186\n",
      "Epoch 17/100\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 1.1174 - acc: 0.6952Epoch 00016: val_loss improved from 1.01407 to 0.98047, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 1.1168 - acc: 0.6943 - val_loss: 0.9805 - val_acc: 0.7305\n",
      "Epoch 18/100\n",
      "6580/6680 [============================>.] - ETA: 0s - loss: 1.0763 - acc: 0.7017Epoch 00017: val_loss improved from 0.98047 to 0.97110, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 1.0743 - acc: 0.7015 - val_loss: 0.9711 - val_acc: 0.7281\n",
      "Epoch 19/100\n",
      "6540/6680 [============================>.] - ETA: 0s - loss: 1.0215 - acc: 0.7138Epoch 00018: val_loss improved from 0.97110 to 0.94798, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 1.0195 - acc: 0.7147 - val_loss: 0.9480 - val_acc: 0.7305\n",
      "Epoch 20/100\n",
      "6480/6680 [============================>.] - ETA: 0s - loss: 0.9489 - acc: 0.7285Epoch 00019: val_loss improved from 0.94798 to 0.93411, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 0.9495 - acc: 0.7287 - val_loss: 0.9341 - val_acc: 0.7329\n",
      "Epoch 21/100\n",
      "6540/6680 [============================>.] - ETA: 0s - loss: 0.9398 - acc: 0.7326Epoch 00020: val_loss improved from 0.93411 to 0.91707, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 0.9419 - acc: 0.7320 - val_loss: 0.9171 - val_acc: 0.7341\n",
      "Epoch 22/100\n",
      "6560/6680 [============================>.] - ETA: 0s - loss: 0.8980 - acc: 0.7462Epoch 00021: val_loss improved from 0.91707 to 0.90068, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 0.8966 - acc: 0.7470 - val_loss: 0.9007 - val_acc: 0.7389\n",
      "Epoch 23/100\n",
      "6520/6680 [============================>.] - ETA: 0s - loss: 0.8956 - acc: 0.7434Epoch 00022: val_loss improved from 0.90068 to 0.89882, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 0.8993 - acc: 0.7422 - val_loss: 0.8988 - val_acc: 0.7329\n",
      "Epoch 24/100\n",
      "6560/6680 [============================>.] - ETA: 0s - loss: 0.8350 - acc: 0.7623Epoch 00023: val_loss improved from 0.89882 to 0.88301, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 0.8356 - acc: 0.7623 - val_loss: 0.8830 - val_acc: 0.7461\n",
      "Epoch 25/100\n",
      "6480/6680 [============================>.] - ETA: 0s - loss: 0.7967 - acc: 0.7648Epoch 00024: val_loss improved from 0.88301 to 0.86680, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 0.7957 - acc: 0.7645 - val_loss: 0.8668 - val_acc: 0.7485\n",
      "Epoch 26/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8008 - acc: 0.7635Epoch 00025: val_loss improved from 0.86680 to 0.85282, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 0.7986 - acc: 0.7645 - val_loss: 0.8528 - val_acc: 0.7485\n",
      "Epoch 27/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.7755 - acc: 0.7786Epoch 00026: val_loss improved from 0.85282 to 0.84298, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 0.7780 - acc: 0.7775 - val_loss: 0.8430 - val_acc: 0.7485\n",
      "Epoch 28/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.7433 - acc: 0.7826Epoch 00027: val_loss improved from 0.84298 to 0.83378, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 0.7492 - acc: 0.7811 - val_loss: 0.8338 - val_acc: 0.7593\n",
      "Epoch 29/100\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.7145 - acc: 0.7978Epoch 00028: val_loss improved from 0.83378 to 0.82492, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 0.7152 - acc: 0.7975 - val_loss: 0.8249 - val_acc: 0.7569\n",
      "Epoch 30/100\n",
      "6540/6680 [============================>.] - ETA: 0s - loss: 0.6982 - acc: 0.7940Epoch 00029: val_loss improved from 0.82492 to 0.81884, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 0.6968 - acc: 0.7942 - val_loss: 0.8188 - val_acc: 0.7641\n",
      "Epoch 31/100\n",
      "6560/6680 [============================>.] - ETA: 0s - loss: 0.6818 - acc: 0.7973Epoch 00030: val_loss improved from 0.81884 to 0.81647, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 0.6827 - acc: 0.7970 - val_loss: 0.8165 - val_acc: 0.7593\n",
      "Epoch 32/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.6664 - acc: 0.7998Epoch 00031: val_loss improved from 0.81647 to 0.80789, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 0.6670 - acc: 0.8000 - val_loss: 0.8079 - val_acc: 0.7533\n",
      "Epoch 33/100\n",
      "6560/6680 [============================>.] - ETA: 0s - loss: 0.6599 - acc: 0.8055Epoch 00032: val_loss improved from 0.80789 to 0.80558, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 0.6573 - acc: 0.8063 - val_loss: 0.8056 - val_acc: 0.7629\n",
      "Epoch 34/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.6262 - acc: 0.8130Epoch 00033: val_loss improved from 0.80558 to 0.80142, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.6266 - acc: 0.8136 - val_loss: 0.8014 - val_acc: 0.7665\n",
      "Epoch 35/100\n",
      "6580/6680 [============================>.] - ETA: 0s - loss: 0.6197 - acc: 0.8188Epoch 00034: val_loss improved from 0.80142 to 0.79479, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.6188 - acc: 0.8192 - val_loss: 0.7948 - val_acc: 0.7641\n",
      "Epoch 36/100\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.5911 - acc: 0.8242Epoch 00035: val_loss improved from 0.79479 to 0.78380, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 0.5913 - acc: 0.8244 - val_loss: 0.7838 - val_acc: 0.7629\n",
      "Epoch 37/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.5888 - acc: 0.8225Epoch 00036: val_loss improved from 0.78380 to 0.77776, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 0.5880 - acc: 0.8226 - val_loss: 0.7778 - val_acc: 0.7701\n",
      "Epoch 38/100\n",
      "6560/6680 [============================>.] - ETA: 0s - loss: 0.5861 - acc: 0.8221Epoch 00037: val_loss improved from 0.77776 to 0.77570, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 0.5843 - acc: 0.8231 - val_loss: 0.7757 - val_acc: 0.7677\n",
      "Epoch 39/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.5686 - acc: 0.8326Epoch 00038: val_loss improved from 0.77570 to 0.76257, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 0.5678 - acc: 0.8323 - val_loss: 0.7626 - val_acc: 0.7784\n",
      "Epoch 40/100\n",
      "6520/6680 [============================>.] - ETA: 0s - loss: 0.5388 - acc: 0.8362Epoch 00039: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.5376 - acc: 0.8361 - val_loss: 0.7689 - val_acc: 0.7784\n",
      "Epoch 41/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.5408 - acc: 0.8355Epoch 00040: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.5418 - acc: 0.8356 - val_loss: 0.7663 - val_acc: 0.7713\n",
      "Epoch 42/100\n",
      "6560/6680 [============================>.] - ETA: 0s - loss: 0.5152 - acc: 0.8495Epoch 00041: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.5180 - acc: 0.8485 - val_loss: 0.7631 - val_acc: 0.7665\n",
      "Epoch 43/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4857 - acc: 0.8530Epoch 00042: val_loss improved from 0.76257 to 0.76057, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 0.4846 - acc: 0.8528 - val_loss: 0.7606 - val_acc: 0.7713\n",
      "Epoch 44/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.5076 - acc: 0.8489Epoch 00043: val_loss improved from 0.76057 to 0.75320, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 0.5073 - acc: 0.8491 - val_loss: 0.7532 - val_acc: 0.7677\n",
      "Epoch 45/100\n",
      "6520/6680 [============================>.] - ETA: 0s - loss: 0.4947 - acc: 0.8505Epoch 00044: val_loss improved from 0.75320 to 0.74614, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4915 - acc: 0.8519 - val_loss: 0.7461 - val_acc: 0.7772\n",
      "Epoch 46/100\n",
      "6480/6680 [============================>.] - ETA: 0s - loss: 0.4713 - acc: 0.8568Epoch 00045: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.4739 - acc: 0.8561 - val_loss: 0.7475 - val_acc: 0.7772\n",
      "Epoch 47/100\n",
      "6520/6680 [============================>.] - ETA: 0s - loss: 0.4833 - acc: 0.8538Epoch 00046: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.4850 - acc: 0.8536 - val_loss: 0.7544 - val_acc: 0.7772\n",
      "Epoch 48/100\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.4765 - acc: 0.8560Epoch 00047: val_loss improved from 0.74614 to 0.74457, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 0.4747 - acc: 0.8570 - val_loss: 0.7446 - val_acc: 0.7796\n",
      "Epoch 49/100\n",
      "6560/6680 [============================>.] - ETA: 0s - loss: 0.4540 - acc: 0.8671Epoch 00048: val_loss improved from 0.74457 to 0.73510, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 0.4571 - acc: 0.8662 - val_loss: 0.7351 - val_acc: 0.7808\n",
      "Epoch 50/100\n",
      "6520/6680 [============================>.] - ETA: 0s - loss: 0.4642 - acc: 0.8584Epoch 00049: val_loss improved from 0.73510 to 0.73199, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 0.4625 - acc: 0.8584 - val_loss: 0.7320 - val_acc: 0.7796\n",
      "Epoch 51/100\n",
      "6580/6680 [============================>.] - ETA: 0s - loss: 0.4530 - acc: 0.8612Epoch 00050: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.4536 - acc: 0.8606 - val_loss: 0.7330 - val_acc: 0.7820\n",
      "Epoch 52/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.4470 - acc: 0.8616Epoch 00051: val_loss improved from 0.73199 to 0.73156, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 0.4468 - acc: 0.8617 - val_loss: 0.7316 - val_acc: 0.7808\n",
      "Epoch 53/100\n",
      "6560/6680 [============================>.] - ETA: 0s - loss: 0.4196 - acc: 0.8721Epoch 00052: val_loss improved from 0.73156 to 0.72301, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4212 - acc: 0.8725 - val_loss: 0.7230 - val_acc: 0.7868\n",
      "Epoch 54/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.4111 - acc: 0.8743Epoch 00053: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.4117 - acc: 0.8744 - val_loss: 0.7277 - val_acc: 0.7772\n",
      "Epoch 55/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.4080 - acc: 0.8763Epoch 00054: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.4083 - acc: 0.8762 - val_loss: 0.7254 - val_acc: 0.7796\n",
      "Epoch 56/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.4039 - acc: 0.8785Epoch 00055: val_loss improved from 0.72301 to 0.72278, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 0.4048 - acc: 0.8781 - val_loss: 0.7228 - val_acc: 0.7784\n",
      "Epoch 57/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3966 - acc: 0.8785Epoch 00056: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.3963 - acc: 0.8790 - val_loss: 0.7257 - val_acc: 0.7784\n",
      "Epoch 58/100\n",
      "6520/6680 [============================>.] - ETA: 0s - loss: 0.3673 - acc: 0.8900Epoch 00057: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.3679 - acc: 0.8895 - val_loss: 0.7279 - val_acc: 0.7796\n",
      "Epoch 59/100\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.3886 - acc: 0.8774Epoch 00058: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.3902 - acc: 0.8772 - val_loss: 0.7242 - val_acc: 0.7832\n",
      "Epoch 60/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.3763 - acc: 0.8841Epoch 00059: val_loss improved from 0.72278 to 0.71435, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 0.3759 - acc: 0.8846 - val_loss: 0.7143 - val_acc: 0.7892\n",
      "Epoch 61/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.3622 - acc: 0.8931Epoch 00060: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.3627 - acc: 0.8930 - val_loss: 0.7162 - val_acc: 0.7892\n",
      "Epoch 62/100\n",
      "6540/6680 [============================>.] - ETA: 0s - loss: 0.3618 - acc: 0.8907Epoch 00061: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.3608 - acc: 0.8912 - val_loss: 0.7190 - val_acc: 0.7868\n",
      "Epoch 63/100\n",
      "6580/6680 [============================>.] - ETA: 0s - loss: 0.3493 - acc: 0.8919Epoch 00062: val_loss improved from 0.71435 to 0.71108, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 0.3496 - acc: 0.8919 - val_loss: 0.7111 - val_acc: 0.7892\n",
      "Epoch 64/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.3635 - acc: 0.8890Epoch 00063: val_loss improved from 0.71108 to 0.70865, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 0.3633 - acc: 0.8891 - val_loss: 0.7087 - val_acc: 0.7844\n",
      "Epoch 65/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.3509 - acc: 0.8940Epoch 00064: val_loss improved from 0.70865 to 0.70093, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 0.3504 - acc: 0.8940 - val_loss: 0.7009 - val_acc: 0.7868\n",
      "Epoch 66/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.3402 - acc: 0.8934Epoch 00065: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.3404 - acc: 0.8934 - val_loss: 0.7066 - val_acc: 0.7916\n",
      "Epoch 67/100\n",
      "6520/6680 [============================>.] - ETA: 0s - loss: 0.3380 - acc: 0.8952Epoch 00066: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.3400 - acc: 0.8943 - val_loss: 0.7038 - val_acc: 0.7892\n",
      "Epoch 68/100\n",
      "6520/6680 [============================>.] - ETA: 0s - loss: 0.3263 - acc: 0.8998Epoch 00067: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.3248 - acc: 0.8997 - val_loss: 0.7058 - val_acc: 0.7892\n",
      "Epoch 69/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.3368 - acc: 0.8997Epoch 00068: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.3359 - acc: 0.9001 - val_loss: 0.7054 - val_acc: 0.7916\n",
      "Epoch 70/100\n",
      "6580/6680 [============================>.] - ETA: 0s - loss: 0.3191 - acc: 0.9053Epoch 00069: val_loss improved from 0.70093 to 0.69526, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 0.3209 - acc: 0.9052 - val_loss: 0.6953 - val_acc: 0.7976\n",
      "Epoch 71/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.3101 - acc: 0.9077Epoch 00070: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.3105 - acc: 0.9075 - val_loss: 0.6988 - val_acc: 0.7892\n",
      "Epoch 72/100\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.3198 - acc: 0.9049Epoch 00071: val_loss improved from 0.69526 to 0.69498, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 0.3211 - acc: 0.9043 - val_loss: 0.6950 - val_acc: 0.7868\n",
      "Epoch 73/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3082 - acc: 0.9071Epoch 00072: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.3073 - acc: 0.9075 - val_loss: 0.6981 - val_acc: 0.7928\n",
      "Epoch 74/100\n",
      "6560/6680 [============================>.] - ETA: 0s - loss: 0.3004 - acc: 0.9076Epoch 00073: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.3021 - acc: 0.9069 - val_loss: 0.6974 - val_acc: 0.7892\n",
      "Epoch 75/100\n",
      "6560/6680 [============================>.] - ETA: 0s - loss: 0.2988 - acc: 0.9137Epoch 00074: val_loss improved from 0.69498 to 0.69226, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 0.2985 - acc: 0.9133 - val_loss: 0.6923 - val_acc: 0.7916\n",
      "Epoch 76/100\n",
      "6520/6680 [============================>.] - ETA: 0s - loss: 0.2942 - acc: 0.9106Epoch 00075: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.2932 - acc: 0.9112 - val_loss: 0.6973 - val_acc: 0.7916\n",
      "Epoch 77/100\n",
      "6480/6680 [============================>.] - ETA: 0s - loss: 0.3075 - acc: 0.9082Epoch 00076: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.3082 - acc: 0.9081 - val_loss: 0.7008 - val_acc: 0.7844\n",
      "Epoch 78/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.3007 - acc: 0.9081Epoch 00077: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.3020 - acc: 0.9076 - val_loss: 0.6977 - val_acc: 0.7916\n",
      "Epoch 79/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2886 - acc: 0.9127Epoch 00078: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.2882 - acc: 0.9124 - val_loss: 0.6954 - val_acc: 0.7976\n",
      "Epoch 80/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.2790 - acc: 0.9192Epoch 00079: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2798 - acc: 0.9190 - val_loss: 0.6954 - val_acc: 0.7880\n",
      "Epoch 81/100\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.2771 - acc: 0.9171Epoch 00080: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2803 - acc: 0.9154 - val_loss: 0.6936 - val_acc: 0.7892\n",
      "Epoch 82/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.2830 - acc: 0.9156Epoch 00081: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.2834 - acc: 0.9154 - val_loss: 0.6994 - val_acc: 0.7880\n",
      "Epoch 83/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.2705 - acc: 0.9209Epoch 00082: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.2706 - acc: 0.9205 - val_loss: 0.6936 - val_acc: 0.7880\n",
      "Epoch 84/100\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.2734 - acc: 0.9175Epoch 00083: val_loss improved from 0.69226 to 0.68489, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 0.2736 - acc: 0.9177 - val_loss: 0.6849 - val_acc: 0.7868\n",
      "Epoch 85/100\n",
      "6520/6680 [============================>.] - ETA: 0s - loss: 0.2612 - acc: 0.9247Epoch 00084: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.2617 - acc: 0.9243 - val_loss: 0.6954 - val_acc: 0.7868\n",
      "Epoch 86/100\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.2610 - acc: 0.9205Epoch 00085: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.2592 - acc: 0.9214 - val_loss: 0.6893 - val_acc: 0.7976\n",
      "Epoch 87/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.2595 - acc: 0.9210Epoch 00086: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.2592 - acc: 0.9210 - val_loss: 0.6858 - val_acc: 0.7964\n",
      "Epoch 88/100\n",
      "6480/6680 [============================>.] - ETA: 0s - loss: 0.2580 - acc: 0.9224Epoch 00087: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.2571 - acc: 0.9222 - val_loss: 0.6897 - val_acc: 0.7892\n",
      "Epoch 89/100\n",
      "6540/6680 [============================>.] - ETA: 0s - loss: 0.2509 - acc: 0.9258Epoch 00088: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.2490 - acc: 0.9266 - val_loss: 0.6929 - val_acc: 0.7940\n",
      "Epoch 90/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.2441 - acc: 0.9297Epoch 00089: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.2450 - acc: 0.9293 - val_loss: 0.6850 - val_acc: 0.8060\n",
      "Epoch 91/100\n",
      "6520/6680 [============================>.] - ETA: 0s - loss: 0.2525 - acc: 0.9256Epoch 00090: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.2533 - acc: 0.9253 - val_loss: 0.6881 - val_acc: 0.7916\n",
      "Epoch 92/100\n",
      "6580/6680 [============================>.] - ETA: 0s - loss: 0.2513 - acc: 0.9229Epoch 00091: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.2517 - acc: 0.9234 - val_loss: 0.6865 - val_acc: 0.7988\n",
      "Epoch 93/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.2390 - acc: 0.9285Epoch 00092: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.2385 - acc: 0.9290 - val_loss: 0.6860 - val_acc: 0.7928\n",
      "Epoch 94/100\n",
      "6480/6680 [============================>.] - ETA: 0s - loss: 0.2310 - acc: 0.9352Epoch 00093: val_loss improved from 0.68489 to 0.68006, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 0.2305 - acc: 0.9358 - val_loss: 0.6801 - val_acc: 0.8048\n",
      "Epoch 95/100\n",
      "6480/6680 [============================>.] - ETA: 0s - loss: 0.2456 - acc: 0.9253Epoch 00094: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.2472 - acc: 0.9249 - val_loss: 0.6825 - val_acc: 0.7940\n",
      "Epoch 96/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.2341 - acc: 0.9319Epoch 00095: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.2348 - acc: 0.9322 - val_loss: 0.6901 - val_acc: 0.7940\n",
      "Epoch 97/100\n",
      "6480/6680 [============================>.] - ETA: 0s - loss: 0.2262 - acc: 0.9326Epoch 00096: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.2258 - acc: 0.9331 - val_loss: 0.6844 - val_acc: 0.7976\n",
      "Epoch 98/100\n",
      "6520/6680 [============================>.] - ETA: 0s - loss: 0.2397 - acc: 0.9293Epoch 00097: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.2412 - acc: 0.9284 - val_loss: 0.6878 - val_acc: 0.8024\n",
      "Epoch 99/100\n",
      "6580/6680 [============================>.] - ETA: 0s - loss: 0.2281 - acc: 0.9333Epoch 00098: val_loss improved from 0.68006 to 0.67991, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 0.2288 - acc: 0.9331 - val_loss: 0.6799 - val_acc: 0.7988\n",
      "Epoch 100/100\n",
      "6560/6680 [============================>.] - ETA: 0s - loss: 0.2271 - acc: 0.9320Epoch 00099: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.2260 - acc: 0.9322 - val_loss: 0.6847 - val_acc: 0.7976\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8af030db38>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint \n",
    "### TODO: Train the model.\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.VGG19.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "VGG19_model.fit(train_VGG19, train_targets, \n",
    "          validation_data=(valid_VGG19, valid_targets),\n",
    "          epochs=100, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b134ae7c-d30b-413a-b5e9-8eb8fc5a5332"
    }
   },
   "source": [
    "### (IMPLEMENTATION) Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "1c02170f-4661-4741-98fb-2e73fae3c15e"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### TODO: Load the model weights with the best validation loss.\n",
    "VGG19_model.load_weights('saved_models/weights.best.VGG19.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "82f89478-4ac7-4e36-bdcf-4190c8ddcfc9"
    }
   },
   "source": [
    "### (IMPLEMENTATION) Test the Model\n",
    "\n",
    "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "nbpresent": {
     "id": "93e448c9-cba4-4132-8123-fcc29eecf5cd"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 79.0670%\n"
     ]
    }
   ],
   "source": [
    "### TODO: Calculate classification accuracy on the test dataset.\n",
    "# get index of predicted dog breed for each image in test set\n",
    "VGG19_predictions = [np.argmax(VGG19_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG19]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(VGG19_predictions)==np.argmax(test_targets, axis=1))/len(VGG19_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "nbpresent": {
     "id": "1e1c5169-867f-4c11-b9cd-8257f4dc3d5a"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 5.1390 - acc: 0.0156Epoch 00000: val_loss improved from inf to 4.57412, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 5.1372 - acc: 0.0157 - val_loss: 4.5741 - val_acc: 0.0778\n",
      "Epoch 2/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 4.5535 - acc: 0.0616Epoch 00001: val_loss improved from 4.57412 to 4.22501, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 4.5529 - acc: 0.0614 - val_loss: 4.2250 - val_acc: 0.2012\n",
      "Epoch 3/100\n",
      "6580/6680 [============================>.] - ETA: 0s - loss: 4.1955 - acc: 0.1240Epoch 00002: val_loss improved from 4.22501 to 3.88529, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 4.1946 - acc: 0.1244 - val_loss: 3.8853 - val_acc: 0.3042\n",
      "Epoch 4/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 3.8687 - acc: 0.1856Epoch 00003: val_loss improved from 3.88529 to 3.53244, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 3.8649 - acc: 0.1867 - val_loss: 3.5324 - val_acc: 0.3641\n",
      "Epoch 5/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 3.5381 - acc: 0.2514Epoch 00004: val_loss improved from 3.53244 to 3.18637, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 3.5362 - acc: 0.2519 - val_loss: 3.1864 - val_acc: 0.4228\n",
      "Epoch 6/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 3.2216 - acc: 0.3194Epoch 00005: val_loss improved from 3.18637 to 2.86339, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 3.2231 - acc: 0.3189 - val_loss: 2.8634 - val_acc: 0.4874\n",
      "Epoch 7/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 2.9510 - acc: 0.3677Epoch 00006: val_loss improved from 2.86339 to 2.57452, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 2.9508 - acc: 0.3678 - val_loss: 2.5745 - val_acc: 0.5293\n",
      "Epoch 8/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 2.6993 - acc: 0.4044Epoch 00007: val_loss improved from 2.57452 to 2.32757, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 2.6999 - acc: 0.4040 - val_loss: 2.3276 - val_acc: 0.5749\n",
      "Epoch 9/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 2.4839 - acc: 0.4462Epoch 00008: val_loss improved from 2.32757 to 2.11815, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 2.4838 - acc: 0.4461 - val_loss: 2.1181 - val_acc: 0.6096\n",
      "Epoch 10/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 2.3026 - acc: 0.4752Epoch 00009: val_loss improved from 2.11815 to 1.93847, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 2.3015 - acc: 0.4754 - val_loss: 1.9385 - val_acc: 0.6359\n",
      "Epoch 11/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 2.1448 - acc: 0.4998Epoch 00010: val_loss improved from 1.93847 to 1.78660, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 2.1451 - acc: 0.4996 - val_loss: 1.7866 - val_acc: 0.6575\n",
      "Epoch 12/100\n",
      "6580/6680 [============================>.] - ETA: 0s - loss: 2.0036 - acc: 0.5316Epoch 00011: val_loss improved from 1.78660 to 1.65788, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 2.0026 - acc: 0.5319 - val_loss: 1.6579 - val_acc: 0.6695\n",
      "Epoch 13/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.8873 - acc: 0.5406Epoch 00012: val_loss improved from 1.65788 to 1.55242, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 1.8870 - acc: 0.5410 - val_loss: 1.5524 - val_acc: 0.6802\n",
      "Epoch 14/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 1.7681 - acc: 0.5705Epoch 00013: val_loss improved from 1.55242 to 1.45853, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 1.7684 - acc: 0.5702 - val_loss: 1.4585 - val_acc: 0.6898\n",
      "Epoch 15/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 1.6550 - acc: 0.5929Epoch 00014: val_loss improved from 1.45853 to 1.37571, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 1.6564 - acc: 0.5928 - val_loss: 1.3757 - val_acc: 0.7006\n",
      "Epoch 16/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 1.5886 - acc: 0.6020Epoch 00015: val_loss improved from 1.37571 to 1.31050, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 1.5905 - acc: 0.6015 - val_loss: 1.3105 - val_acc: 0.7102\n",
      "Epoch 17/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 1.5031 - acc: 0.6300Epoch 00016: val_loss improved from 1.31050 to 1.24481, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 1.5027 - acc: 0.6304 - val_loss: 1.2448 - val_acc: 0.7186\n",
      "Epoch 18/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 1.4397 - acc: 0.6342Epoch 00017: val_loss improved from 1.24481 to 1.19210, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 1.4380 - acc: 0.6347 - val_loss: 1.1921 - val_acc: 0.7198\n",
      "Epoch 19/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 1.3810 - acc: 0.6480Epoch 00018: val_loss improved from 1.19210 to 1.14659, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 1.3813 - acc: 0.6481 - val_loss: 1.1466 - val_acc: 0.7329\n",
      "Epoch 20/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.3336 - acc: 0.6529Epoch 00019: val_loss improved from 1.14659 to 1.10611, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 1.3347 - acc: 0.6527 - val_loss: 1.1061 - val_acc: 0.7509\n",
      "Epoch 21/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.2716 - acc: 0.6712Epoch 00020: val_loss improved from 1.10611 to 1.06607, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 1.2738 - acc: 0.6705 - val_loss: 1.0661 - val_acc: 0.7545\n",
      "Epoch 22/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 1.2400 - acc: 0.6801Epoch 00021: val_loss improved from 1.06607 to 1.03323, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 1.2390 - acc: 0.6805 - val_loss: 1.0332 - val_acc: 0.7641\n",
      "Epoch 23/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 1.1667 - acc: 0.6994Epoch 00022: val_loss improved from 1.03323 to 1.00050, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 1.1677 - acc: 0.6991 - val_loss: 1.0005 - val_acc: 0.7665\n",
      "Epoch 24/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 1.1527 - acc: 0.6928Epoch 00023: val_loss improved from 1.00050 to 0.97495, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 1.1517 - acc: 0.6933 - val_loss: 0.9750 - val_acc: 0.7701\n",
      "Epoch 25/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 1.1228 - acc: 0.6997Epoch 00024: val_loss improved from 0.97495 to 0.94908, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 1.1224 - acc: 0.6997 - val_loss: 0.9491 - val_acc: 0.7749\n",
      "Epoch 26/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 1.0734 - acc: 0.7173Epoch 00025: val_loss improved from 0.94908 to 0.92481, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 1.0739 - acc: 0.7172 - val_loss: 0.9248 - val_acc: 0.7784\n",
      "Epoch 27/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 1.0542 - acc: 0.7200Epoch 00026: val_loss improved from 0.92481 to 0.90396, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 1.0545 - acc: 0.7199 - val_loss: 0.9040 - val_acc: 0.7784\n",
      "Epoch 28/100\n",
      "6580/6680 [============================>.] - ETA: 0s - loss: 1.0256 - acc: 0.7252Epoch 00027: val_loss improved from 0.90396 to 0.88816, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 1.0240 - acc: 0.7256 - val_loss: 0.8882 - val_acc: 0.7832\n",
      "Epoch 29/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.9836 - acc: 0.7409Epoch 00028: val_loss improved from 0.88816 to 0.86932, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.9854 - acc: 0.7401 - val_loss: 0.8693 - val_acc: 0.7832\n",
      "Epoch 30/100\n",
      "6580/6680 [============================>.] - ETA: 0s - loss: 0.9747 - acc: 0.7362Epoch 00029: val_loss improved from 0.86932 to 0.85407, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.9758 - acc: 0.7359 - val_loss: 0.8541 - val_acc: 0.7832\n",
      "Epoch 31/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.9460 - acc: 0.7492Epoch 00030: val_loss improved from 0.85407 to 0.83752, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.9469 - acc: 0.7491 - val_loss: 0.8375 - val_acc: 0.7868\n",
      "Epoch 32/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.9261 - acc: 0.7526Epoch 00031: val_loss improved from 0.83752 to 0.82424, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.9261 - acc: 0.7525 - val_loss: 0.8242 - val_acc: 0.7832\n",
      "Epoch 33/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.8936 - acc: 0.7533Epoch 00032: val_loss improved from 0.82424 to 0.80889, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.8931 - acc: 0.7533 - val_loss: 0.8089 - val_acc: 0.7904\n",
      "Epoch 34/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8726 - acc: 0.7645Epoch 00033: val_loss improved from 0.80889 to 0.79643, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.8723 - acc: 0.7642 - val_loss: 0.7964 - val_acc: 0.7928\n",
      "Epoch 35/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.8570 - acc: 0.7676Epoch 00034: val_loss improved from 0.79643 to 0.78389, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.8569 - acc: 0.7678 - val_loss: 0.7839 - val_acc: 0.7940\n",
      "Epoch 36/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.8263 - acc: 0.7749Epoch 00035: val_loss improved from 0.78389 to 0.77243, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.8272 - acc: 0.7746 - val_loss: 0.7724 - val_acc: 0.8012\n",
      "Epoch 37/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.8231 - acc: 0.7771Epoch 00036: val_loss improved from 0.77243 to 0.76259, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.8230 - acc: 0.7775 - val_loss: 0.7626 - val_acc: 0.7952\n",
      "Epoch 38/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.7989 - acc: 0.7826Epoch 00037: val_loss improved from 0.76259 to 0.75311, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.7986 - acc: 0.7826 - val_loss: 0.7531 - val_acc: 0.8000\n",
      "Epoch 39/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.7974 - acc: 0.7761Epoch 00038: val_loss improved from 0.75311 to 0.74660, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.7992 - acc: 0.7756 - val_loss: 0.7466 - val_acc: 0.7988\n",
      "Epoch 40/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.7743 - acc: 0.7922Epoch 00039: val_loss improved from 0.74660 to 0.73477, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.7734 - acc: 0.7924 - val_loss: 0.7348 - val_acc: 0.7988\n",
      "Epoch 41/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.7537 - acc: 0.7922Epoch 00040: val_loss improved from 0.73477 to 0.72669, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.7538 - acc: 0.7922 - val_loss: 0.7267 - val_acc: 0.8036\n",
      "Epoch 42/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.7475 - acc: 0.7949Epoch 00041: val_loss improved from 0.72669 to 0.71630, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.7471 - acc: 0.7949 - val_loss: 0.7163 - val_acc: 0.8048\n",
      "Epoch 43/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.7354 - acc: 0.7958Epoch 00042: val_loss improved from 0.71630 to 0.71040, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.7346 - acc: 0.7961 - val_loss: 0.7104 - val_acc: 0.8000\n",
      "Epoch 44/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.7245 - acc: 0.8003Epoch 00043: val_loss improved from 0.71040 to 0.70252, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.7252 - acc: 0.7997 - val_loss: 0.7025 - val_acc: 0.8000\n",
      "Epoch 45/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.7167 - acc: 0.8065Epoch 00044: val_loss improved from 0.70252 to 0.69598, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.7157 - acc: 0.8070 - val_loss: 0.6960 - val_acc: 0.8048\n",
      "Epoch 46/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.7003 - acc: 0.8059Epoch 00045: val_loss improved from 0.69598 to 0.68837, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.6991 - acc: 0.8063 - val_loss: 0.6884 - val_acc: 0.8060\n",
      "Epoch 47/100\n",
      "6580/6680 [============================>.] - ETA: 0s - loss: 0.6882 - acc: 0.8117Epoch 00046: val_loss improved from 0.68837 to 0.68471, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.6887 - acc: 0.8118 - val_loss: 0.6847 - val_acc: 0.8096\n",
      "Epoch 48/100\n",
      "6580/6680 [============================>.] - ETA: 0s - loss: 0.6807 - acc: 0.8106Epoch 00047: val_loss improved from 0.68471 to 0.67936, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.6817 - acc: 0.8102 - val_loss: 0.6794 - val_acc: 0.8048\n",
      "Epoch 49/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.6644 - acc: 0.8137Epoch 00048: val_loss improved from 0.67936 to 0.67361, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.6641 - acc: 0.8136 - val_loss: 0.6736 - val_acc: 0.8084\n",
      "Epoch 50/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.6671 - acc: 0.8105Epoch 00049: val_loss improved from 0.67361 to 0.66840, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.6669 - acc: 0.8105 - val_loss: 0.6684 - val_acc: 0.8060\n",
      "Epoch 51/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.6471 - acc: 0.8229Epoch 00050: val_loss improved from 0.66840 to 0.66126, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.6485 - acc: 0.8225 - val_loss: 0.6613 - val_acc: 0.8072\n",
      "Epoch 52/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.6384 - acc: 0.8238Epoch 00051: val_loss improved from 0.66126 to 0.65986, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.6386 - acc: 0.8237 - val_loss: 0.6599 - val_acc: 0.8060\n",
      "Epoch 53/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.6236 - acc: 0.8278Epoch 00052: val_loss improved from 0.65986 to 0.65550, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.6241 - acc: 0.8272 - val_loss: 0.6555 - val_acc: 0.8048\n",
      "Epoch 54/100\n",
      "6580/6680 [============================>.] - ETA: 0s - loss: 0.6270 - acc: 0.8196Epoch 00053: val_loss improved from 0.65550 to 0.64952, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.6271 - acc: 0.8198 - val_loss: 0.6495 - val_acc: 0.8084\n",
      "Epoch 55/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.6224 - acc: 0.8243Epoch 00054: val_loss improved from 0.64952 to 0.64642, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.6208 - acc: 0.8251 - val_loss: 0.6464 - val_acc: 0.8132\n",
      "Epoch 56/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.5910 - acc: 0.8389Epoch 00055: val_loss improved from 0.64642 to 0.64121, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.5904 - acc: 0.8391 - val_loss: 0.6412 - val_acc: 0.8072\n",
      "Epoch 57/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.5916 - acc: 0.8308Epoch 00056: val_loss improved from 0.64121 to 0.63609, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.5902 - acc: 0.8316 - val_loss: 0.6361 - val_acc: 0.8108\n",
      "Epoch 58/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.5789 - acc: 0.8426Epoch 00057: val_loss improved from 0.63609 to 0.63082, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.5794 - acc: 0.8421 - val_loss: 0.6308 - val_acc: 0.8084\n",
      "Epoch 59/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.5872 - acc: 0.8315Epoch 00058: val_loss improved from 0.63082 to 0.62646, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.5876 - acc: 0.8311 - val_loss: 0.6265 - val_acc: 0.8108\n",
      "Epoch 60/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.5676 - acc: 0.8435Epoch 00059: val_loss improved from 0.62646 to 0.62481, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.5666 - acc: 0.8442 - val_loss: 0.6248 - val_acc: 0.8156\n",
      "Epoch 61/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.5540 - acc: 0.8476Epoch 00060: val_loss improved from 0.62481 to 0.61956, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.5574 - acc: 0.8464 - val_loss: 0.6196 - val_acc: 0.8228\n",
      "Epoch 62/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.5522 - acc: 0.8482Epoch 00061: val_loss improved from 0.61956 to 0.61787, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.5515 - acc: 0.8490 - val_loss: 0.6179 - val_acc: 0.8120\n",
      "Epoch 63/100\n",
      "6580/6680 [============================>.] - ETA: 0s - loss: 0.5418 - acc: 0.8494Epoch 00062: val_loss improved from 0.61787 to 0.61350, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.5405 - acc: 0.8500 - val_loss: 0.6135 - val_acc: 0.8144\n",
      "Epoch 64/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.5400 - acc: 0.8467Epoch 00063: val_loss improved from 0.61350 to 0.61008, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.5402 - acc: 0.8469 - val_loss: 0.6101 - val_acc: 0.8156\n",
      "Epoch 65/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.5216 - acc: 0.8596Epoch 00064: val_loss improved from 0.61008 to 0.60844, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.5215 - acc: 0.8596 - val_loss: 0.6084 - val_acc: 0.8132\n",
      "Epoch 66/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.5319 - acc: 0.8524Epoch 00065: val_loss improved from 0.60844 to 0.60390, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.5310 - acc: 0.8530 - val_loss: 0.6039 - val_acc: 0.8144\n",
      "Epoch 67/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.5257 - acc: 0.8574Epoch 00066: val_loss improved from 0.60390 to 0.60065, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.5263 - acc: 0.8572 - val_loss: 0.6007 - val_acc: 0.8156\n",
      "Epoch 68/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.5140 - acc: 0.8550Epoch 00067: val_loss improved from 0.60065 to 0.59744, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.5137 - acc: 0.8549 - val_loss: 0.5974 - val_acc: 0.8168\n",
      "Epoch 69/100\n",
      "6580/6680 [============================>.] - ETA: 0s - loss: 0.5054 - acc: 0.8608Epoch 00068: val_loss improved from 0.59744 to 0.59542, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.5064 - acc: 0.8606 - val_loss: 0.5954 - val_acc: 0.8192\n",
      "Epoch 70/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.5024 - acc: 0.8560Epoch 00069: val_loss improved from 0.59542 to 0.59394, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.5025 - acc: 0.8561 - val_loss: 0.5939 - val_acc: 0.8192\n",
      "Epoch 71/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4935 - acc: 0.8586Epoch 00070: val_loss improved from 0.59394 to 0.59063, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.4938 - acc: 0.8585 - val_loss: 0.5906 - val_acc: 0.8168\n",
      "Epoch 72/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.4896 - acc: 0.8637Epoch 00071: val_loss improved from 0.59063 to 0.58965, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.4893 - acc: 0.8639 - val_loss: 0.5896 - val_acc: 0.8144\n",
      "Epoch 73/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4899 - acc: 0.8632Epoch 00072: val_loss improved from 0.58965 to 0.58696, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.4883 - acc: 0.8633 - val_loss: 0.5870 - val_acc: 0.8216\n",
      "Epoch 74/100\n",
      "6580/6680 [============================>.] - ETA: 0s - loss: 0.4744 - acc: 0.8707Epoch 00073: val_loss improved from 0.58696 to 0.58323, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.4736 - acc: 0.8707 - val_loss: 0.5832 - val_acc: 0.8251\n",
      "Epoch 75/100\n",
      "6580/6680 [============================>.] - ETA: 0s - loss: 0.4681 - acc: 0.8678Epoch 00074: val_loss improved from 0.58323 to 0.58104, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.4695 - acc: 0.8675 - val_loss: 0.5810 - val_acc: 0.8251\n",
      "Epoch 76/100\n",
      "6580/6680 [============================>.] - ETA: 0s - loss: 0.4666 - acc: 0.8733Epoch 00075: val_loss improved from 0.58104 to 0.58061, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.4662 - acc: 0.8731 - val_loss: 0.5806 - val_acc: 0.8311\n",
      "Epoch 77/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.4585 - acc: 0.8766Epoch 00076: val_loss improved from 0.58061 to 0.57934, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.4597 - acc: 0.8760 - val_loss: 0.5793 - val_acc: 0.8240\n",
      "Epoch 78/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4649 - acc: 0.8695Epoch 00077: val_loss improved from 0.57934 to 0.57735, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.4659 - acc: 0.8695 - val_loss: 0.5773 - val_acc: 0.8263\n",
      "Epoch 79/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.4663 - acc: 0.8707Epoch 00078: val_loss improved from 0.57735 to 0.57478, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.4656 - acc: 0.8710 - val_loss: 0.5748 - val_acc: 0.8299\n",
      "Epoch 80/100\n",
      "6580/6680 [============================>.] - ETA: 0s - loss: 0.4369 - acc: 0.8809Epoch 00079: val_loss improved from 0.57478 to 0.57396, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.4361 - acc: 0.8811 - val_loss: 0.5740 - val_acc: 0.8263\n",
      "Epoch 81/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.4483 - acc: 0.8767Epoch 00080: val_loss improved from 0.57396 to 0.57271, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.4476 - acc: 0.8771 - val_loss: 0.5727 - val_acc: 0.8263\n",
      "Epoch 82/100\n",
      "6580/6680 [============================>.] - ETA: 0s - loss: 0.4430 - acc: 0.8766Epoch 00081: val_loss improved from 0.57271 to 0.56907, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.4427 - acc: 0.8765 - val_loss: 0.5691 - val_acc: 0.8275\n",
      "Epoch 83/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.4408 - acc: 0.8787Epoch 00082: val_loss improved from 0.56907 to 0.56829, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.4408 - acc: 0.8784 - val_loss: 0.5683 - val_acc: 0.8251\n",
      "Epoch 84/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4265 - acc: 0.8820Epoch 00083: val_loss improved from 0.56829 to 0.56666, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.4272 - acc: 0.8816 - val_loss: 0.5667 - val_acc: 0.8251\n",
      "Epoch 85/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4254 - acc: 0.8841Epoch 00084: val_loss improved from 0.56666 to 0.56341, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.4279 - acc: 0.8834 - val_loss: 0.5634 - val_acc: 0.8275\n",
      "Epoch 86/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.4201 - acc: 0.8878Epoch 00085: val_loss improved from 0.56341 to 0.56152, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.4195 - acc: 0.8879 - val_loss: 0.5615 - val_acc: 0.8287\n",
      "Epoch 87/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.4198 - acc: 0.8846Epoch 00086: val_loss improved from 0.56152 to 0.55939, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.4201 - acc: 0.8846 - val_loss: 0.5594 - val_acc: 0.8347\n",
      "Epoch 88/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.4153 - acc: 0.8851Epoch 00087: val_loss improved from 0.55939 to 0.55925, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.4155 - acc: 0.8852 - val_loss: 0.5593 - val_acc: 0.8251\n",
      "Epoch 89/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.4091 - acc: 0.8865Epoch 00088: val_loss improved from 0.55925 to 0.55834, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.4096 - acc: 0.8862 - val_loss: 0.5583 - val_acc: 0.8275\n",
      "Epoch 90/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4102 - acc: 0.8848Epoch 00089: val_loss improved from 0.55834 to 0.55684, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.4093 - acc: 0.8855 - val_loss: 0.5568 - val_acc: 0.8299\n",
      "Epoch 91/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.4074 - acc: 0.8884Epoch 00090: val_loss improved from 0.55684 to 0.55539, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.4079 - acc: 0.8883 - val_loss: 0.5554 - val_acc: 0.8323\n",
      "Epoch 92/100\n",
      "6580/6680 [============================>.] - ETA: 0s - loss: 0.3964 - acc: 0.8903Epoch 00091: val_loss improved from 0.55539 to 0.55261, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.3954 - acc: 0.8907 - val_loss: 0.5526 - val_acc: 0.8263\n",
      "Epoch 93/100\n",
      "6580/6680 [============================>.] - ETA: 0s - loss: 0.4024 - acc: 0.8903Epoch 00092: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.4020 - acc: 0.8904 - val_loss: 0.5531 - val_acc: 0.8275\n",
      "Epoch 94/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.3833 - acc: 0.8961Epoch 00093: val_loss improved from 0.55261 to 0.55144, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.3838 - acc: 0.8960 - val_loss: 0.5514 - val_acc: 0.8287\n",
      "Epoch 95/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.3938 - acc: 0.8946Epoch 00094: val_loss improved from 0.55144 to 0.54919, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.3926 - acc: 0.8951 - val_loss: 0.5492 - val_acc: 0.8287\n",
      "Epoch 96/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.3909 - acc: 0.8958Epoch 00095: val_loss improved from 0.54919 to 0.54507, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.3905 - acc: 0.8960 - val_loss: 0.5451 - val_acc: 0.8371\n",
      "Epoch 97/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.3910 - acc: 0.8962Epoch 00096: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.3907 - acc: 0.8964 - val_loss: 0.5459 - val_acc: 0.8228\n",
      "Epoch 98/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.3802 - acc: 0.8970Epoch 00097: val_loss improved from 0.54507 to 0.54434, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.3791 - acc: 0.8975 - val_loss: 0.5443 - val_acc: 0.8263\n",
      "Epoch 99/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.3725 - acc: 0.8971Epoch 00098: val_loss improved from 0.54434 to 0.54254, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.3721 - acc: 0.8973 - val_loss: 0.5425 - val_acc: 0.8347\n",
      "Epoch 100/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.3687 - acc: 0.9015Epoch 00099: val_loss improved from 0.54254 to 0.54153, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.3693 - acc: 0.9012 - val_loss: 0.5415 - val_acc: 0.8359\n"
     ]
    }
   ],
   "source": [
    "#######################################       RESNET50 Implementation       ####################################################\n",
    "\n",
    "### TODO: Obtain bottleneck features from another pre-trained CNN.\n",
    "bottleneck_features = np.load('bottleneck_features/DogResnet50Data.npz')\n",
    "train_Res = bottleneck_features['train']\n",
    "valid_Res = bottleneck_features['valid']\n",
    "test_Res = bottleneck_features['test']\n",
    "\n",
    "### TODO: Define your architecture.\n",
    "Res_model = Sequential()\n",
    "Res_model.add(GlobalAveragePooling2D(input_shape=train_Res.shape[1:]))\n",
    "Res_model.add(Dense(512, activation='relu'))\n",
    "Res_model.add(Dropout(0.5))\n",
    "Res_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "Res_model.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=1e-4, momentum=0.9), metrics=['accuracy'])\n",
    "\n",
    " \n",
    "### TODO: Train the model.\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.Res.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "Res_model.fit(train_Res, train_targets, \n",
    "          validation_data=(valid_Res, valid_targets),\n",
    "          epochs=100, batch_size=20, callbacks=[checkpointer], verbose=1)\n",
    "### TODO: Load the model weights with the best validation loss.\n",
    "Res_model.load_weights('saved_models/weights.best.Res.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "nbpresent": {
     "id": "79ac0fae-69ea-4386-a3a3-2f5a104fe452"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 82.8947%\n"
     ]
    }
   ],
   "source": [
    "### TODO: Calculate classification accuracy on the test dataset.\n",
    "# get index of predicted dog breed for each image in test set\n",
    "Res_predictions = [np.argmax(Res_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Res]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(Res_predictions)==np.argmax(test_targets, axis=1))/len(Res_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "nbpresent": {
     "id": "164067d0-394b-4d40-8d5e-029e6affe38c"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 5.0812 - acc: 0.0171Epoch 00000: val_loss improved from inf to 4.38869, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 6s - loss: 5.0775 - acc: 0.0172 - val_loss: 4.3887 - val_acc: 0.1353\n",
      "Epoch 2/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 4.3237 - acc: 0.1008Epoch 00001: val_loss improved from 4.38869 to 3.82998, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 4.3237 - acc: 0.1006 - val_loss: 3.8300 - val_acc: 0.3521\n",
      "Epoch 3/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 3.8055 - acc: 0.2402Epoch 00002: val_loss improved from 3.82998 to 3.29579, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 3.8029 - acc: 0.2410 - val_loss: 3.2958 - val_acc: 0.5198\n",
      "Epoch 4/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 3.3033 - acc: 0.3665Epoch 00003: val_loss improved from 3.29579 to 2.77557, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 3.3024 - acc: 0.3665 - val_loss: 2.7756 - val_acc: 0.6048\n",
      "Epoch 5/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 2.8300 - acc: 0.4580Epoch 00004: val_loss improved from 2.77557 to 2.31587, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 2.8293 - acc: 0.4581 - val_loss: 2.3159 - val_acc: 0.6467\n",
      "Epoch 6/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 2.4653 - acc: 0.5251Epoch 00005: val_loss improved from 2.31587 to 1.94443, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 2.4651 - acc: 0.5249 - val_loss: 1.9444 - val_acc: 0.6994\n",
      "Epoch 7/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 2.1479 - acc: 0.5764Epoch 00006: val_loss improved from 1.94443 to 1.65087, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 2.1466 - acc: 0.5768 - val_loss: 1.6509 - val_acc: 0.7341\n",
      "Epoch 8/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 1.8930 - acc: 0.6156Epoch 00007: val_loss improved from 1.65087 to 1.42877, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.8897 - acc: 0.6165 - val_loss: 1.4288 - val_acc: 0.7593\n",
      "Epoch 9/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 1.6897 - acc: 0.6464Epoch 00008: val_loss improved from 1.42877 to 1.26261, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.6888 - acc: 0.6469 - val_loss: 1.2626 - val_acc: 0.7689\n",
      "Epoch 10/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 1.5301 - acc: 0.6708Epoch 00009: val_loss improved from 1.26261 to 1.13202, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.5304 - acc: 0.6701 - val_loss: 1.1320 - val_acc: 0.7796\n",
      "Epoch 11/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 1.4100 - acc: 0.6873Epoch 00010: val_loss improved from 1.13202 to 1.03281, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.4097 - acc: 0.6877 - val_loss: 1.0328 - val_acc: 0.7868\n",
      "Epoch 12/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 1.3152 - acc: 0.7041Epoch 00011: val_loss improved from 1.03281 to 0.95384, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.3158 - acc: 0.7037 - val_loss: 0.9538 - val_acc: 0.7904\n",
      "Epoch 13/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.2147 - acc: 0.7183Epoch 00012: val_loss improved from 0.95384 to 0.89117, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.2164 - acc: 0.7175 - val_loss: 0.8912 - val_acc: 0.8012\n",
      "Epoch 14/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 1.1630 - acc: 0.7280Epoch 00013: val_loss improved from 0.89117 to 0.84010, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.1642 - acc: 0.7280 - val_loss: 0.8401 - val_acc: 0.8060\n",
      "Epoch 15/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 1.1109 - acc: 0.7333Epoch 00014: val_loss improved from 0.84010 to 0.80069, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.1121 - acc: 0.7331 - val_loss: 0.8007 - val_acc: 0.8084\n",
      "Epoch 16/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 1.0465 - acc: 0.7376Epoch 00015: val_loss improved from 0.80069 to 0.76374, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.0461 - acc: 0.7383 - val_loss: 0.7637 - val_acc: 0.8084\n",
      "Epoch 17/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.9828 - acc: 0.7586Epoch 00016: val_loss improved from 0.76374 to 0.73328, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.9846 - acc: 0.7582 - val_loss: 0.7333 - val_acc: 0.8144\n",
      "Epoch 18/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.9553 - acc: 0.7575Epoch 00017: val_loss improved from 0.73328 to 0.70710, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.9540 - acc: 0.7579 - val_loss: 0.7071 - val_acc: 0.8168\n",
      "Epoch 19/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.9137 - acc: 0.7685Epoch 00018: val_loss improved from 0.70710 to 0.68353, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.9127 - acc: 0.7687 - val_loss: 0.6835 - val_acc: 0.8299\n",
      "Epoch 20/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.8827 - acc: 0.7716Epoch 00019: val_loss improved from 0.68353 to 0.66535, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.8798 - acc: 0.7726 - val_loss: 0.6653 - val_acc: 0.8323\n",
      "Epoch 21/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.8658 - acc: 0.7721Epoch 00020: val_loss improved from 0.66535 to 0.64846, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.8679 - acc: 0.7717 - val_loss: 0.6485 - val_acc: 0.8287\n",
      "Epoch 22/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8301 - acc: 0.7865Epoch 00021: val_loss improved from 0.64846 to 0.63307, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.8297 - acc: 0.7865 - val_loss: 0.6331 - val_acc: 0.8383\n",
      "Epoch 23/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8178 - acc: 0.7792Epoch 00022: val_loss improved from 0.63307 to 0.61991, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.8187 - acc: 0.7789 - val_loss: 0.6199 - val_acc: 0.8359\n",
      "Epoch 24/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.7750 - acc: 0.7968Epoch 00023: val_loss improved from 0.61991 to 0.60758, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.7752 - acc: 0.7967 - val_loss: 0.6076 - val_acc: 0.8383\n",
      "Epoch 25/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.7644 - acc: 0.7953Epoch 00024: val_loss improved from 0.60758 to 0.59720, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.7648 - acc: 0.7952 - val_loss: 0.5972 - val_acc: 0.8443\n",
      "Epoch 26/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.7499 - acc: 0.7968Epoch 00025: val_loss improved from 0.59720 to 0.58992, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.7495 - acc: 0.7970 - val_loss: 0.5899 - val_acc: 0.8419\n",
      "Epoch 27/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.7247 - acc: 0.8072Epoch 00026: val_loss improved from 0.58992 to 0.58044, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.7256 - acc: 0.8070 - val_loss: 0.5804 - val_acc: 0.8479\n",
      "Epoch 28/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.7045 - acc: 0.8017Epoch 00027: val_loss improved from 0.58044 to 0.57291, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.7041 - acc: 0.8018 - val_loss: 0.5729 - val_acc: 0.8503\n",
      "Epoch 29/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.6967 - acc: 0.8092Epoch 00028: val_loss improved from 0.57291 to 0.56454, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.6964 - acc: 0.8094 - val_loss: 0.5645 - val_acc: 0.8467\n",
      "Epoch 30/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.6894 - acc: 0.8093Epoch 00029: val_loss improved from 0.56454 to 0.56001, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.6906 - acc: 0.8090 - val_loss: 0.5600 - val_acc: 0.8443\n",
      "Epoch 31/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.6707 - acc: 0.8159Epoch 00030: val_loss improved from 0.56001 to 0.55453, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.6700 - acc: 0.8163 - val_loss: 0.5545 - val_acc: 0.8467\n",
      "Epoch 32/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.6733 - acc: 0.8124Epoch 00031: val_loss improved from 0.55453 to 0.54778, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.6711 - acc: 0.8132 - val_loss: 0.5478 - val_acc: 0.8491\n",
      "Epoch 33/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.6470 - acc: 0.8221Epoch 00032: val_loss improved from 0.54778 to 0.54166, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.6467 - acc: 0.8223 - val_loss: 0.5417 - val_acc: 0.8479\n",
      "Epoch 34/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.6359 - acc: 0.8231Epoch 00033: val_loss improved from 0.54166 to 0.53774, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.6355 - acc: 0.8232 - val_loss: 0.5377 - val_acc: 0.8515\n",
      "Epoch 35/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.6318 - acc: 0.8258Epoch 00034: val_loss improved from 0.53774 to 0.53329, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.6305 - acc: 0.8257 - val_loss: 0.5333 - val_acc: 0.8539\n",
      "Epoch 36/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.6134 - acc: 0.8299Epoch 00035: val_loss improved from 0.53329 to 0.52850, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.6129 - acc: 0.8299 - val_loss: 0.5285 - val_acc: 0.8563\n",
      "Epoch 37/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.5929 - acc: 0.8318Epoch 00036: val_loss improved from 0.52850 to 0.52481, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.5925 - acc: 0.8323 - val_loss: 0.5248 - val_acc: 0.8539\n",
      "Epoch 38/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.6068 - acc: 0.8267Epoch 00037: val_loss improved from 0.52481 to 0.52307, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.6068 - acc: 0.8268 - val_loss: 0.5231 - val_acc: 0.8551\n",
      "Epoch 39/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.5906 - acc: 0.8320Epoch 00038: val_loss improved from 0.52307 to 0.52105, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.5909 - acc: 0.8322 - val_loss: 0.5211 - val_acc: 0.8539\n",
      "Epoch 40/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.5915 - acc: 0.8339Epoch 00039: val_loss improved from 0.52105 to 0.51563, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.5922 - acc: 0.8335 - val_loss: 0.5156 - val_acc: 0.8563\n",
      "Epoch 41/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.5742 - acc: 0.8382Epoch 00040: val_loss improved from 0.51563 to 0.51402, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.5730 - acc: 0.8383 - val_loss: 0.5140 - val_acc: 0.8527\n",
      "Epoch 42/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.5584 - acc: 0.8429Epoch 00041: val_loss improved from 0.51402 to 0.51045, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.5587 - acc: 0.8430 - val_loss: 0.5105 - val_acc: 0.8551\n",
      "Epoch 43/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.5590 - acc: 0.8411Epoch 00042: val_loss improved from 0.51045 to 0.50665, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.5616 - acc: 0.8406 - val_loss: 0.5066 - val_acc: 0.8599\n",
      "Epoch 44/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.5476 - acc: 0.8444Epoch 00043: val_loss improved from 0.50665 to 0.50457, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.5467 - acc: 0.8446 - val_loss: 0.5046 - val_acc: 0.8575\n",
      "Epoch 45/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.5468 - acc: 0.8444Epoch 00044: val_loss improved from 0.50457 to 0.50233, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.5465 - acc: 0.8446 - val_loss: 0.5023 - val_acc: 0.8539\n",
      "Epoch 46/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.5328 - acc: 0.8486Epoch 00045: val_loss improved from 0.50233 to 0.49927, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.5329 - acc: 0.8485 - val_loss: 0.4993 - val_acc: 0.8599\n",
      "Epoch 47/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.5187 - acc: 0.8500Epoch 00046: val_loss improved from 0.49927 to 0.49725, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.5193 - acc: 0.8497 - val_loss: 0.4972 - val_acc: 0.8599\n",
      "Epoch 48/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.5223 - acc: 0.8489Epoch 00047: val_loss improved from 0.49725 to 0.49422, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.5220 - acc: 0.8494 - val_loss: 0.4942 - val_acc: 0.8587\n",
      "Epoch 49/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.5131 - acc: 0.8512Epoch 00048: val_loss improved from 0.49422 to 0.49245, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.5129 - acc: 0.8507 - val_loss: 0.4924 - val_acc: 0.8599\n",
      "Epoch 50/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.5027 - acc: 0.8555Epoch 00049: val_loss improved from 0.49245 to 0.49163, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.5029 - acc: 0.8557 - val_loss: 0.4916 - val_acc: 0.8563\n",
      "Epoch 51/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.5039 - acc: 0.8542Epoch 00050: val_loss improved from 0.49163 to 0.48879, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.5045 - acc: 0.8546 - val_loss: 0.4888 - val_acc: 0.8563\n",
      "Epoch 52/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.4948 - acc: 0.8562Epoch 00051: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.4951 - acc: 0.8563 - val_loss: 0.4890 - val_acc: 0.8611\n",
      "Epoch 53/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.4908 - acc: 0.8538Epoch 00052: val_loss improved from 0.48879 to 0.48505, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.4890 - acc: 0.8543 - val_loss: 0.4850 - val_acc: 0.8563\n",
      "Epoch 54/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.4784 - acc: 0.8628Epoch 00053: val_loss improved from 0.48505 to 0.48290, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.4787 - acc: 0.8630 - val_loss: 0.4829 - val_acc: 0.8611\n",
      "Epoch 55/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.4785 - acc: 0.8596Epoch 00054: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.4785 - acc: 0.8593 - val_loss: 0.4833 - val_acc: 0.8551\n",
      "Epoch 56/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.4737 - acc: 0.8584Epoch 00055: val_loss improved from 0.48290 to 0.48288, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.4770 - acc: 0.8578 - val_loss: 0.4829 - val_acc: 0.8563\n",
      "Epoch 57/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4691 - acc: 0.8639Epoch 00056: val_loss improved from 0.48288 to 0.48057, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.4699 - acc: 0.8639 - val_loss: 0.4806 - val_acc: 0.8599\n",
      "Epoch 58/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.4694 - acc: 0.8609Epoch 00057: val_loss improved from 0.48057 to 0.47977, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.4680 - acc: 0.8612 - val_loss: 0.4798 - val_acc: 0.8611\n",
      "Epoch 59/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4624 - acc: 0.8709Epoch 00058: val_loss improved from 0.47977 to 0.47880, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.4616 - acc: 0.8707 - val_loss: 0.4788 - val_acc: 0.8647\n",
      "Epoch 60/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.4474 - acc: 0.8711Epoch 00059: val_loss improved from 0.47880 to 0.47755, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.4484 - acc: 0.8705 - val_loss: 0.4775 - val_acc: 0.8611\n",
      "Epoch 61/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.4445 - acc: 0.8694Epoch 00060: val_loss improved from 0.47755 to 0.47630, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.4450 - acc: 0.8692 - val_loss: 0.4763 - val_acc: 0.8611\n",
      "Epoch 62/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.4391 - acc: 0.8725Epoch 00061: val_loss improved from 0.47630 to 0.47434, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.4391 - acc: 0.8725 - val_loss: 0.4743 - val_acc: 0.8599\n",
      "Epoch 63/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.4408 - acc: 0.8686Epoch 00062: val_loss improved from 0.47434 to 0.47401, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.4410 - acc: 0.8686 - val_loss: 0.4740 - val_acc: 0.8611\n",
      "Epoch 64/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.4371 - acc: 0.8693Epoch 00063: val_loss improved from 0.47401 to 0.47181, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.4376 - acc: 0.8687 - val_loss: 0.4718 - val_acc: 0.8635\n",
      "Epoch 65/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.4317 - acc: 0.8717Epoch 00064: val_loss improved from 0.47181 to 0.47035, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.4318 - acc: 0.8720 - val_loss: 0.4703 - val_acc: 0.8623\n",
      "Epoch 66/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.4275 - acc: 0.8758Epoch 00065: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.4289 - acc: 0.8754 - val_loss: 0.4712 - val_acc: 0.8611\n",
      "Epoch 67/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.4278 - acc: 0.8728Epoch 00066: val_loss improved from 0.47035 to 0.46902, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.4272 - acc: 0.8729 - val_loss: 0.4690 - val_acc: 0.8599\n",
      "Epoch 68/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.4210 - acc: 0.8758Epoch 00067: val_loss improved from 0.46902 to 0.46845, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.4210 - acc: 0.8759 - val_loss: 0.4684 - val_acc: 0.8563\n",
      "Epoch 69/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.4148 - acc: 0.8789Epoch 00068: val_loss improved from 0.46845 to 0.46771, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.4149 - acc: 0.8787 - val_loss: 0.4677 - val_acc: 0.8575\n",
      "Epoch 70/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4113 - acc: 0.8812Epoch 00069: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.4126 - acc: 0.8799 - val_loss: 0.4695 - val_acc: 0.8575\n",
      "Epoch 71/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4112 - acc: 0.8759Epoch 00070: val_loss improved from 0.46771 to 0.46749, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.4121 - acc: 0.8754 - val_loss: 0.4675 - val_acc: 0.8575\n",
      "Epoch 72/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4073 - acc: 0.8824Epoch 00071: val_loss improved from 0.46749 to 0.46687, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.4076 - acc: 0.8819 - val_loss: 0.4669 - val_acc: 0.8587\n",
      "Epoch 73/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.4032 - acc: 0.8800Epoch 00072: val_loss improved from 0.46687 to 0.46666, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.4033 - acc: 0.8799 - val_loss: 0.4667 - val_acc: 0.8575\n",
      "Epoch 74/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.3901 - acc: 0.8896Epoch 00073: val_loss improved from 0.46666 to 0.46459, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.3915 - acc: 0.8891 - val_loss: 0.4646 - val_acc: 0.8623\n",
      "Epoch 75/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.3969 - acc: 0.8842Epoch 00074: val_loss improved from 0.46459 to 0.46384, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.3959 - acc: 0.8846 - val_loss: 0.4638 - val_acc: 0.8635\n",
      "Epoch 76/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.3930 - acc: 0.8853Epoch 00075: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.3922 - acc: 0.8859 - val_loss: 0.4656 - val_acc: 0.8611\n",
      "Epoch 77/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.3865 - acc: 0.8869Epoch 00076: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.3880 - acc: 0.8867 - val_loss: 0.4654 - val_acc: 0.8575\n",
      "Epoch 78/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.3872 - acc: 0.8866Epoch 00077: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.3869 - acc: 0.8865 - val_loss: 0.4648 - val_acc: 0.8599\n",
      "Epoch 79/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.3794 - acc: 0.8893Epoch 00078: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.3797 - acc: 0.8888 - val_loss: 0.4640 - val_acc: 0.8587\n",
      "Epoch 80/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3781 - acc: 0.8862Epoch 00079: val_loss improved from 0.46384 to 0.46277, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.3784 - acc: 0.8862 - val_loss: 0.4628 - val_acc: 0.8575\n",
      "Epoch 81/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.3815 - acc: 0.8873Epoch 00080: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.3807 - acc: 0.8874 - val_loss: 0.4639 - val_acc: 0.8587\n",
      "Epoch 82/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3866 - acc: 0.8852Epoch 00081: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.3868 - acc: 0.8849 - val_loss: 0.4634 - val_acc: 0.8611\n",
      "Epoch 83/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.3803 - acc: 0.8866Epoch 00082: val_loss improved from 0.46277 to 0.46256, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.3802 - acc: 0.8864 - val_loss: 0.4626 - val_acc: 0.8599\n",
      "Epoch 84/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.3694 - acc: 0.8914Epoch 00083: val_loss improved from 0.46256 to 0.46246, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.3692 - acc: 0.8915 - val_loss: 0.4625 - val_acc: 0.8599\n",
      "Epoch 85/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3643 - acc: 0.8938Epoch 00084: val_loss improved from 0.46246 to 0.46119, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.3642 - acc: 0.8937 - val_loss: 0.4612 - val_acc: 0.8587\n",
      "Epoch 86/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.3650 - acc: 0.8908Epoch 00085: val_loss improved from 0.46119 to 0.46005, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.3653 - acc: 0.8907 - val_loss: 0.4601 - val_acc: 0.8587\n",
      "Epoch 87/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.3583 - acc: 0.8937Epoch 00086: val_loss improved from 0.46005 to 0.45974, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.3579 - acc: 0.8939 - val_loss: 0.4597 - val_acc: 0.8599\n",
      "Epoch 88/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3567 - acc: 0.8942Epoch 00087: val_loss improved from 0.45974 to 0.45820, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.3563 - acc: 0.8948 - val_loss: 0.4582 - val_acc: 0.8563\n",
      "Epoch 89/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.3516 - acc: 0.8980Epoch 00088: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.3516 - acc: 0.8979 - val_loss: 0.4595 - val_acc: 0.8599\n",
      "Epoch 90/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3569 - acc: 0.8929Epoch 00089: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.3575 - acc: 0.8925 - val_loss: 0.4590 - val_acc: 0.8587\n",
      "Epoch 91/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3421 - acc: 0.8979Epoch 00090: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.3441 - acc: 0.8970 - val_loss: 0.4591 - val_acc: 0.8623\n",
      "Epoch 92/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3441 - acc: 0.8964Epoch 00091: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.3449 - acc: 0.8961 - val_loss: 0.4584 - val_acc: 0.8587\n",
      "Epoch 93/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.3453 - acc: 0.8968Epoch 00092: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.3458 - acc: 0.8963 - val_loss: 0.4585 - val_acc: 0.8587\n",
      "Epoch 94/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.3374 - acc: 0.9014Epoch 00093: val_loss improved from 0.45820 to 0.45818, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.3383 - acc: 0.9012 - val_loss: 0.4582 - val_acc: 0.8575\n",
      "Epoch 95/100\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.3383 - acc: 0.8970Epoch 00094: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.3390 - acc: 0.8969 - val_loss: 0.4607 - val_acc: 0.8563\n",
      "Epoch 96/100\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.3353 - acc: 0.9015Epoch 00095: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.3347 - acc: 0.9018 - val_loss: 0.4591 - val_acc: 0.8611\n",
      "Epoch 97/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.3312 - acc: 0.9024Epoch 00096: val_loss improved from 0.45818 to 0.45740, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.3304 - acc: 0.9027 - val_loss: 0.4574 - val_acc: 0.8623\n",
      "Epoch 98/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.3350 - acc: 0.8997Epoch 00097: val_loss improved from 0.45740 to 0.45633, saving model to saved_models/weights.best.Inc.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.3353 - acc: 0.8996 - val_loss: 0.4563 - val_acc: 0.8611\n",
      "Epoch 99/100\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.3297 - acc: 0.9008Epoch 00098: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.3296 - acc: 0.9007 - val_loss: 0.4568 - val_acc: 0.8611\n",
      "Epoch 100/100\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3260 - acc: 0.9035Epoch 00099: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.3254 - acc: 0.9036 - val_loss: 0.4567 - val_acc: 0.8587\n"
     ]
    }
   ],
   "source": [
    "#######################################       Inception Implementation       ####################################################\n",
    "\n",
    "### TODO: Obtain bottleneck features from another pre-trained CNN.\n",
    "bottleneck_features = np.load('bottleneck_features/DogInceptionV3Data.npz')\n",
    "train_Inc = bottleneck_features['train']\n",
    "valid_Inc = bottleneck_features['valid']\n",
    "test_Inc = bottleneck_features['test']\n",
    "\n",
    "### TODO: Define your architecture.\n",
    "Inc_model = Sequential()\n",
    "Inc_model.add(GlobalAveragePooling2D(input_shape=train_Inc.shape[1:]))\n",
    "Inc_model.add(Dense(512, activation='relu'))\n",
    "Inc_model.add(Dropout(0.5))\n",
    "Inc_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "Inc_model.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=1e-4, momentum=0.9), metrics=['accuracy'])\n",
    "\n",
    " \n",
    "### TODO: Train the model.\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.Inc.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "Inc_model.fit(train_Inc, train_targets, \n",
    "          validation_data=(valid_Inc, valid_targets),\n",
    "          epochs=100, batch_size=20, callbacks=[checkpointer], verbose=1)\n",
    "### TODO: Load the model weights with the best validation loss.\n",
    "Res_model.load_weights('saved_models/weights.best.Inc.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "nbpresent": {
     "id": "4e23d757-aa9f-423f-98c7-c8183e23cda4"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 82.7751%\n"
     ]
    }
   ],
   "source": [
    "### TODO: Calculate classification accuracy on the test dataset.\n",
    "# get index of predicted dog breed for each image in test set\n",
    "Inc_predictions = [np.argmax(Inc_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Inc]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(Inc_predictions)==np.argmax(test_targets, axis=1))/len(Inc_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "nbpresent": {
     "id": "6e6e0bc3-bfbd-4612-92c5-d0a931482eda"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 5.4410 - acc: 0.0073Epoch 00000: val_loss improved from inf to 4.90084, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 5.4348 - acc: 0.0072 - val_loss: 4.9008 - val_acc: 0.0180\n",
      "Epoch 2/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 5.0413 - acc: 0.0185Epoch 00001: val_loss improved from 4.90084 to 4.66500, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 5.0383 - acc: 0.0184 - val_loss: 4.6650 - val_acc: 0.0479\n",
      "Epoch 3/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 4.7700 - acc: 0.0368Epoch 00002: val_loss improved from 4.66500 to 4.48842, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 4.7686 - acc: 0.0370 - val_loss: 4.4884 - val_acc: 0.0886\n",
      "Epoch 4/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 4.5900 - acc: 0.0536Epoch 00003: val_loss improved from 4.48842 to 4.33935, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 4.5894 - acc: 0.0536 - val_loss: 4.3394 - val_acc: 0.1353\n",
      "Epoch 5/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 4.4249 - acc: 0.0711Epoch 00004: val_loss improved from 4.33935 to 4.19835, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 4.4284 - acc: 0.0711 - val_loss: 4.1984 - val_acc: 0.1964\n",
      "Epoch 6/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 4.2696 - acc: 0.1138Epoch 00005: val_loss improved from 4.19835 to 4.06003, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 4.2699 - acc: 0.1132 - val_loss: 4.0600 - val_acc: 0.2323\n",
      "Epoch 7/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 4.1405 - acc: 0.1373Epoch 00006: val_loss improved from 4.06003 to 3.92115, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 4.1385 - acc: 0.1379 - val_loss: 3.9211 - val_acc: 0.2707\n",
      "Epoch 8/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 4.0038 - acc: 0.1574Epoch 00007: val_loss improved from 3.92115 to 3.77994, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 4.0022 - acc: 0.1579 - val_loss: 3.7799 - val_acc: 0.3114\n",
      "Epoch 9/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 3.8810 - acc: 0.1880Epoch 00008: val_loss improved from 3.77994 to 3.63853, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 3.8822 - acc: 0.1880 - val_loss: 3.6385 - val_acc: 0.3473\n",
      "Epoch 10/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 3.7510 - acc: 0.2195Epoch 00009: val_loss improved from 3.63853 to 3.49866, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 3.7508 - acc: 0.2184 - val_loss: 3.4987 - val_acc: 0.3868\n",
      "Epoch 11/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 3.6201 - acc: 0.2359Epoch 00010: val_loss improved from 3.49866 to 3.35831, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 3.6167 - acc: 0.2376 - val_loss: 3.3583 - val_acc: 0.4132\n",
      "Epoch 12/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 3.4822 - acc: 0.2707Epoch 00011: val_loss improved from 3.35831 to 3.22047, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 3.4821 - acc: 0.2702 - val_loss: 3.2205 - val_acc: 0.4455\n",
      "Epoch 13/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 3.3600 - acc: 0.2878Epoch 00012: val_loss improved from 3.22047 to 3.08839, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 3.3595 - acc: 0.2880 - val_loss: 3.0884 - val_acc: 0.4611\n",
      "Epoch 14/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 3.2471 - acc: 0.3051Epoch 00013: val_loss improved from 3.08839 to 2.95959, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 3.2426 - acc: 0.3054 - val_loss: 2.9596 - val_acc: 0.4862\n",
      "Epoch 15/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 3.1176 - acc: 0.3298Epoch 00014: val_loss improved from 2.95959 to 2.83499, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 3.1180 - acc: 0.3296 - val_loss: 2.8350 - val_acc: 0.5054\n",
      "Epoch 16/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 3.0070 - acc: 0.3481Epoch 00015: val_loss improved from 2.83499 to 2.71747, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 3.0085 - acc: 0.3487 - val_loss: 2.7175 - val_acc: 0.5210\n",
      "Epoch 17/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 2.8876 - acc: 0.3694Epoch 00016: val_loss improved from 2.71747 to 2.60579, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 2.8903 - acc: 0.3689 - val_loss: 2.6058 - val_acc: 0.5377\n",
      "Epoch 18/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 2.8045 - acc: 0.3839Epoch 00017: val_loss improved from 2.60579 to 2.50215, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 2.8040 - acc: 0.3837 - val_loss: 2.5022 - val_acc: 0.5545\n",
      "Epoch 19/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 2.7266 - acc: 0.3940Epoch 00018: val_loss improved from 2.50215 to 2.40416, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 2.7282 - acc: 0.3931 - val_loss: 2.4042 - val_acc: 0.5689\n",
      "Epoch 20/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 2.5963 - acc: 0.4261Epoch 00019: val_loss improved from 2.40416 to 2.31076, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 2.5966 - acc: 0.4256 - val_loss: 2.3108 - val_acc: 0.5892\n",
      "Epoch 21/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 2.5365 - acc: 0.4317Epoch 00020: val_loss improved from 2.31076 to 2.22387, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 2.5355 - acc: 0.4314 - val_loss: 2.2239 - val_acc: 0.5988\n",
      "Epoch 22/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 2.4455 - acc: 0.4479Epoch 00021: val_loss improved from 2.22387 to 2.14198, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 2.4457 - acc: 0.4485 - val_loss: 2.1420 - val_acc: 0.6144\n",
      "Epoch 23/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 2.3934 - acc: 0.4553Epoch 00022: val_loss improved from 2.14198 to 2.06643, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 2.3928 - acc: 0.4548 - val_loss: 2.0664 - val_acc: 0.6287\n",
      "Epoch 24/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 2.3212 - acc: 0.4678Epoch 00023: val_loss improved from 2.06643 to 1.99622, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 2.3162 - acc: 0.4689 - val_loss: 1.9962 - val_acc: 0.6443\n",
      "Epoch 25/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 2.2452 - acc: 0.4824Epoch 00024: val_loss improved from 1.99622 to 1.92984, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 2.2439 - acc: 0.4828 - val_loss: 1.9298 - val_acc: 0.6539\n",
      "Epoch 26/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 2.1937 - acc: 0.4829Epoch 00025: val_loss improved from 1.92984 to 1.86797, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 2.1918 - acc: 0.4837 - val_loss: 1.8680 - val_acc: 0.6551\n",
      "Epoch 27/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 2.1045 - acc: 0.5043Epoch 00026: val_loss improved from 1.86797 to 1.80821, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 2.1044 - acc: 0.5036 - val_loss: 1.8082 - val_acc: 0.6671\n",
      "Epoch 28/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 2.0753 - acc: 0.5191Epoch 00027: val_loss improved from 1.80821 to 1.75362, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 2.0743 - acc: 0.5195 - val_loss: 1.7536 - val_acc: 0.6802\n",
      "Epoch 29/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 2.0075 - acc: 0.5223Epoch 00028: val_loss improved from 1.75362 to 1.70096, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 2.0070 - acc: 0.5219 - val_loss: 1.7010 - val_acc: 0.6898\n",
      "Epoch 30/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.9558 - acc: 0.5286Epoch 00029: val_loss improved from 1.70096 to 1.65237, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.9560 - acc: 0.5287 - val_loss: 1.6524 - val_acc: 0.6934\n",
      "Epoch 31/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 1.9059 - acc: 0.5446Epoch 00030: val_loss improved from 1.65237 to 1.60730, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.9043 - acc: 0.5446 - val_loss: 1.6073 - val_acc: 0.6994\n",
      "Epoch 32/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.8786 - acc: 0.5538Epoch 00031: val_loss improved from 1.60730 to 1.56475, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.8775 - acc: 0.5543 - val_loss: 1.5647 - val_acc: 0.7006\n",
      "Epoch 33/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.8236 - acc: 0.5611Epoch 00032: val_loss improved from 1.56475 to 1.52421, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.8213 - acc: 0.5620 - val_loss: 1.5242 - val_acc: 0.7042\n",
      "Epoch 34/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.7854 - acc: 0.5635Epoch 00033: val_loss improved from 1.52421 to 1.48648, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.7826 - acc: 0.5645 - val_loss: 1.4865 - val_acc: 0.7150\n",
      "Epoch 35/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.7256 - acc: 0.5750Epoch 00034: val_loss improved from 1.48648 to 1.45151, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.7272 - acc: 0.5737 - val_loss: 1.4515 - val_acc: 0.7150\n",
      "Epoch 36/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.6784 - acc: 0.5868Epoch 00035: val_loss improved from 1.45151 to 1.41610, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.6759 - acc: 0.5871 - val_loss: 1.4161 - val_acc: 0.7210\n",
      "Epoch 37/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 1.6510 - acc: 0.5905Epoch 00036: val_loss improved from 1.41610 to 1.38448, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.6504 - acc: 0.5900 - val_loss: 1.3845 - val_acc: 0.7246\n",
      "Epoch 38/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 1.5955 - acc: 0.6041Epoch 00037: val_loss improved from 1.38448 to 1.35396, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.5958 - acc: 0.6037 - val_loss: 1.3540 - val_acc: 0.7257\n",
      "Epoch 39/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 1.6011 - acc: 0.5998Epoch 00038: val_loss improved from 1.35396 to 1.32528, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.6005 - acc: 0.6000 - val_loss: 1.3253 - val_acc: 0.7269\n",
      "Epoch 40/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 1.5723 - acc: 0.6056Epoch 00039: val_loss improved from 1.32528 to 1.29923, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.5686 - acc: 0.6066 - val_loss: 1.2992 - val_acc: 0.7293\n",
      "Epoch 41/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 1.5405 - acc: 0.6119Epoch 00040: val_loss improved from 1.29923 to 1.27335, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.5384 - acc: 0.6123 - val_loss: 1.2734 - val_acc: 0.7317\n",
      "Epoch 42/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 1.5003 - acc: 0.6174Epoch 00041: val_loss improved from 1.27335 to 1.24850, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.5026 - acc: 0.6168 - val_loss: 1.2485 - val_acc: 0.7341\n",
      "Epoch 43/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.4573 - acc: 0.6329Epoch 00042: val_loss improved from 1.24850 to 1.22676, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.4583 - acc: 0.6326 - val_loss: 1.2268 - val_acc: 0.7365\n",
      "Epoch 44/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.4348 - acc: 0.6364Epoch 00043: val_loss improved from 1.22676 to 1.20576, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.4373 - acc: 0.6355 - val_loss: 1.2058 - val_acc: 0.7365\n",
      "Epoch 45/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 1.4029 - acc: 0.6451Epoch 00044: val_loss improved from 1.20576 to 1.18416, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.4030 - acc: 0.6460 - val_loss: 1.1842 - val_acc: 0.7377\n",
      "Epoch 46/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 1.4078 - acc: 0.6342Epoch 00045: val_loss improved from 1.18416 to 1.16544, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.4038 - acc: 0.6361 - val_loss: 1.1654 - val_acc: 0.7413\n",
      "Epoch 47/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 1.3684 - acc: 0.6508Epoch 00046: val_loss improved from 1.16544 to 1.14786, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.3687 - acc: 0.6506 - val_loss: 1.1479 - val_acc: 0.7437\n",
      "Epoch 48/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.3418 - acc: 0.6564Epoch 00047: val_loss improved from 1.14786 to 1.12976, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.3412 - acc: 0.6560 - val_loss: 1.1298 - val_acc: 0.7461\n",
      "Epoch 49/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.3381 - acc: 0.6505Epoch 00048: val_loss improved from 1.12976 to 1.11408, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.3370 - acc: 0.6509 - val_loss: 1.1141 - val_acc: 0.7437\n",
      "Epoch 50/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 1.3144 - acc: 0.6562Epoch 00049: val_loss improved from 1.11408 to 1.09769, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.3123 - acc: 0.6569 - val_loss: 1.0977 - val_acc: 0.7533\n",
      "Epoch 51/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 1.2996 - acc: 0.6589Epoch 00050: val_loss improved from 1.09769 to 1.08351, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.2996 - acc: 0.6588 - val_loss: 1.0835 - val_acc: 0.7557\n",
      "Epoch 52/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.2743 - acc: 0.6683Epoch 00051: val_loss improved from 1.08351 to 1.06916, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.2742 - acc: 0.6677 - val_loss: 1.0692 - val_acc: 0.7569\n",
      "Epoch 53/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.2393 - acc: 0.6824Epoch 00052: val_loss improved from 1.06916 to 1.05474, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.2406 - acc: 0.6822 - val_loss: 1.0547 - val_acc: 0.7593\n",
      "Epoch 54/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.2138 - acc: 0.6824Epoch 00053: val_loss improved from 1.05474 to 1.04045, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.2162 - acc: 0.6822 - val_loss: 1.0404 - val_acc: 0.7605\n",
      "Epoch 55/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 1.2277 - acc: 0.6744Epoch 00054: val_loss improved from 1.04045 to 1.02804, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.2290 - acc: 0.6741 - val_loss: 1.0280 - val_acc: 0.7605\n",
      "Epoch 56/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.1964 - acc: 0.6895Epoch 00055: val_loss improved from 1.02804 to 1.01546, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.1976 - acc: 0.6886 - val_loss: 1.0155 - val_acc: 0.7593\n",
      "Epoch 57/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 1.1782 - acc: 0.6889Epoch 00056: val_loss improved from 1.01546 to 1.00430, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.1766 - acc: 0.6895 - val_loss: 1.0043 - val_acc: 0.7641\n",
      "Epoch 58/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 1.1639 - acc: 0.6918Epoch 00057: val_loss improved from 1.00430 to 0.99317, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.1630 - acc: 0.6927 - val_loss: 0.9932 - val_acc: 0.7665\n",
      "Epoch 59/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.1636 - acc: 0.6886Epoch 00058: val_loss improved from 0.99317 to 0.98150, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.1608 - acc: 0.6898 - val_loss: 0.9815 - val_acc: 0.7689\n",
      "Epoch 60/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.1439 - acc: 0.6935Epoch 00059: val_loss improved from 0.98150 to 0.97210, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.1467 - acc: 0.6930 - val_loss: 0.9721 - val_acc: 0.7701\n",
      "Epoch 61/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.1205 - acc: 0.7058Epoch 00060: val_loss improved from 0.97210 to 0.96149, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.1188 - acc: 0.7063 - val_loss: 0.9615 - val_acc: 0.7725\n",
      "Epoch 62/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.0946 - acc: 0.7105Epoch 00061: val_loss improved from 0.96149 to 0.95065, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.0979 - acc: 0.7091 - val_loss: 0.9507 - val_acc: 0.7760\n",
      "Epoch 63/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 1.0862 - acc: 0.7158Epoch 00062: val_loss improved from 0.95065 to 0.94120, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.0869 - acc: 0.7150 - val_loss: 0.9412 - val_acc: 0.7760\n",
      "Epoch 64/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 1.0672 - acc: 0.7183Epoch 00063: val_loss improved from 0.94120 to 0.93227, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.0664 - acc: 0.7186 - val_loss: 0.9323 - val_acc: 0.7772\n",
      "Epoch 65/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 1.0631 - acc: 0.7200Epoch 00064: val_loss improved from 0.93227 to 0.92406, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.0638 - acc: 0.7196 - val_loss: 0.9241 - val_acc: 0.7820\n",
      "Epoch 66/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 1.0478 - acc: 0.7209Epoch 00065: val_loss improved from 0.92406 to 0.91606, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.0487 - acc: 0.7199 - val_loss: 0.9161 - val_acc: 0.7844\n",
      "Epoch 67/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 1.0318 - acc: 0.7269Epoch 00066: val_loss improved from 0.91606 to 0.90684, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.0309 - acc: 0.7274 - val_loss: 0.9068 - val_acc: 0.7856\n",
      "Epoch 68/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 1.0427 - acc: 0.7223Epoch 00067: val_loss improved from 0.90684 to 0.89969, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.0412 - acc: 0.7228 - val_loss: 0.8997 - val_acc: 0.7844\n",
      "Epoch 69/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.9969 - acc: 0.7338Epoch 00068: val_loss improved from 0.89969 to 0.89196, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.9963 - acc: 0.7334 - val_loss: 0.8920 - val_acc: 0.7856\n",
      "Epoch 70/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 1.0120 - acc: 0.7256Epoch 00069: val_loss improved from 0.89196 to 0.88492, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.0117 - acc: 0.7259 - val_loss: 0.8849 - val_acc: 0.7868\n",
      "Epoch 71/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 1.0043 - acc: 0.7245Epoch 00070: val_loss improved from 0.88492 to 0.87829, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.0045 - acc: 0.7243 - val_loss: 0.8783 - val_acc: 0.7856\n",
      "Epoch 72/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.9870 - acc: 0.7320Epoch 00071: val_loss improved from 0.87829 to 0.87086, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.9862 - acc: 0.7326 - val_loss: 0.8709 - val_acc: 0.7868\n",
      "Epoch 73/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.9712 - acc: 0.7403Epoch 00072: val_loss improved from 0.87086 to 0.86434, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.9696 - acc: 0.7406 - val_loss: 0.8643 - val_acc: 0.7916\n",
      "Epoch 74/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.9559 - acc: 0.7411Epoch 00073: val_loss improved from 0.86434 to 0.85780, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.9585 - acc: 0.7403 - val_loss: 0.8578 - val_acc: 0.7916\n",
      "Epoch 75/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.9476 - acc: 0.7443Epoch 00074: val_loss improved from 0.85780 to 0.85028, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.9442 - acc: 0.7451 - val_loss: 0.8503 - val_acc: 0.7928\n",
      "Epoch 76/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.9373 - acc: 0.7412Epoch 00075: val_loss improved from 0.85028 to 0.84392, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.9380 - acc: 0.7413 - val_loss: 0.8439 - val_acc: 0.7928\n",
      "Epoch 77/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.9589 - acc: 0.7388Epoch 00076: val_loss improved from 0.84392 to 0.83808, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.9579 - acc: 0.7391 - val_loss: 0.8381 - val_acc: 0.7952\n",
      "Epoch 78/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.9262 - acc: 0.7497Epoch 00077: val_loss improved from 0.83808 to 0.83205, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.9247 - acc: 0.7501 - val_loss: 0.8320 - val_acc: 0.7928\n",
      "Epoch 79/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.9285 - acc: 0.7483Epoch 00078: val_loss improved from 0.83205 to 0.82783, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.9295 - acc: 0.7488 - val_loss: 0.8278 - val_acc: 0.7988\n",
      "Epoch 80/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.9135 - acc: 0.7462Epoch 00079: val_loss improved from 0.82783 to 0.82296, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.9148 - acc: 0.7461 - val_loss: 0.8230 - val_acc: 0.7976\n",
      "Epoch 81/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.9046 - acc: 0.7579Epoch 00080: val_loss improved from 0.82296 to 0.81679, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.9040 - acc: 0.7579 - val_loss: 0.8168 - val_acc: 0.8000\n",
      "Epoch 82/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.8875 - acc: 0.7624Epoch 00081: val_loss improved from 0.81679 to 0.81151, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.8878 - acc: 0.7620 - val_loss: 0.8115 - val_acc: 0.8036\n",
      "Epoch 83/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.8730 - acc: 0.7591Epoch 00082: val_loss improved from 0.81151 to 0.80569, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.8725 - acc: 0.7587 - val_loss: 0.8057 - val_acc: 0.8048\n",
      "Epoch 84/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.8801 - acc: 0.7632Epoch 00083: val_loss improved from 0.80569 to 0.80094, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.8799 - acc: 0.7632 - val_loss: 0.8009 - val_acc: 0.8048\n",
      "Epoch 85/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8685 - acc: 0.7629Epoch 00084: val_loss improved from 0.80094 to 0.79693, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.8676 - acc: 0.7632 - val_loss: 0.7969 - val_acc: 0.8060\n",
      "Epoch 86/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.8632 - acc: 0.7614Epoch 00085: val_loss improved from 0.79693 to 0.79234, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.8622 - acc: 0.7620 - val_loss: 0.7923 - val_acc: 0.8120\n",
      "Epoch 87/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8501 - acc: 0.7720Epoch 00086: val_loss improved from 0.79234 to 0.78762, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.8498 - acc: 0.7722 - val_loss: 0.7876 - val_acc: 0.8060\n",
      "Epoch 88/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.8342 - acc: 0.7752Epoch 00087: val_loss improved from 0.78762 to 0.78288, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.8349 - acc: 0.7753 - val_loss: 0.7829 - val_acc: 0.8132\n",
      "Epoch 89/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.8246 - acc: 0.7690Epoch 00088: val_loss improved from 0.78288 to 0.77904, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.8245 - acc: 0.7690 - val_loss: 0.7790 - val_acc: 0.8084\n",
      "Epoch 90/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.8117 - acc: 0.7864Epoch 00089: val_loss improved from 0.77904 to 0.77404, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.8137 - acc: 0.7853 - val_loss: 0.7740 - val_acc: 0.8096\n",
      "Epoch 91/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.8332 - acc: 0.7725Epoch 00090: val_loss improved from 0.77404 to 0.77023, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.8332 - acc: 0.7723 - val_loss: 0.7702 - val_acc: 0.8156\n",
      "Epoch 92/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8247 - acc: 0.7714Epoch 00091: val_loss improved from 0.77023 to 0.76546, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.8235 - acc: 0.7717 - val_loss: 0.7655 - val_acc: 0.8120\n",
      "Epoch 93/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.7953 - acc: 0.7789Epoch 00092: val_loss improved from 0.76546 to 0.76245, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.7991 - acc: 0.7771 - val_loss: 0.7624 - val_acc: 0.8120\n",
      "Epoch 94/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.7987 - acc: 0.7794Epoch 00093: val_loss improved from 0.76245 to 0.75793, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.7980 - acc: 0.7808 - val_loss: 0.7579 - val_acc: 0.8180\n",
      "Epoch 95/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.7941 - acc: 0.7808Epoch 00094: val_loss improved from 0.75793 to 0.75446, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.7963 - acc: 0.7801 - val_loss: 0.7545 - val_acc: 0.8132\n",
      "Epoch 96/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.7968 - acc: 0.7771Epoch 00095: val_loss improved from 0.75446 to 0.75211, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.7969 - acc: 0.7772 - val_loss: 0.7521 - val_acc: 0.8156\n",
      "Epoch 97/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.7926 - acc: 0.7747Epoch 00096: val_loss improved from 0.75211 to 0.74819, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.7937 - acc: 0.7743 - val_loss: 0.7482 - val_acc: 0.8156\n",
      "Epoch 98/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.7626 - acc: 0.7911Epoch 00097: val_loss improved from 0.74819 to 0.74389, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.7661 - acc: 0.7897 - val_loss: 0.7439 - val_acc: 0.8192\n",
      "Epoch 99/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.7734 - acc: 0.7889Epoch 00098: val_loss improved from 0.74389 to 0.74078, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.7733 - acc: 0.7888 - val_loss: 0.7408 - val_acc: 0.8192\n",
      "Epoch 100/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.7664 - acc: 0.7871Epoch 00099: val_loss improved from 0.74078 to 0.73725, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.7669 - acc: 0.7862 - val_loss: 0.7373 - val_acc: 0.8192\n",
      "Epoch 101/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.7697 - acc: 0.7897Epoch 00100: val_loss improved from 0.73725 to 0.73414, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.7678 - acc: 0.7901 - val_loss: 0.7341 - val_acc: 0.8132\n",
      "Epoch 102/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.7392 - acc: 0.7980Epoch 00101: val_loss improved from 0.73414 to 0.73086, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.7411 - acc: 0.7978 - val_loss: 0.7309 - val_acc: 0.8168\n",
      "Epoch 103/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.7395 - acc: 0.7954Epoch 00102: val_loss improved from 0.73086 to 0.72736, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.7384 - acc: 0.7957 - val_loss: 0.7274 - val_acc: 0.8216\n",
      "Epoch 104/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.7443 - acc: 0.7971Epoch 00103: val_loss improved from 0.72736 to 0.72357, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.7453 - acc: 0.7969 - val_loss: 0.7236 - val_acc: 0.8216\n",
      "Epoch 105/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.7299 - acc: 0.7950Epoch 00104: val_loss improved from 0.72357 to 0.72065, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.7296 - acc: 0.7946 - val_loss: 0.7206 - val_acc: 0.8287\n",
      "Epoch 106/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.7149 - acc: 0.8043Epoch 00105: val_loss improved from 0.72065 to 0.71753, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.7161 - acc: 0.8045 - val_loss: 0.7175 - val_acc: 0.8204\n",
      "Epoch 107/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.7302 - acc: 0.8017Epoch 00106: val_loss improved from 0.71753 to 0.71546, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.7297 - acc: 0.8018 - val_loss: 0.7155 - val_acc: 0.8192\n",
      "Epoch 108/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.7103 - acc: 0.8027Epoch 00107: val_loss improved from 0.71546 to 0.71189, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.7095 - acc: 0.8031 - val_loss: 0.7119 - val_acc: 0.8228\n",
      "Epoch 109/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.7104 - acc: 0.8020Epoch 00108: val_loss improved from 0.71189 to 0.70936, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.7089 - acc: 0.8028 - val_loss: 0.7094 - val_acc: 0.8251\n",
      "Epoch 110/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.7048 - acc: 0.8067Epoch 00109: val_loss improved from 0.70936 to 0.70676, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.7032 - acc: 0.8072 - val_loss: 0.7068 - val_acc: 0.8204\n",
      "Epoch 111/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.7188 - acc: 0.8027Epoch 00110: val_loss improved from 0.70676 to 0.70464, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.7173 - acc: 0.8033 - val_loss: 0.7046 - val_acc: 0.8251\n",
      "Epoch 112/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.7034 - acc: 0.8070Epoch 00111: val_loss improved from 0.70464 to 0.70239, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.7019 - acc: 0.8070 - val_loss: 0.7024 - val_acc: 0.8228\n",
      "Epoch 113/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.6842 - acc: 0.8118Epoch 00112: val_loss improved from 0.70239 to 0.69946, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.6854 - acc: 0.8117 - val_loss: 0.6995 - val_acc: 0.8251\n",
      "Epoch 114/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.6934 - acc: 0.8071Epoch 00113: val_loss improved from 0.69946 to 0.69706, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.6926 - acc: 0.8072 - val_loss: 0.6971 - val_acc: 0.8287\n",
      "Epoch 115/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.6999 - acc: 0.8032Epoch 00114: val_loss improved from 0.69706 to 0.69491, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.7002 - acc: 0.8028 - val_loss: 0.6949 - val_acc: 0.8299\n",
      "Epoch 116/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.6899 - acc: 0.8047Epoch 00115: val_loss improved from 0.69491 to 0.69242, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.6890 - acc: 0.8051 - val_loss: 0.6924 - val_acc: 0.8311\n",
      "Epoch 117/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.6760 - acc: 0.8123Epoch 00116: val_loss improved from 0.69242 to 0.69081, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.6768 - acc: 0.8121 - val_loss: 0.6908 - val_acc: 0.8299\n",
      "Epoch 118/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.6790 - acc: 0.8099Epoch 00117: val_loss improved from 0.69081 to 0.68935, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.6793 - acc: 0.8099 - val_loss: 0.6894 - val_acc: 0.8299\n",
      "Epoch 119/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.6748 - acc: 0.8134Epoch 00118: val_loss improved from 0.68935 to 0.68604, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.6725 - acc: 0.8138 - val_loss: 0.6860 - val_acc: 0.8275\n",
      "Epoch 120/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.6526 - acc: 0.8238Epoch 00119: val_loss improved from 0.68604 to 0.68327, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.6503 - acc: 0.8249 - val_loss: 0.6833 - val_acc: 0.8263\n",
      "Epoch 121/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.6666 - acc: 0.8147Epoch 00120: val_loss improved from 0.68327 to 0.68130, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.6665 - acc: 0.8147 - val_loss: 0.6813 - val_acc: 0.8299\n",
      "Epoch 122/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.6489 - acc: 0.8232Epoch 00121: val_loss improved from 0.68130 to 0.67927, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.6494 - acc: 0.8234 - val_loss: 0.6793 - val_acc: 0.8287\n",
      "Epoch 123/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.6808 - acc: 0.8069Epoch 00122: val_loss improved from 0.67927 to 0.67701, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.6804 - acc: 0.8070 - val_loss: 0.6770 - val_acc: 0.8299\n",
      "Epoch 124/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.6458 - acc: 0.8189Epoch 00123: val_loss improved from 0.67701 to 0.67532, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.6471 - acc: 0.8187 - val_loss: 0.6753 - val_acc: 0.8323\n",
      "Epoch 125/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.6599 - acc: 0.8155Epoch 00124: val_loss improved from 0.67532 to 0.67273, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.6573 - acc: 0.8160 - val_loss: 0.6727 - val_acc: 0.8311\n",
      "Epoch 126/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.6361 - acc: 0.8220Epoch 00125: val_loss improved from 0.67273 to 0.67140, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.6351 - acc: 0.8223 - val_loss: 0.6714 - val_acc: 0.8347\n",
      "Epoch 127/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.6434 - acc: 0.8215Epoch 00126: val_loss improved from 0.67140 to 0.67015, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.6414 - acc: 0.8229 - val_loss: 0.6701 - val_acc: 0.8323\n",
      "Epoch 128/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.6285 - acc: 0.8256Epoch 00127: val_loss improved from 0.67015 to 0.66741, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.6300 - acc: 0.8253 - val_loss: 0.6674 - val_acc: 0.8335\n",
      "Epoch 129/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.6243 - acc: 0.8314Epoch 00128: val_loss improved from 0.66741 to 0.66480, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.6236 - acc: 0.8316 - val_loss: 0.6648 - val_acc: 0.8359\n",
      "Epoch 130/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.6256 - acc: 0.8269Epoch 00129: val_loss improved from 0.66480 to 0.66313, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.6248 - acc: 0.8262 - val_loss: 0.6631 - val_acc: 0.8371\n",
      "Epoch 131/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.6445 - acc: 0.8239Epoch 00130: val_loss improved from 0.66313 to 0.66238, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.6442 - acc: 0.8241 - val_loss: 0.6624 - val_acc: 0.8323\n",
      "Epoch 132/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.6224 - acc: 0.8238Epoch 00131: val_loss improved from 0.66238 to 0.66098, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.6218 - acc: 0.8243 - val_loss: 0.6610 - val_acc: 0.8347\n",
      "Epoch 133/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.6180 - acc: 0.8304Epoch 00132: val_loss improved from 0.66098 to 0.65854, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.6179 - acc: 0.8302 - val_loss: 0.6585 - val_acc: 0.8335\n",
      "Epoch 134/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.6170 - acc: 0.8259Epoch 00133: val_loss improved from 0.65854 to 0.65670, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.6174 - acc: 0.8257 - val_loss: 0.6567 - val_acc: 0.8335\n",
      "Epoch 135/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.6140 - acc: 0.8298Epoch 00134: val_loss improved from 0.65670 to 0.65461, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.6148 - acc: 0.8289 - val_loss: 0.6546 - val_acc: 0.8359\n",
      "Epoch 136/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.6035 - acc: 0.8327Epoch 00135: val_loss improved from 0.65461 to 0.65327, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.6037 - acc: 0.8332 - val_loss: 0.6533 - val_acc: 0.8347\n",
      "Epoch 137/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.6011 - acc: 0.8358Epoch 00136: val_loss improved from 0.65327 to 0.65126, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.6019 - acc: 0.8355 - val_loss: 0.6513 - val_acc: 0.8371\n",
      "Epoch 138/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.6047 - acc: 0.8313Epoch 00137: val_loss improved from 0.65126 to 0.64945, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.6040 - acc: 0.8316 - val_loss: 0.6494 - val_acc: 0.8347\n",
      "Epoch 139/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.6004 - acc: 0.8359Epoch 00138: val_loss improved from 0.64945 to 0.64677, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.6023 - acc: 0.8346 - val_loss: 0.6468 - val_acc: 0.8383\n",
      "Epoch 140/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.5842 - acc: 0.8385Epoch 00139: val_loss improved from 0.64677 to 0.64565, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.5857 - acc: 0.8377 - val_loss: 0.6457 - val_acc: 0.8359\n",
      "Epoch 141/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.5902 - acc: 0.8364Epoch 00140: val_loss improved from 0.64565 to 0.64467, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.5891 - acc: 0.8362 - val_loss: 0.6447 - val_acc: 0.8371\n",
      "Epoch 142/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.5848 - acc: 0.8421Epoch 00141: val_loss improved from 0.64467 to 0.64333, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.5838 - acc: 0.8424 - val_loss: 0.6433 - val_acc: 0.8371\n",
      "Epoch 143/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.5901 - acc: 0.8394Epoch 00142: val_loss improved from 0.64333 to 0.64216, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.5882 - acc: 0.8403 - val_loss: 0.6422 - val_acc: 0.8335\n",
      "Epoch 144/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.5792 - acc: 0.8408Epoch 00143: val_loss improved from 0.64216 to 0.64087, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.5770 - acc: 0.8416 - val_loss: 0.6409 - val_acc: 0.8347\n",
      "Epoch 145/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.5725 - acc: 0.8359Epoch 00144: val_loss improved from 0.64087 to 0.63910, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.5728 - acc: 0.8353 - val_loss: 0.6391 - val_acc: 0.8347\n",
      "Epoch 146/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.5636 - acc: 0.8442Epoch 00145: val_loss improved from 0.63910 to 0.63857, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.5627 - acc: 0.8443 - val_loss: 0.6386 - val_acc: 0.8359\n",
      "Epoch 147/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.5558 - acc: 0.8503Epoch 00146: val_loss improved from 0.63857 to 0.63711, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.5546 - acc: 0.8510 - val_loss: 0.6371 - val_acc: 0.8359\n",
      "Epoch 148/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.5669 - acc: 0.8451Epoch 00147: val_loss improved from 0.63711 to 0.63655, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.5670 - acc: 0.8449 - val_loss: 0.6366 - val_acc: 0.8347\n",
      "Epoch 149/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.5522 - acc: 0.8474Epoch 00148: val_loss improved from 0.63655 to 0.63476, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.5517 - acc: 0.8481 - val_loss: 0.6348 - val_acc: 0.8359\n",
      "Epoch 150/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.5497 - acc: 0.8531Epoch 00149: val_loss improved from 0.63476 to 0.63210, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.5494 - acc: 0.8531 - val_loss: 0.6321 - val_acc: 0.8383\n",
      "Epoch 151/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.5585 - acc: 0.8417Epoch 00150: val_loss improved from 0.63210 to 0.63090, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.5598 - acc: 0.8413 - val_loss: 0.6309 - val_acc: 0.8383\n",
      "Epoch 152/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.5522 - acc: 0.8429Epoch 00151: val_loss improved from 0.63090 to 0.62971, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.5529 - acc: 0.8422 - val_loss: 0.6297 - val_acc: 0.8383\n",
      "Epoch 153/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.5476 - acc: 0.8485Epoch 00152: val_loss improved from 0.62971 to 0.62911, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.5482 - acc: 0.8482 - val_loss: 0.6291 - val_acc: 0.8383\n",
      "Epoch 154/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.5578 - acc: 0.8430Epoch 00153: val_loss improved from 0.62911 to 0.62741, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.5579 - acc: 0.8434 - val_loss: 0.6274 - val_acc: 0.8383\n",
      "Epoch 155/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.5313 - acc: 0.8559Epoch 00154: val_loss improved from 0.62741 to 0.62568, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.5308 - acc: 0.8563 - val_loss: 0.6257 - val_acc: 0.8395\n",
      "Epoch 156/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.5322 - acc: 0.8528Epoch 00155: val_loss improved from 0.62568 to 0.62506, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.5317 - acc: 0.8530 - val_loss: 0.6251 - val_acc: 0.8383\n",
      "Epoch 157/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.5291 - acc: 0.8562Epoch 00156: val_loss improved from 0.62506 to 0.62405, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.5296 - acc: 0.8555 - val_loss: 0.6240 - val_acc: 0.8395\n",
      "Epoch 158/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.5417 - acc: 0.8474Epoch 00157: val_loss improved from 0.62405 to 0.62313, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.5417 - acc: 0.8475 - val_loss: 0.6231 - val_acc: 0.8395\n",
      "Epoch 159/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.5331 - acc: 0.8532Epoch 00158: val_loss improved from 0.62313 to 0.62218, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.5332 - acc: 0.8531 - val_loss: 0.6222 - val_acc: 0.8395\n",
      "Epoch 160/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.5394 - acc: 0.8475Epoch 00159: val_loss improved from 0.62218 to 0.62015, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.5386 - acc: 0.8481 - val_loss: 0.6202 - val_acc: 0.8395\n",
      "Epoch 161/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.5373 - acc: 0.8489Epoch 00160: val_loss improved from 0.62015 to 0.61840, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.5377 - acc: 0.8482 - val_loss: 0.6184 - val_acc: 0.8407\n",
      "Epoch 162/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.5295 - acc: 0.8525Epoch 00161: val_loss improved from 0.61840 to 0.61689, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.5305 - acc: 0.8521 - val_loss: 0.6169 - val_acc: 0.8407\n",
      "Epoch 163/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.5390 - acc: 0.8481Epoch 00162: val_loss improved from 0.61689 to 0.61556, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.5376 - acc: 0.8488 - val_loss: 0.6156 - val_acc: 0.8419\n",
      "Epoch 164/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.5221 - acc: 0.8574Epoch 00163: val_loss improved from 0.61556 to 0.61440, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.5212 - acc: 0.8576 - val_loss: 0.6144 - val_acc: 0.8371\n",
      "Epoch 165/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.5236 - acc: 0.8573Epoch 00164: val_loss improved from 0.61440 to 0.61309, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.5231 - acc: 0.8570 - val_loss: 0.6131 - val_acc: 0.8407\n",
      "Epoch 166/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.5129 - acc: 0.8561Epoch 00165: val_loss improved from 0.61309 to 0.61267, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.5129 - acc: 0.8561 - val_loss: 0.6127 - val_acc: 0.8371\n",
      "Epoch 167/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.5088 - acc: 0.8565Epoch 00166: val_loss improved from 0.61267 to 0.61171, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.5108 - acc: 0.8569 - val_loss: 0.6117 - val_acc: 0.8419\n",
      "Epoch 168/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.5018 - acc: 0.8644Epoch 00167: val_loss improved from 0.61171 to 0.61042, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.5013 - acc: 0.8647 - val_loss: 0.6104 - val_acc: 0.8407\n",
      "Epoch 169/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.5224 - acc: 0.8516Epoch 00168: val_loss improved from 0.61042 to 0.60993, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.5223 - acc: 0.8524 - val_loss: 0.6099 - val_acc: 0.8407\n",
      "Epoch 170/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.5076 - acc: 0.8652Epoch 00169: val_loss improved from 0.60993 to 0.60835, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.5077 - acc: 0.8645 - val_loss: 0.6083 - val_acc: 0.8419\n",
      "Epoch 171/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.5136 - acc: 0.8571Epoch 00170: val_loss improved from 0.60835 to 0.60745, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.5150 - acc: 0.8566 - val_loss: 0.6074 - val_acc: 0.8407\n",
      "Epoch 172/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.5008 - acc: 0.8611Epoch 00171: val_loss improved from 0.60745 to 0.60585, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.5019 - acc: 0.8609 - val_loss: 0.6059 - val_acc: 0.8431\n",
      "Epoch 173/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.5113 - acc: 0.8551Epoch 00172: val_loss improved from 0.60585 to 0.60501, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.5108 - acc: 0.8552 - val_loss: 0.6050 - val_acc: 0.8419\n",
      "Epoch 174/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.4845 - acc: 0.8677Epoch 00173: val_loss improved from 0.60501 to 0.60462, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4856 - acc: 0.8674 - val_loss: 0.6046 - val_acc: 0.8395\n",
      "Epoch 175/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.4949 - acc: 0.8646Epoch 00174: val_loss improved from 0.60462 to 0.60460, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4941 - acc: 0.8650 - val_loss: 0.6046 - val_acc: 0.8395\n",
      "Epoch 176/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.4881 - acc: 0.8696Epoch 00175: val_loss improved from 0.60460 to 0.60278, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4875 - acc: 0.8696 - val_loss: 0.6028 - val_acc: 0.8395\n",
      "Epoch 177/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4867 - acc: 0.8674Epoch 00176: val_loss improved from 0.60278 to 0.60190, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4864 - acc: 0.8677 - val_loss: 0.6019 - val_acc: 0.8407\n",
      "Epoch 178/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.5071 - acc: 0.8615Epoch 00177: val_loss improved from 0.60190 to 0.60096, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.5059 - acc: 0.8618 - val_loss: 0.6010 - val_acc: 0.8431\n",
      "Epoch 179/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4935 - acc: 0.8611Epoch 00178: val_loss improved from 0.60096 to 0.60015, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4923 - acc: 0.8614 - val_loss: 0.6001 - val_acc: 0.8395\n",
      "Epoch 180/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.4881 - acc: 0.8603Epoch 00179: val_loss improved from 0.60015 to 0.59878, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4855 - acc: 0.8612 - val_loss: 0.5988 - val_acc: 0.8395\n",
      "Epoch 181/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4826 - acc: 0.8688Epoch 00180: val_loss improved from 0.59878 to 0.59810, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4822 - acc: 0.8692 - val_loss: 0.5981 - val_acc: 0.8395\n",
      "Epoch 182/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.4805 - acc: 0.8669Epoch 00181: val_loss improved from 0.59810 to 0.59794, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4793 - acc: 0.8674 - val_loss: 0.5979 - val_acc: 0.8407\n",
      "Epoch 183/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.4870 - acc: 0.8689Epoch 00182: val_loss improved from 0.59794 to 0.59678, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4862 - acc: 0.8692 - val_loss: 0.5968 - val_acc: 0.8419\n",
      "Epoch 184/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4714 - acc: 0.8689Epoch 00183: val_loss improved from 0.59678 to 0.59500, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4708 - acc: 0.8692 - val_loss: 0.5950 - val_acc: 0.8419\n",
      "Epoch 185/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.4673 - acc: 0.8714Epoch 00184: val_loss improved from 0.59500 to 0.59474, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4668 - acc: 0.8717 - val_loss: 0.5947 - val_acc: 0.8419\n",
      "Epoch 186/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.4727 - acc: 0.8687Epoch 00185: val_loss improved from 0.59474 to 0.59386, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4730 - acc: 0.8687 - val_loss: 0.5939 - val_acc: 0.8443\n",
      "Epoch 187/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4670 - acc: 0.8703Epoch 00186: val_loss improved from 0.59386 to 0.59278, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4688 - acc: 0.8699 - val_loss: 0.5928 - val_acc: 0.8407\n",
      "Epoch 188/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4642 - acc: 0.8706Epoch 00187: val_loss improved from 0.59278 to 0.59249, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4647 - acc: 0.8707 - val_loss: 0.5925 - val_acc: 0.8395\n",
      "Epoch 189/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4804 - acc: 0.8667Epoch 00188: val_loss improved from 0.59249 to 0.59149, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4784 - acc: 0.8671 - val_loss: 0.5915 - val_acc: 0.8395\n",
      "Epoch 190/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4765 - acc: 0.8662Epoch 00189: val_loss improved from 0.59149 to 0.59109, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4760 - acc: 0.8665 - val_loss: 0.5911 - val_acc: 0.8395\n",
      "Epoch 191/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.4601 - acc: 0.8729Epoch 00190: val_loss improved from 0.59109 to 0.58993, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4591 - acc: 0.8734 - val_loss: 0.5899 - val_acc: 0.8407\n",
      "Epoch 192/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4618 - acc: 0.8724Epoch 00191: val_loss improved from 0.58993 to 0.58851, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4624 - acc: 0.8725 - val_loss: 0.5885 - val_acc: 0.8455\n",
      "Epoch 193/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4699 - acc: 0.8652Epoch 00192: val_loss improved from 0.58851 to 0.58733, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4685 - acc: 0.8659 - val_loss: 0.5873 - val_acc: 0.8467\n",
      "Epoch 194/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4703 - acc: 0.8656Epoch 00193: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.4708 - acc: 0.8654 - val_loss: 0.5874 - val_acc: 0.8443\n",
      "Epoch 195/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.4602 - acc: 0.8726Epoch 00194: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.4600 - acc: 0.8729 - val_loss: 0.5875 - val_acc: 0.8443\n",
      "Epoch 196/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.4658 - acc: 0.8741Epoch 00195: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.4651 - acc: 0.8744 - val_loss: 0.5878 - val_acc: 0.8431\n",
      "Epoch 197/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4383 - acc: 0.8792Epoch 00196: val_loss improved from 0.58733 to 0.58611, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4390 - acc: 0.8787 - val_loss: 0.5861 - val_acc: 0.8443\n",
      "Epoch 198/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.4497 - acc: 0.8745Epoch 00197: val_loss improved from 0.58611 to 0.58455, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4489 - acc: 0.8746 - val_loss: 0.5846 - val_acc: 0.8455\n",
      "Epoch 199/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.4581 - acc: 0.8734Epoch 00198: val_loss improved from 0.58455 to 0.58370, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4590 - acc: 0.8731 - val_loss: 0.5837 - val_acc: 0.8467\n",
      "Epoch 200/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4530 - acc: 0.8708Epoch 00199: val_loss improved from 0.58370 to 0.58333, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4540 - acc: 0.8701 - val_loss: 0.5833 - val_acc: 0.8455\n",
      "Epoch 201/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.4383 - acc: 0.8820Epoch 00200: val_loss improved from 0.58333 to 0.58281, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4384 - acc: 0.8820 - val_loss: 0.5828 - val_acc: 0.8431\n",
      "Epoch 202/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.4523 - acc: 0.8729Epoch 00201: val_loss improved from 0.58281 to 0.58196, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4520 - acc: 0.8732 - val_loss: 0.5820 - val_acc: 0.8455\n",
      "Epoch 203/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.4438 - acc: 0.8798Epoch 00202: val_loss improved from 0.58196 to 0.58030, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4430 - acc: 0.8810 - val_loss: 0.5803 - val_acc: 0.8455\n",
      "Epoch 204/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.4388 - acc: 0.8823Epoch 00203: val_loss improved from 0.58030 to 0.57958, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4367 - acc: 0.8832 - val_loss: 0.5796 - val_acc: 0.8443\n",
      "Epoch 205/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4452 - acc: 0.8770Epoch 00204: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.4456 - acc: 0.8765 - val_loss: 0.5798 - val_acc: 0.8467\n",
      "Epoch 206/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.4333 - acc: 0.8806Epoch 00205: val_loss improved from 0.57958 to 0.57895, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4331 - acc: 0.8805 - val_loss: 0.5790 - val_acc: 0.8455\n",
      "Epoch 207/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4331 - acc: 0.8865Epoch 00206: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.4336 - acc: 0.8862 - val_loss: 0.5795 - val_acc: 0.8419\n",
      "Epoch 208/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.4331 - acc: 0.8820Epoch 00207: val_loss improved from 0.57895 to 0.57860, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4328 - acc: 0.8817 - val_loss: 0.5786 - val_acc: 0.8479\n",
      "Epoch 209/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4331 - acc: 0.8774Epoch 00208: val_loss improved from 0.57860 to 0.57705, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4328 - acc: 0.8772 - val_loss: 0.5771 - val_acc: 0.8455\n",
      "Epoch 210/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.4317 - acc: 0.8797Epoch 00209: val_loss improved from 0.57705 to 0.57594, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4323 - acc: 0.8793 - val_loss: 0.5759 - val_acc: 0.8467\n",
      "Epoch 211/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.4242 - acc: 0.8844Epoch 00210: val_loss improved from 0.57594 to 0.57504, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4242 - acc: 0.8841 - val_loss: 0.5750 - val_acc: 0.8455\n",
      "Epoch 212/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.4143 - acc: 0.8844Epoch 00211: val_loss improved from 0.57504 to 0.57446, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4136 - acc: 0.8849 - val_loss: 0.5745 - val_acc: 0.8431\n",
      "Epoch 213/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.4194 - acc: 0.8847Epoch 00212: val_loss improved from 0.57446 to 0.57318, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4192 - acc: 0.8844 - val_loss: 0.5732 - val_acc: 0.8419\n",
      "Epoch 214/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.4207 - acc: 0.8867Epoch 00213: val_loss improved from 0.57318 to 0.57207, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4206 - acc: 0.8867 - val_loss: 0.5721 - val_acc: 0.8443\n",
      "Epoch 215/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.4208 - acc: 0.8850Epoch 00214: val_loss improved from 0.57207 to 0.57163, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4223 - acc: 0.8846 - val_loss: 0.5716 - val_acc: 0.8419\n",
      "Epoch 216/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.4285 - acc: 0.8775Epoch 00215: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.4271 - acc: 0.8778 - val_loss: 0.5718 - val_acc: 0.8431\n",
      "Epoch 217/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.4152 - acc: 0.8901Epoch 00216: val_loss improved from 0.57163 to 0.57089, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4156 - acc: 0.8901 - val_loss: 0.5709 - val_acc: 0.8431\n",
      "Epoch 218/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4157 - acc: 0.8859Epoch 00217: val_loss improved from 0.57089 to 0.56998, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4160 - acc: 0.8862 - val_loss: 0.5700 - val_acc: 0.8419\n",
      "Epoch 219/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.4128 - acc: 0.8895Epoch 00218: val_loss improved from 0.56998 to 0.56996, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4116 - acc: 0.8894 - val_loss: 0.5700 - val_acc: 0.8431\n",
      "Epoch 220/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.4089 - acc: 0.8875Epoch 00219: val_loss improved from 0.56996 to 0.56905, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4091 - acc: 0.8876 - val_loss: 0.5690 - val_acc: 0.8467\n",
      "Epoch 221/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.4099 - acc: 0.8872Epoch 00220: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.4099 - acc: 0.8868 - val_loss: 0.5697 - val_acc: 0.8443\n",
      "Epoch 222/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.4107 - acc: 0.8854Epoch 00221: val_loss improved from 0.56905 to 0.56830, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4112 - acc: 0.8852 - val_loss: 0.5683 - val_acc: 0.8431\n",
      "Epoch 223/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.4021 - acc: 0.8910Epoch 00222: val_loss improved from 0.56830 to 0.56805, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4031 - acc: 0.8904 - val_loss: 0.5680 - val_acc: 0.8443\n",
      "Epoch 224/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4042 - acc: 0.8891Epoch 00223: val_loss improved from 0.56805 to 0.56769, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4041 - acc: 0.8892 - val_loss: 0.5677 - val_acc: 0.8443\n",
      "Epoch 225/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.4011 - acc: 0.8905Epoch 00224: val_loss improved from 0.56769 to 0.56717, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4017 - acc: 0.8906 - val_loss: 0.5672 - val_acc: 0.8479\n",
      "Epoch 226/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.4036 - acc: 0.8852Epoch 00225: val_loss improved from 0.56717 to 0.56646, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4022 - acc: 0.8858 - val_loss: 0.5665 - val_acc: 0.8467\n",
      "Epoch 227/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.3968 - acc: 0.8947Epoch 00226: val_loss improved from 0.56646 to 0.56594, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3975 - acc: 0.8946 - val_loss: 0.5659 - val_acc: 0.8467\n",
      "Epoch 228/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.3926 - acc: 0.8977Epoch 00227: val_loss improved from 0.56594 to 0.56475, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3899 - acc: 0.8987 - val_loss: 0.5648 - val_acc: 0.8455\n",
      "Epoch 229/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.3997 - acc: 0.8920Epoch 00228: val_loss improved from 0.56475 to 0.56393, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3994 - acc: 0.8919 - val_loss: 0.5639 - val_acc: 0.8479\n",
      "Epoch 230/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4019 - acc: 0.8865Epoch 00229: val_loss improved from 0.56393 to 0.56383, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4020 - acc: 0.8861 - val_loss: 0.5638 - val_acc: 0.8503\n",
      "Epoch 231/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3930 - acc: 0.8900Epoch 00230: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.3929 - acc: 0.8904 - val_loss: 0.5643 - val_acc: 0.8503\n",
      "Epoch 232/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.4020 - acc: 0.8899Epoch 00231: val_loss improved from 0.56383 to 0.56380, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4024 - acc: 0.8901 - val_loss: 0.5638 - val_acc: 0.8491\n",
      "Epoch 233/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4010 - acc: 0.8830Epoch 00232: val_loss improved from 0.56380 to 0.56253, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.4009 - acc: 0.8834 - val_loss: 0.5625 - val_acc: 0.8491\n",
      "Epoch 234/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.3791 - acc: 0.8985Epoch 00233: val_loss improved from 0.56253 to 0.56247, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3782 - acc: 0.8990 - val_loss: 0.5625 - val_acc: 0.8443\n",
      "Epoch 235/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.3967 - acc: 0.8880Epoch 00234: val_loss improved from 0.56247 to 0.56171, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3975 - acc: 0.8880 - val_loss: 0.5617 - val_acc: 0.8467\n",
      "Epoch 236/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3842 - acc: 0.8964Epoch 00235: val_loss improved from 0.56171 to 0.56127, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3852 - acc: 0.8958 - val_loss: 0.5613 - val_acc: 0.8479\n",
      "Epoch 237/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.3751 - acc: 0.9014Epoch 00236: val_loss improved from 0.56127 to 0.56125, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3755 - acc: 0.9019 - val_loss: 0.5612 - val_acc: 0.8455\n",
      "Epoch 238/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.3873 - acc: 0.8961Epoch 00237: val_loss improved from 0.56125 to 0.56020, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3876 - acc: 0.8961 - val_loss: 0.5602 - val_acc: 0.8431\n",
      "Epoch 239/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3859 - acc: 0.8959Epoch 00238: val_loss improved from 0.56020 to 0.56009, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3858 - acc: 0.8958 - val_loss: 0.5601 - val_acc: 0.8443\n",
      "Epoch 240/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.3859 - acc: 0.8943Epoch 00239: val_loss improved from 0.56009 to 0.55882, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3858 - acc: 0.8943 - val_loss: 0.5588 - val_acc: 0.8443\n",
      "Epoch 241/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.3827 - acc: 0.8929Epoch 00240: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.3822 - acc: 0.8931 - val_loss: 0.5591 - val_acc: 0.8491\n",
      "Epoch 242/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3800 - acc: 0.8948Epoch 00241: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.3797 - acc: 0.8951 - val_loss: 0.5590 - val_acc: 0.8491\n",
      "Epoch 243/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.3781 - acc: 0.8969Epoch 00242: val_loss improved from 0.55882 to 0.55836, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3782 - acc: 0.8969 - val_loss: 0.5584 - val_acc: 0.8479\n",
      "Epoch 244/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.3690 - acc: 0.9015Epoch 00243: val_loss improved from 0.55836 to 0.55699, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3692 - acc: 0.9007 - val_loss: 0.5570 - val_acc: 0.8455\n",
      "Epoch 245/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.3725 - acc: 0.9015Epoch 00244: val_loss improved from 0.55699 to 0.55644, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3738 - acc: 0.9010 - val_loss: 0.5564 - val_acc: 0.8479\n",
      "Epoch 246/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.3766 - acc: 0.8982Epoch 00245: val_loss improved from 0.55644 to 0.55499, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3747 - acc: 0.8991 - val_loss: 0.5550 - val_acc: 0.8479\n",
      "Epoch 247/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.3642 - acc: 0.9041Epoch 00246: val_loss improved from 0.55499 to 0.55496, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3655 - acc: 0.9037 - val_loss: 0.5550 - val_acc: 0.8467\n",
      "Epoch 248/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3718 - acc: 0.8955Epoch 00247: val_loss improved from 0.55496 to 0.55459, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3733 - acc: 0.8952 - val_loss: 0.5546 - val_acc: 0.8479\n",
      "Epoch 249/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3730 - acc: 0.8986Epoch 00248: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.3733 - acc: 0.8982 - val_loss: 0.5548 - val_acc: 0.8467\n",
      "Epoch 250/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3653 - acc: 0.9047Epoch 00249: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.3660 - acc: 0.9045 - val_loss: 0.5547 - val_acc: 0.8443\n",
      "Epoch 251/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.3559 - acc: 0.9070Epoch 00250: val_loss improved from 0.55459 to 0.55434, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3568 - acc: 0.9067 - val_loss: 0.5543 - val_acc: 0.8455\n",
      "Epoch 252/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.3683 - acc: 0.9024Epoch 00251: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.3681 - acc: 0.9025 - val_loss: 0.5546 - val_acc: 0.8455\n",
      "Epoch 253/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.3636 - acc: 0.9017Epoch 00252: val_loss improved from 0.55434 to 0.55341, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3634 - acc: 0.9025 - val_loss: 0.5534 - val_acc: 0.8479\n",
      "Epoch 254/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.3611 - acc: 0.9047Epoch 00253: val_loss improved from 0.55341 to 0.55243, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3608 - acc: 0.9049 - val_loss: 0.5524 - val_acc: 0.8491\n",
      "Epoch 255/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.3580 - acc: 0.9041Epoch 00254: val_loss improved from 0.55243 to 0.55176, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3581 - acc: 0.9040 - val_loss: 0.5518 - val_acc: 0.8491\n",
      "Epoch 256/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3646 - acc: 0.8991Epoch 00255: val_loss improved from 0.55176 to 0.55117, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3646 - acc: 0.8988 - val_loss: 0.5512 - val_acc: 0.8527\n",
      "Epoch 257/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3651 - acc: 0.8991Epoch 00256: val_loss improved from 0.55117 to 0.55099, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3662 - acc: 0.8987 - val_loss: 0.5510 - val_acc: 0.8503\n",
      "Epoch 258/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3504 - acc: 0.9102Epoch 00257: val_loss improved from 0.55099 to 0.54946, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3497 - acc: 0.9108 - val_loss: 0.5495 - val_acc: 0.8491\n",
      "Epoch 259/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.3601 - acc: 0.8980Epoch 00258: val_loss improved from 0.54946 to 0.54913, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3602 - acc: 0.8982 - val_loss: 0.5491 - val_acc: 0.8503\n",
      "Epoch 260/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.3575 - acc: 0.9033Epoch 00259: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.3572 - acc: 0.9034 - val_loss: 0.5495 - val_acc: 0.8479\n",
      "Epoch 261/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3641 - acc: 0.9055Epoch 00260: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.3639 - acc: 0.9052 - val_loss: 0.5496 - val_acc: 0.8503\n",
      "Epoch 262/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.3564 - acc: 0.9026Epoch 00261: val_loss improved from 0.54913 to 0.54896, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3573 - acc: 0.9025 - val_loss: 0.5490 - val_acc: 0.8503\n",
      "Epoch 263/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.3567 - acc: 0.9044Epoch 00262: val_loss improved from 0.54896 to 0.54881, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3552 - acc: 0.9049 - val_loss: 0.5488 - val_acc: 0.8491\n",
      "Epoch 264/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3585 - acc: 0.9068Epoch 00263: val_loss improved from 0.54881 to 0.54789, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3584 - acc: 0.9064 - val_loss: 0.5479 - val_acc: 0.8503\n",
      "Epoch 265/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3471 - acc: 0.9067Epoch 00264: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.3469 - acc: 0.9064 - val_loss: 0.5482 - val_acc: 0.8479\n",
      "Epoch 266/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.3530 - acc: 0.9041Epoch 00265: val_loss improved from 0.54789 to 0.54760, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3536 - acc: 0.9039 - val_loss: 0.5476 - val_acc: 0.8479\n",
      "Epoch 267/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3567 - acc: 0.9050Epoch 00266: val_loss improved from 0.54760 to 0.54715, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3565 - acc: 0.9049 - val_loss: 0.5472 - val_acc: 0.8503\n",
      "Epoch 268/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3530 - acc: 0.9071Epoch 00267: val_loss improved from 0.54715 to 0.54694, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3538 - acc: 0.9069 - val_loss: 0.5469 - val_acc: 0.8491\n",
      "Epoch 269/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3372 - acc: 0.9138Epoch 00268: val_loss improved from 0.54694 to 0.54681, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3389 - acc: 0.9130 - val_loss: 0.5468 - val_acc: 0.8491\n",
      "Epoch 270/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.3389 - acc: 0.9093Epoch 00269: val_loss improved from 0.54681 to 0.54637, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3384 - acc: 0.9096 - val_loss: 0.5464 - val_acc: 0.8467\n",
      "Epoch 271/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.3403 - acc: 0.9110Epoch 00270: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.3409 - acc: 0.9105 - val_loss: 0.5468 - val_acc: 0.8467\n",
      "Epoch 272/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.3387 - acc: 0.9104Epoch 00271: val_loss improved from 0.54637 to 0.54553, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3389 - acc: 0.9103 - val_loss: 0.5455 - val_acc: 0.8467\n",
      "Epoch 273/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3402 - acc: 0.9098Epoch 00272: val_loss improved from 0.54553 to 0.54420, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3396 - acc: 0.9097 - val_loss: 0.5442 - val_acc: 0.8491\n",
      "Epoch 274/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.3368 - acc: 0.9111Epoch 00273: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.3374 - acc: 0.9114 - val_loss: 0.5447 - val_acc: 0.8479\n",
      "Epoch 275/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.3385 - acc: 0.9067Epoch 00274: val_loss improved from 0.54420 to 0.54404, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3394 - acc: 0.9064 - val_loss: 0.5440 - val_acc: 0.8455\n",
      "Epoch 276/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3426 - acc: 0.9109Epoch 00275: val_loss improved from 0.54404 to 0.54354, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3415 - acc: 0.9112 - val_loss: 0.5435 - val_acc: 0.8467\n",
      "Epoch 277/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.3374 - acc: 0.9134Epoch 00276: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.3376 - acc: 0.9132 - val_loss: 0.5437 - val_acc: 0.8467\n",
      "Epoch 278/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3292 - acc: 0.9155Epoch 00277: val_loss improved from 0.54354 to 0.54340, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3288 - acc: 0.9156 - val_loss: 0.5434 - val_acc: 0.8479\n",
      "Epoch 279/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.3415 - acc: 0.9049Epoch 00278: val_loss improved from 0.54340 to 0.54222, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3441 - acc: 0.9040 - val_loss: 0.5422 - val_acc: 0.8491\n",
      "Epoch 280/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3347 - acc: 0.9108Epoch 00279: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.3342 - acc: 0.9109 - val_loss: 0.5424 - val_acc: 0.8491\n",
      "Epoch 281/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.3310 - acc: 0.9130Epoch 00280: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.3313 - acc: 0.9127 - val_loss: 0.5424 - val_acc: 0.8479\n",
      "Epoch 282/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3339 - acc: 0.9112Epoch 00281: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.3334 - acc: 0.9114 - val_loss: 0.5425 - val_acc: 0.8491\n",
      "Epoch 283/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3258 - acc: 0.9129Epoch 00282: val_loss improved from 0.54222 to 0.54204, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3270 - acc: 0.9124 - val_loss: 0.5420 - val_acc: 0.8479\n",
      "Epoch 284/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.3313 - acc: 0.9151Epoch 00283: val_loss improved from 0.54204 to 0.54094, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3310 - acc: 0.9153 - val_loss: 0.5409 - val_acc: 0.8491\n",
      "Epoch 285/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.3333 - acc: 0.9120Epoch 00284: val_loss improved from 0.54094 to 0.54087, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3343 - acc: 0.9109 - val_loss: 0.5409 - val_acc: 0.8467\n",
      "Epoch 286/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.3260 - acc: 0.9108Epoch 00285: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.3261 - acc: 0.9105 - val_loss: 0.5409 - val_acc: 0.8455\n",
      "Epoch 287/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3216 - acc: 0.9176Epoch 00286: val_loss improved from 0.54087 to 0.54076, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3209 - acc: 0.9181 - val_loss: 0.5408 - val_acc: 0.8455\n",
      "Epoch 288/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.3225 - acc: 0.9153Epoch 00287: val_loss improved from 0.54076 to 0.53910, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3224 - acc: 0.9156 - val_loss: 0.5391 - val_acc: 0.8467\n",
      "Epoch 289/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3302 - acc: 0.9058Epoch 00288: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.3302 - acc: 0.9061 - val_loss: 0.5394 - val_acc: 0.8455\n",
      "Epoch 290/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.3225 - acc: 0.9096Epoch 00289: val_loss improved from 0.53910 to 0.53872, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3221 - acc: 0.9094 - val_loss: 0.5387 - val_acc: 0.8443\n",
      "Epoch 291/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.3214 - acc: 0.9169Epoch 00290: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.3207 - acc: 0.9171 - val_loss: 0.5392 - val_acc: 0.8455\n",
      "Epoch 292/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.3171 - acc: 0.9186Epoch 00291: val_loss improved from 0.53872 to 0.53842, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3165 - acc: 0.9189 - val_loss: 0.5384 - val_acc: 0.8443\n",
      "Epoch 293/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.3194 - acc: 0.9147Epoch 00292: val_loss improved from 0.53842 to 0.53783, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3198 - acc: 0.9145 - val_loss: 0.5378 - val_acc: 0.8467\n",
      "Epoch 294/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3197 - acc: 0.9136Epoch 00293: val_loss improved from 0.53783 to 0.53750, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3198 - acc: 0.9133 - val_loss: 0.5375 - val_acc: 0.8503\n",
      "Epoch 295/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.3096 - acc: 0.9165Epoch 00294: val_loss improved from 0.53750 to 0.53710, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3092 - acc: 0.9169 - val_loss: 0.5371 - val_acc: 0.8503\n",
      "Epoch 296/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3240 - acc: 0.9115Epoch 00295: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.3235 - acc: 0.9118 - val_loss: 0.5377 - val_acc: 0.8491\n",
      "Epoch 297/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.3180 - acc: 0.9167Epoch 00296: val_loss improved from 0.53710 to 0.53662, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3181 - acc: 0.9168 - val_loss: 0.5366 - val_acc: 0.8467\n",
      "Epoch 298/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.3141 - acc: 0.9160Epoch 00297: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.3159 - acc: 0.9151 - val_loss: 0.5366 - val_acc: 0.8467\n",
      "Epoch 299/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3186 - acc: 0.9139Epoch 00298: val_loss improved from 0.53662 to 0.53621, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3180 - acc: 0.9142 - val_loss: 0.5362 - val_acc: 0.8479\n",
      "Epoch 300/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.3086 - acc: 0.9235Epoch 00299: val_loss improved from 0.53621 to 0.53566, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3083 - acc: 0.9237 - val_loss: 0.5357 - val_acc: 0.8479\n",
      "Epoch 301/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.3101 - acc: 0.9131Epoch 00300: val_loss improved from 0.53566 to 0.53518, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3129 - acc: 0.9121 - val_loss: 0.5352 - val_acc: 0.8479\n",
      "Epoch 302/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.3207 - acc: 0.9122Epoch 00301: val_loss improved from 0.53518 to 0.53512, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3214 - acc: 0.9117 - val_loss: 0.5351 - val_acc: 0.8491\n",
      "Epoch 303/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.3120 - acc: 0.9185Epoch 00302: val_loss improved from 0.53512 to 0.53491, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3116 - acc: 0.9189 - val_loss: 0.5349 - val_acc: 0.8491\n",
      "Epoch 304/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.3142 - acc: 0.9147Epoch 00303: val_loss improved from 0.53491 to 0.53423, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3143 - acc: 0.9147 - val_loss: 0.5342 - val_acc: 0.8491\n",
      "Epoch 305/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.3107 - acc: 0.9146Epoch 00304: val_loss improved from 0.53423 to 0.53357, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3108 - acc: 0.9147 - val_loss: 0.5336 - val_acc: 0.8479\n",
      "Epoch 306/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.3123 - acc: 0.9209Epoch 00305: val_loss improved from 0.53357 to 0.53321, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3123 - acc: 0.9211 - val_loss: 0.5332 - val_acc: 0.8491\n",
      "Epoch 307/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.3024 - acc: 0.9205Epoch 00306: val_loss improved from 0.53321 to 0.53241, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3026 - acc: 0.9204 - val_loss: 0.5324 - val_acc: 0.8503\n",
      "Epoch 308/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.3114 - acc: 0.9147Epoch 00307: val_loss improved from 0.53241 to 0.53230, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3118 - acc: 0.9147 - val_loss: 0.5323 - val_acc: 0.8491\n",
      "Epoch 309/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2976 - acc: 0.9221Epoch 00308: val_loss improved from 0.53230 to 0.53133, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2988 - acc: 0.9217 - val_loss: 0.5313 - val_acc: 0.8491\n",
      "Epoch 310/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3097 - acc: 0.9185Epoch 00309: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.3093 - acc: 0.9189 - val_loss: 0.5315 - val_acc: 0.8479\n",
      "Epoch 311/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.3032 - acc: 0.9227Epoch 00310: val_loss improved from 0.53133 to 0.53132, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3016 - acc: 0.9234 - val_loss: 0.5313 - val_acc: 0.8479\n",
      "Epoch 312/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.2985 - acc: 0.9171Epoch 00311: val_loss improved from 0.53132 to 0.53080, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2974 - acc: 0.9181 - val_loss: 0.5308 - val_acc: 0.8503\n",
      "Epoch 313/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.2989 - acc: 0.9223Epoch 00312: val_loss improved from 0.53080 to 0.53018, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2992 - acc: 0.9217 - val_loss: 0.5302 - val_acc: 0.8503\n",
      "Epoch 314/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3033 - acc: 0.9174Epoch 00313: val_loss improved from 0.53018 to 0.52983, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3046 - acc: 0.9166 - val_loss: 0.5298 - val_acc: 0.8467\n",
      "Epoch 315/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2981 - acc: 0.9229Epoch 00314: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2984 - acc: 0.9228 - val_loss: 0.5303 - val_acc: 0.8467\n",
      "Epoch 316/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.3022 - acc: 0.9235Epoch 00315: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.3022 - acc: 0.9235 - val_loss: 0.5313 - val_acc: 0.8479\n",
      "Epoch 317/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.2982 - acc: 0.9208Epoch 00316: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2988 - acc: 0.9208 - val_loss: 0.5311 - val_acc: 0.8479\n",
      "Epoch 318/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.2992 - acc: 0.9264-Epoch 00317: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2992 - acc: 0.9262 - val_loss: 0.5310 - val_acc: 0.8467\n",
      "Epoch 319/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.2932 - acc: 0.9277Epoch 00318: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2926 - acc: 0.9275 - val_loss: 0.5308 - val_acc: 0.8467\n",
      "Epoch 320/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.2927 - acc: 0.9237Epoch 00319: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2907 - acc: 0.9241 - val_loss: 0.5299 - val_acc: 0.8479\n",
      "Epoch 321/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3027 - acc: 0.9212Epoch 00320: val_loss improved from 0.52983 to 0.52923, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.3025 - acc: 0.9216 - val_loss: 0.5292 - val_acc: 0.8503\n",
      "Epoch 322/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.2954 - acc: 0.9245Epoch 00321: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2949 - acc: 0.9243 - val_loss: 0.5294 - val_acc: 0.8479\n",
      "Epoch 323/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.2980 - acc: 0.9224Epoch 00322: val_loss improved from 0.52923 to 0.52866, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2980 - acc: 0.9220 - val_loss: 0.5287 - val_acc: 0.8491\n",
      "Epoch 324/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.2871 - acc: 0.9241Epoch 00323: val_loss improved from 0.52866 to 0.52788, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2884 - acc: 0.9235 - val_loss: 0.5279 - val_acc: 0.8491\n",
      "Epoch 325/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2894 - acc: 0.9279Epoch 00324: val_loss improved from 0.52788 to 0.52690, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2896 - acc: 0.9275 - val_loss: 0.5269 - val_acc: 0.8479\n",
      "Epoch 326/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.2898 - acc: 0.9264Epoch 00325: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2900 - acc: 0.9262 - val_loss: 0.5270 - val_acc: 0.8503\n",
      "Epoch 327/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2886 - acc: 0.9258Epoch 00326: val_loss improved from 0.52690 to 0.52633, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2886 - acc: 0.9260 - val_loss: 0.5263 - val_acc: 0.8479\n",
      "Epoch 328/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.2933 - acc: 0.9224Epoch 00327: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2930 - acc: 0.9229 - val_loss: 0.5266 - val_acc: 0.8467\n",
      "Epoch 329/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.2897 - acc: 0.9267Epoch 00328: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2890 - acc: 0.9269 - val_loss: 0.5270 - val_acc: 0.8479\n",
      "Epoch 330/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.2892 - acc: 0.9229Epoch 00329: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2895 - acc: 0.9228 - val_loss: 0.5266 - val_acc: 0.8467\n",
      "Epoch 331/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2803 - acc: 0.9274Epoch 00330: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2799 - acc: 0.9275 - val_loss: 0.5264 - val_acc: 0.8467\n",
      "Epoch 332/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2834 - acc: 0.9274Epoch 00331: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2835 - acc: 0.9272 - val_loss: 0.5264 - val_acc: 0.8479\n",
      "Epoch 333/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.2785 - acc: 0.9337Epoch 00332: val_loss improved from 0.52633 to 0.52629, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2774 - acc: 0.9341 - val_loss: 0.5263 - val_acc: 0.8479\n",
      "Epoch 334/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2825 - acc: 0.9238Epoch 00333: val_loss improved from 0.52629 to 0.52617, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2825 - acc: 0.9243 - val_loss: 0.5262 - val_acc: 0.8479\n",
      "Epoch 335/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.2788 - acc: 0.9299Epoch 00334: val_loss improved from 0.52617 to 0.52548, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2801 - acc: 0.9296 - val_loss: 0.5255 - val_acc: 0.8467\n",
      "Epoch 336/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2857 - acc: 0.9299Epoch 00335: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2853 - acc: 0.9301 - val_loss: 0.5256 - val_acc: 0.8467\n",
      "Epoch 337/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2833 - acc: 0.9250Epoch 00336: val_loss improved from 0.52548 to 0.52534, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2830 - acc: 0.9250 - val_loss: 0.5253 - val_acc: 0.8479\n",
      "Epoch 338/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2811 - acc: 0.9257Epoch 00337: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2809 - acc: 0.9256 - val_loss: 0.5255 - val_acc: 0.8467\n",
      "Epoch 339/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2857 - acc: 0.9239Epoch 00338: val_loss improved from 0.52534 to 0.52525, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2855 - acc: 0.9243 - val_loss: 0.5253 - val_acc: 0.8467\n",
      "Epoch 340/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.2780 - acc: 0.9338Epoch 00339: val_loss improved from 0.52525 to 0.52393, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2781 - acc: 0.9334 - val_loss: 0.5239 - val_acc: 0.8467\n",
      "Epoch 341/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2728 - acc: 0.9323Epoch 00340: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2738 - acc: 0.9323 - val_loss: 0.5241 - val_acc: 0.8479\n",
      "Epoch 342/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.2756 - acc: 0.9275Epoch 00341: val_loss improved from 0.52393 to 0.52334, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2769 - acc: 0.9271 - val_loss: 0.5233 - val_acc: 0.8467\n",
      "Epoch 343/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2768 - acc: 0.9274Epoch 00342: val_loss improved from 0.52334 to 0.52307, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2768 - acc: 0.9277 - val_loss: 0.5231 - val_acc: 0.8455\n",
      "Epoch 344/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.2762 - acc: 0.9282Epoch 00343: val_loss improved from 0.52307 to 0.52269, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2766 - acc: 0.9286 - val_loss: 0.5227 - val_acc: 0.8491\n",
      "Epoch 345/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.2733 - acc: 0.9280Epoch 00344: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2740 - acc: 0.9274 - val_loss: 0.5228 - val_acc: 0.8503\n",
      "Epoch 346/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.2783 - acc: 0.9269Epoch 00345: val_loss improved from 0.52269 to 0.52206, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2786 - acc: 0.9265 - val_loss: 0.5221 - val_acc: 0.8503\n",
      "Epoch 347/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.2730 - acc: 0.9310Epoch 00346: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2729 - acc: 0.9311 - val_loss: 0.5221 - val_acc: 0.8503\n",
      "Epoch 348/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2725 - acc: 0.9290Epoch 00347: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2721 - acc: 0.9292 - val_loss: 0.5223 - val_acc: 0.8491\n",
      "Epoch 349/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.2732 - acc: 0.9305Epoch 00348: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2723 - acc: 0.9313 - val_loss: 0.5225 - val_acc: 0.8479\n",
      "Epoch 350/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.2727 - acc: 0.9283Epoch 00349: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2736 - acc: 0.9272 - val_loss: 0.5229 - val_acc: 0.8503\n",
      "Epoch 351/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2775 - acc: 0.9296Epoch 00350: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2781 - acc: 0.9295 - val_loss: 0.5226 - val_acc: 0.8491\n",
      "Epoch 352/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2687 - acc: 0.9286Epoch 00351: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2681 - acc: 0.9290 - val_loss: 0.5224 - val_acc: 0.8479\n",
      "Epoch 353/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2733 - acc: 0.9312Epoch 00352: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2735 - acc: 0.9310 - val_loss: 0.5222 - val_acc: 0.8491\n",
      "Epoch 354/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2632 - acc: 0.9327Epoch 00353: val_loss improved from 0.52206 to 0.52148, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2637 - acc: 0.9322 - val_loss: 0.5215 - val_acc: 0.8491\n",
      "Epoch 355/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2641 - acc: 0.9322Epoch 00354: val_loss improved from 0.52148 to 0.52052, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2643 - acc: 0.9320 - val_loss: 0.5205 - val_acc: 0.8491\n",
      "Epoch 356/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.2720 - acc: 0.9261Epoch 00355: val_loss improved from 0.52052 to 0.52005, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2718 - acc: 0.9259 - val_loss: 0.5200 - val_acc: 0.8503\n",
      "Epoch 357/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2680 - acc: 0.9326Epoch 00356: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2681 - acc: 0.9325 - val_loss: 0.5203 - val_acc: 0.8503\n",
      "Epoch 358/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2649 - acc: 0.9311Epoch 00357: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2640 - acc: 0.9316 - val_loss: 0.5201 - val_acc: 0.8503\n",
      "Epoch 359/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2634 - acc: 0.9337Epoch 00358: val_loss improved from 0.52005 to 0.51966, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2641 - acc: 0.9335 - val_loss: 0.5197 - val_acc: 0.8503\n",
      "Epoch 360/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.2682 - acc: 0.9296Epoch 00359: val_loss improved from 0.51966 to 0.51956, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2671 - acc: 0.9298 - val_loss: 0.5196 - val_acc: 0.8527\n",
      "Epoch 361/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2553 - acc: 0.9349Epoch 00360: val_loss improved from 0.51956 to 0.51912, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2558 - acc: 0.9349 - val_loss: 0.5191 - val_acc: 0.8527\n",
      "Epoch 362/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.2605 - acc: 0.9327Epoch 00361: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2626 - acc: 0.9320 - val_loss: 0.5199 - val_acc: 0.8515\n",
      "Epoch 363/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2661 - acc: 0.9330Epoch 00362: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2657 - acc: 0.9332 - val_loss: 0.5201 - val_acc: 0.8539\n",
      "Epoch 364/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2667 - acc: 0.9321Epoch 00363: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2668 - acc: 0.9320 - val_loss: 0.5199 - val_acc: 0.8515\n",
      "Epoch 365/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2606 - acc: 0.9338Epoch 00364: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2604 - acc: 0.9338 - val_loss: 0.5194 - val_acc: 0.8527\n",
      "Epoch 366/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.2723 - acc: 0.9279Epoch 00365: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2734 - acc: 0.9272 - val_loss: 0.5193 - val_acc: 0.8491\n",
      "Epoch 367/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2620 - acc: 0.9314Epoch 00366: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2617 - acc: 0.9314 - val_loss: 0.5197 - val_acc: 0.8503\n",
      "Epoch 368/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.2658 - acc: 0.9328Epoch 00367: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2651 - acc: 0.9331 - val_loss: 0.5193 - val_acc: 0.8527\n",
      "Epoch 369/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2573 - acc: 0.9352Epoch 00368: val_loss improved from 0.51912 to 0.51853, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2576 - acc: 0.9350 - val_loss: 0.5185 - val_acc: 0.8527\n",
      "Epoch 370/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2612 - acc: 0.9344Epoch 00369: val_loss improved from 0.51853 to 0.51848, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2609 - acc: 0.9346 - val_loss: 0.5185 - val_acc: 0.8503\n",
      "Epoch 371/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2642 - acc: 0.9314Epoch 00370: val_loss improved from 0.51848 to 0.51842, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2636 - acc: 0.9317 - val_loss: 0.5184 - val_acc: 0.8503\n",
      "Epoch 372/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2550 - acc: 0.9379Epoch 00371: val_loss improved from 0.51842 to 0.51776, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2554 - acc: 0.9377 - val_loss: 0.5178 - val_acc: 0.8515\n",
      "Epoch 373/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.2519 - acc: 0.9368Epoch 00372: val_loss improved from 0.51776 to 0.51686, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2533 - acc: 0.9361 - val_loss: 0.5169 - val_acc: 0.8491\n",
      "Epoch 374/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2497 - acc: 0.9382Epoch 00373: val_loss improved from 0.51686 to 0.51599, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2494 - acc: 0.9385 - val_loss: 0.5160 - val_acc: 0.8503\n",
      "Epoch 375/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.2535 - acc: 0.9344Epoch 00374: val_loss improved from 0.51599 to 0.51593, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2528 - acc: 0.9347 - val_loss: 0.5159 - val_acc: 0.8515\n",
      "Epoch 376/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2500 - acc: 0.9397Epoch 00375: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2501 - acc: 0.9401 - val_loss: 0.5160 - val_acc: 0.8527\n",
      "Epoch 377/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2505 - acc: 0.9385Epoch 00376: val_loss improved from 0.51593 to 0.51543, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2502 - acc: 0.9388 - val_loss: 0.5154 - val_acc: 0.8539\n",
      "Epoch 378/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.2591 - acc: 0.9332Epoch 00377: val_loss improved from 0.51543 to 0.51508, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2594 - acc: 0.9331 - val_loss: 0.5151 - val_acc: 0.8527\n",
      "Epoch 379/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2495 - acc: 0.9400Epoch 00378: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2490 - acc: 0.9401 - val_loss: 0.5151 - val_acc: 0.8515\n",
      "Epoch 380/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.2475 - acc: 0.9388Epoch 00379: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2476 - acc: 0.9392 - val_loss: 0.5151 - val_acc: 0.8527\n",
      "Epoch 381/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2464 - acc: 0.9403Epoch 00380: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2465 - acc: 0.9401 - val_loss: 0.5152 - val_acc: 0.8527\n",
      "Epoch 382/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2571 - acc: 0.9343Epoch 00381: val_loss improved from 0.51508 to 0.51496, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2574 - acc: 0.9341 - val_loss: 0.5150 - val_acc: 0.8503\n",
      "Epoch 383/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.2544 - acc: 0.9334Epoch 00382: val_loss improved from 0.51496 to 0.51435, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2554 - acc: 0.9332 - val_loss: 0.5144 - val_acc: 0.8491\n",
      "Epoch 384/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2483 - acc: 0.9385Epoch 00383: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2481 - acc: 0.9388 - val_loss: 0.5154 - val_acc: 0.8491\n",
      "Epoch 385/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2504 - acc: 0.9355Epoch 00384: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2502 - acc: 0.9355 - val_loss: 0.5152 - val_acc: 0.8479\n",
      "Epoch 386/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.2458 - acc: 0.9360Epoch 00385: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2455 - acc: 0.9365 - val_loss: 0.5149 - val_acc: 0.8491\n",
      "Epoch 387/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.2474 - acc: 0.9385Epoch 00386: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2471 - acc: 0.9386 - val_loss: 0.5145 - val_acc: 0.8503\n",
      "Epoch 388/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2487 - acc: 0.9347Epoch 00387: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2488 - acc: 0.9347 - val_loss: 0.5145 - val_acc: 0.8503\n",
      "Epoch 389/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.2456 - acc: 0.9392Epoch 00388: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2452 - acc: 0.9395 - val_loss: 0.5144 - val_acc: 0.8515\n",
      "Epoch 390/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2408 - acc: 0.9406Epoch 00389: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2408 - acc: 0.9409 - val_loss: 0.5146 - val_acc: 0.8503\n",
      "Epoch 391/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2466 - acc: 0.9361Epoch 00390: val_loss improved from 0.51435 to 0.51420, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2466 - acc: 0.9362 - val_loss: 0.5142 - val_acc: 0.8515\n",
      "Epoch 392/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.2404 - acc: 0.9429Epoch 00391: val_loss improved from 0.51420 to 0.51406, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2408 - acc: 0.9424 - val_loss: 0.5141 - val_acc: 0.8527\n",
      "Epoch 393/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2441 - acc: 0.9383Epoch 00392: val_loss improved from 0.51406 to 0.51400, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2438 - acc: 0.9385 - val_loss: 0.5140 - val_acc: 0.8515\n",
      "Epoch 394/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2390 - acc: 0.9415Epoch 00393: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2404 - acc: 0.9409 - val_loss: 0.5145 - val_acc: 0.8479\n",
      "Epoch 395/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2426 - acc: 0.9432Epoch 00394: val_loss improved from 0.51400 to 0.51373, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2429 - acc: 0.9430 - val_loss: 0.5137 - val_acc: 0.8491\n",
      "Epoch 396/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.2485 - acc: 0.9380Epoch 00395: val_loss improved from 0.51373 to 0.51299, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2477 - acc: 0.9382 - val_loss: 0.5130 - val_acc: 0.8491\n",
      "Epoch 397/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2415 - acc: 0.9412Epoch 00396: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2419 - acc: 0.9412 - val_loss: 0.5138 - val_acc: 0.8503\n",
      "Epoch 398/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2394 - acc: 0.9417Epoch 00397: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2399 - acc: 0.9415 - val_loss: 0.5140 - val_acc: 0.8515\n",
      "Epoch 399/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2363 - acc: 0.9391Epoch 00398: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2361 - acc: 0.9391 - val_loss: 0.5140 - val_acc: 0.8515\n",
      "Epoch 400/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2411 - acc: 0.9417Epoch 00399: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2417 - acc: 0.9413 - val_loss: 0.5136 - val_acc: 0.8503\n",
      "Epoch 401/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.2372 - acc: 0.9441Epoch 00400: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2372 - acc: 0.9446 - val_loss: 0.5139 - val_acc: 0.8503\n",
      "Epoch 402/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2351 - acc: 0.9405Epoch 00401: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2355 - acc: 0.9401 - val_loss: 0.5135 - val_acc: 0.8503\n",
      "Epoch 403/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2409 - acc: 0.9398Epoch 00402: val_loss improved from 0.51299 to 0.51254, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2408 - acc: 0.9400 - val_loss: 0.5125 - val_acc: 0.8515\n",
      "Epoch 404/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2381 - acc: 0.9418Epoch 00403: val_loss improved from 0.51254 to 0.51237, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2380 - acc: 0.9415 - val_loss: 0.5124 - val_acc: 0.8515\n",
      "Epoch 405/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.2350 - acc: 0.9406Epoch 00404: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2343 - acc: 0.9409 - val_loss: 0.5127 - val_acc: 0.8503\n",
      "Epoch 406/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.2363 - acc: 0.9397Epoch 00405: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2365 - acc: 0.9395 - val_loss: 0.5128 - val_acc: 0.8491\n",
      "Epoch 407/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.2304 - acc: 0.9429Epoch 00406: val_loss improved from 0.51237 to 0.51234, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2303 - acc: 0.9430 - val_loss: 0.5123 - val_acc: 0.8503\n",
      "Epoch 408/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2354 - acc: 0.9385Epoch 00407: val_loss improved from 0.51234 to 0.51226, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2350 - acc: 0.9388 - val_loss: 0.5123 - val_acc: 0.8527\n",
      "Epoch 409/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.2453 - acc: 0.9378Epoch 00408: val_loss improved from 0.51226 to 0.51216, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2447 - acc: 0.9382 - val_loss: 0.5122 - val_acc: 0.8491\n",
      "Epoch 410/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2410 - acc: 0.9358Epoch 00409: val_loss improved from 0.51216 to 0.51164, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2406 - acc: 0.9361 - val_loss: 0.5116 - val_acc: 0.8515\n",
      "Epoch 411/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.2371 - acc: 0.9357Epoch 00410: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2369 - acc: 0.9362 - val_loss: 0.5123 - val_acc: 0.8515\n",
      "Epoch 412/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2342 - acc: 0.9373Epoch 00411: val_loss improved from 0.51164 to 0.51133, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2345 - acc: 0.9373 - val_loss: 0.5113 - val_acc: 0.8515\n",
      "Epoch 413/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2353 - acc: 0.9448Epoch 00412: val_loss improved from 0.51133 to 0.51039, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2355 - acc: 0.9449 - val_loss: 0.5104 - val_acc: 0.8503\n",
      "Epoch 414/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2346 - acc: 0.9389Epoch 00413: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2344 - acc: 0.9383 - val_loss: 0.5104 - val_acc: 0.8503\n",
      "Epoch 415/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.2294 - acc: 0.9443Epoch 00414: val_loss improved from 0.51039 to 0.51018, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2281 - acc: 0.9445 - val_loss: 0.5102 - val_acc: 0.8527\n",
      "Epoch 416/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2383 - acc: 0.9427Epoch 00415: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2381 - acc: 0.9428 - val_loss: 0.5110 - val_acc: 0.8515\n",
      "Epoch 417/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2277 - acc: 0.9448Epoch 00416: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2274 - acc: 0.9451 - val_loss: 0.5108 - val_acc: 0.8527\n",
      "Epoch 418/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.2317 - acc: 0.9412Epoch 00417: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2320 - acc: 0.9412 - val_loss: 0.5105 - val_acc: 0.8551\n",
      "Epoch 419/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.2320 - acc: 0.9432Epoch 00418: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2318 - acc: 0.9437 - val_loss: 0.5104 - val_acc: 0.8551\n",
      "Epoch 420/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2288 - acc: 0.9430Epoch 00419: val_loss improved from 0.51018 to 0.51015, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2279 - acc: 0.9434 - val_loss: 0.5102 - val_acc: 0.8539\n",
      "Epoch 421/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2271 - acc: 0.9445Epoch 00420: val_loss improved from 0.51015 to 0.50959, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2265 - acc: 0.9448 - val_loss: 0.5096 - val_acc: 0.8527\n",
      "Epoch 422/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2257 - acc: 0.9456Epoch 00421: val_loss improved from 0.50959 to 0.50936, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2252 - acc: 0.9458 - val_loss: 0.5094 - val_acc: 0.8539\n",
      "Epoch 423/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2282 - acc: 0.9429Epoch 00422: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2277 - acc: 0.9431 - val_loss: 0.5095 - val_acc: 0.8551\n",
      "Epoch 424/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2258 - acc: 0.9442Epoch 00423: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2262 - acc: 0.9443 - val_loss: 0.5094 - val_acc: 0.8527\n",
      "Epoch 425/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2358 - acc: 0.9388Epoch 00424: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2360 - acc: 0.9389 - val_loss: 0.5099 - val_acc: 0.8515\n",
      "Epoch 426/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2277 - acc: 0.9459Epoch 00425: val_loss improved from 0.50936 to 0.50865, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2271 - acc: 0.9463 - val_loss: 0.5087 - val_acc: 0.8575\n",
      "Epoch 427/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2343 - acc: 0.9394Epoch 00426: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2343 - acc: 0.9394 - val_loss: 0.5092 - val_acc: 0.8527\n",
      "Epoch 428/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2248 - acc: 0.9441Epoch 00427: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2251 - acc: 0.9440 - val_loss: 0.5090 - val_acc: 0.8515\n",
      "Epoch 429/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2246 - acc: 0.9472Epoch 00428: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2246 - acc: 0.9473 - val_loss: 0.5089 - val_acc: 0.8515\n",
      "Epoch 430/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.2296 - acc: 0.9423Epoch 00429: val_loss improved from 0.50865 to 0.50808, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2298 - acc: 0.9425 - val_loss: 0.5081 - val_acc: 0.8563\n",
      "Epoch 431/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2240 - acc: 0.9462Epoch 00430: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2240 - acc: 0.9463 - val_loss: 0.5083 - val_acc: 0.8551\n",
      "Epoch 432/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2234 - acc: 0.9442Epoch 00431: val_loss improved from 0.50808 to 0.50750, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2229 - acc: 0.9445 - val_loss: 0.5075 - val_acc: 0.8527\n",
      "Epoch 433/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2271 - acc: 0.9436Epoch 00432: val_loss improved from 0.50750 to 0.50703, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2273 - acc: 0.9439 - val_loss: 0.5070 - val_acc: 0.8527\n",
      "Epoch 434/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2228 - acc: 0.9459Epoch 00433: val_loss improved from 0.50703 to 0.50671, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2237 - acc: 0.9457 - val_loss: 0.5067 - val_acc: 0.8539\n",
      "Epoch 435/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2198 - acc: 0.9436- ETA: 0s - loss: 0.2200Epoch 00434: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2194 - acc: 0.9436 - val_loss: 0.5067 - val_acc: 0.8539\n",
      "Epoch 436/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2214 - acc: 0.9456Epoch 00435: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2217 - acc: 0.9452 - val_loss: 0.5072 - val_acc: 0.8539\n",
      "Epoch 437/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.2186 - acc: 0.9462Epoch 00436: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2180 - acc: 0.9467 - val_loss: 0.5079 - val_acc: 0.8551\n",
      "Epoch 438/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2223 - acc: 0.9474Epoch 00437: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2222 - acc: 0.9478 - val_loss: 0.5079 - val_acc: 0.8551\n",
      "Epoch 439/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2199 - acc: 0.9417Epoch 00438: val_loss improved from 0.50671 to 0.50665, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2194 - acc: 0.9419 - val_loss: 0.5066 - val_acc: 0.8539\n",
      "Epoch 440/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.2232 - acc: 0.9435Epoch 00439: val_loss improved from 0.50665 to 0.50661, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2238 - acc: 0.9433 - val_loss: 0.5066 - val_acc: 0.8551\n",
      "Epoch 441/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.2154 - acc: 0.9498Epoch 00440: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2153 - acc: 0.9500 - val_loss: 0.5070 - val_acc: 0.8551\n",
      "Epoch 442/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.2158 - acc: 0.9489Epoch 00441: val_loss improved from 0.50661 to 0.50615, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2159 - acc: 0.9488 - val_loss: 0.5062 - val_acc: 0.8563\n",
      "Epoch 443/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2149 - acc: 0.9479Epoch 00442: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2159 - acc: 0.9475 - val_loss: 0.5070 - val_acc: 0.8575\n",
      "Epoch 444/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2208 - acc: 0.9452Epoch 00443: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2209 - acc: 0.9451 - val_loss: 0.5070 - val_acc: 0.8587\n",
      "Epoch 445/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2153 - acc: 0.9468Epoch 00444: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2150 - acc: 0.9469 - val_loss: 0.5070 - val_acc: 0.8587\n",
      "Epoch 446/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.2187 - acc: 0.9446Epoch 00445: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2190 - acc: 0.9443 - val_loss: 0.5072 - val_acc: 0.8587\n",
      "Epoch 447/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2149 - acc: 0.9455Epoch 00446: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2152 - acc: 0.9455 - val_loss: 0.5072 - val_acc: 0.8539\n",
      "Epoch 448/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.2110 - acc: 0.9512Epoch 00447: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2113 - acc: 0.9510 - val_loss: 0.5076 - val_acc: 0.8527\n",
      "Epoch 449/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2101 - acc: 0.9497Epoch 00448: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2091 - acc: 0.9500 - val_loss: 0.5069 - val_acc: 0.8527\n",
      "Epoch 450/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2111 - acc: 0.9496Epoch 00449: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2109 - acc: 0.9497 - val_loss: 0.5068 - val_acc: 0.8539\n",
      "Epoch 451/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.2077 - acc: 0.9491Epoch 00450: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2070 - acc: 0.9493 - val_loss: 0.5068 - val_acc: 0.8527\n",
      "Epoch 452/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2186 - acc: 0.9462Epoch 00451: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2187 - acc: 0.9463 - val_loss: 0.5066 - val_acc: 0.8527\n",
      "Epoch 453/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2083 - acc: 0.9507Epoch 00452: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2081 - acc: 0.9506 - val_loss: 0.5065 - val_acc: 0.8527\n",
      "Epoch 454/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.2151 - acc: 0.9443Epoch 00453: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2153 - acc: 0.9440 - val_loss: 0.5066 - val_acc: 0.8539\n",
      "Epoch 455/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.2132 - acc: 0.9472Epoch 00454: val_loss improved from 0.50615 to 0.50581, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2130 - acc: 0.9475 - val_loss: 0.5058 - val_acc: 0.8527\n",
      "Epoch 456/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.2126 - acc: 0.9455Epoch 00455: val_loss improved from 0.50581 to 0.50571, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2122 - acc: 0.9458 - val_loss: 0.5057 - val_acc: 0.8551\n",
      "Epoch 457/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.2123 - acc: 0.9456Epoch 00456: val_loss improved from 0.50571 to 0.50523, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2125 - acc: 0.9458 - val_loss: 0.5052 - val_acc: 0.8539\n",
      "Epoch 458/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2148 - acc: 0.9492Epoch 00457: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2148 - acc: 0.9491 - val_loss: 0.5055 - val_acc: 0.8515\n",
      "Epoch 459/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2158 - acc: 0.9498Epoch 00458: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2153 - acc: 0.9500 - val_loss: 0.5057 - val_acc: 0.8515\n",
      "Epoch 460/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2155 - acc: 0.9483Epoch 00459: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2150 - acc: 0.9485 - val_loss: 0.5065 - val_acc: 0.8527\n",
      "Epoch 461/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.2068 - acc: 0.9517Epoch 00460: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2076 - acc: 0.9512 - val_loss: 0.5060 - val_acc: 0.8539\n",
      "Epoch 462/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2099 - acc: 0.9484Epoch 00461: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2096 - acc: 0.9485 - val_loss: 0.5053 - val_acc: 0.8551\n",
      "Epoch 463/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2018 - acc: 0.9544Epoch 00462: val_loss improved from 0.50523 to 0.50485, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2019 - acc: 0.9543 - val_loss: 0.5049 - val_acc: 0.8539\n",
      "Epoch 464/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2077 - acc: 0.9511- EEpoch 00463: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2075 - acc: 0.9512 - val_loss: 0.5049 - val_acc: 0.8539\n",
      "Epoch 465/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2118 - acc: 0.9474Epoch 00464: val_loss improved from 0.50485 to 0.50430, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2110 - acc: 0.9479 - val_loss: 0.5043 - val_acc: 0.8527\n",
      "Epoch 466/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2127 - acc: 0.9485Epoch 00465: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2125 - acc: 0.9487 - val_loss: 0.5047 - val_acc: 0.8515\n",
      "Epoch 467/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2163 - acc: 0.9435Epoch 00466: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2159 - acc: 0.9436 - val_loss: 0.5043 - val_acc: 0.8527\n",
      "Epoch 468/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.2080 - acc: 0.9471Epoch 00467: val_loss improved from 0.50430 to 0.50376, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2091 - acc: 0.9472 - val_loss: 0.5038 - val_acc: 0.8563\n",
      "Epoch 469/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.2099 - acc: 0.9501Epoch 00468: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2093 - acc: 0.9504 - val_loss: 0.5047 - val_acc: 0.8551\n",
      "Epoch 470/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2076 - acc: 0.9480Epoch 00469: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2068 - acc: 0.9485 - val_loss: 0.5038 - val_acc: 0.8539\n",
      "Epoch 471/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.2018 - acc: 0.9548Epoch 00470: val_loss improved from 0.50376 to 0.50290, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2022 - acc: 0.9548 - val_loss: 0.5029 - val_acc: 0.8503\n",
      "Epoch 472/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2041 - acc: 0.9519Epoch 00471: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2039 - acc: 0.9518 - val_loss: 0.5031 - val_acc: 0.8539\n",
      "Epoch 473/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.2029 - acc: 0.9513Epoch 00472: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2035 - acc: 0.9512 - val_loss: 0.5032 - val_acc: 0.8539\n",
      "Epoch 474/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.2003 - acc: 0.9515Epoch 00473: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2009 - acc: 0.9513 - val_loss: 0.5035 - val_acc: 0.8563\n",
      "Epoch 475/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.2009 - acc: 0.9522Epoch 00474: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2008 - acc: 0.9522 - val_loss: 0.5033 - val_acc: 0.8551\n",
      "Epoch 476/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.2014 - acc: 0.9525Epoch 00475: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2017 - acc: 0.9524 - val_loss: 0.5031 - val_acc: 0.8563\n",
      "Epoch 477/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.2017 - acc: 0.9532Epoch 00476: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2013 - acc: 0.9530 - val_loss: 0.5031 - val_acc: 0.8575\n",
      "Epoch 478/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.1981 - acc: 0.9527Epoch 00477: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.1974 - acc: 0.9531 - val_loss: 0.5032 - val_acc: 0.8539\n",
      "Epoch 479/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2003 - acc: 0.9530Epoch 00478: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2008 - acc: 0.9524 - val_loss: 0.5032 - val_acc: 0.8563\n",
      "Epoch 480/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.1985 - acc: 0.9547Epoch 00479: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.1989 - acc: 0.9543 - val_loss: 0.5030 - val_acc: 0.8551\n",
      "Epoch 481/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2014 - acc: 0.9528Epoch 00480: val_loss improved from 0.50290 to 0.50229, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2016 - acc: 0.9530 - val_loss: 0.5023 - val_acc: 0.8575\n",
      "Epoch 482/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.2025 - acc: 0.9525Epoch 00481: val_loss improved from 0.50229 to 0.50188, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2022 - acc: 0.9530 - val_loss: 0.5019 - val_acc: 0.8551\n",
      "Epoch 483/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.2036 - acc: 0.9495Epoch 00482: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2032 - acc: 0.9499 - val_loss: 0.5022 - val_acc: 0.8551\n",
      "Epoch 484/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2053 - acc: 0.9502Epoch 00483: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2054 - acc: 0.9501 - val_loss: 0.5024 - val_acc: 0.8539\n",
      "Epoch 485/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.1975 - acc: 0.9545- ETEpoch 00484: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.1986 - acc: 0.9546 - val_loss: 0.5024 - val_acc: 0.8527\n",
      "Epoch 486/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.1960 - acc: 0.9543Epoch 00485: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.1945 - acc: 0.9548 - val_loss: 0.5019 - val_acc: 0.8563\n",
      "Epoch 487/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.1937 - acc: 0.9537Epoch 00486: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.1937 - acc: 0.9536 - val_loss: 0.5024 - val_acc: 0.8539\n",
      "Epoch 488/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.1954 - acc: 0.9538Epoch 00487: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.1959 - acc: 0.9534 - val_loss: 0.5023 - val_acc: 0.8575\n",
      "Epoch 489/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.1970 - acc: 0.9521Epoch 00488: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.1975 - acc: 0.9519 - val_loss: 0.5023 - val_acc: 0.8575\n",
      "Epoch 490/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2049 - acc: 0.9498Epoch 00489: val_loss improved from 0.50188 to 0.50144, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2049 - acc: 0.9499 - val_loss: 0.5014 - val_acc: 0.8563\n",
      "Epoch 491/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.1972 - acc: 0.9521Epoch 00490: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.1968 - acc: 0.9521 - val_loss: 0.5021 - val_acc: 0.8563\n",
      "Epoch 492/500\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 0.1965 - acc: 0.9512Epoch 00491: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.1970 - acc: 0.9510 - val_loss: 0.5016 - val_acc: 0.8527\n",
      "Epoch 493/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.1986 - acc: 0.9524Epoch 00492: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.1983 - acc: 0.9530 - val_loss: 0.5017 - val_acc: 0.8503\n",
      "Epoch 494/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.1939 - acc: 0.9536Epoch 00493: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.1947 - acc: 0.9533 - val_loss: 0.5016 - val_acc: 0.8527\n",
      "Epoch 495/500\n",
      "6550/6680 [============================>.] - ETA: 0s - loss: 0.1947 - acc: 0.9565Epoch 00494: val_loss improved from 0.50144 to 0.50139, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.1947 - acc: 0.9566 - val_loss: 0.5014 - val_acc: 0.8539\n",
      "Epoch 496/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.1969 - acc: 0.9535Epoch 00495: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.1967 - acc: 0.9537 - val_loss: 0.5021 - val_acc: 0.8539\n",
      "Epoch 497/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.1862 - acc: 0.9598Epoch 00496: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.1859 - acc: 0.9599 - val_loss: 0.5024 - val_acc: 0.8551\n",
      "Epoch 498/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.1927 - acc: 0.9534Epoch 00497: val_loss improved from 0.50139 to 0.50129, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.1924 - acc: 0.9536 - val_loss: 0.5013 - val_acc: 0.8539\n",
      "Epoch 499/500\n",
      "6650/6680 [============================>.] - ETA: 0s - loss: 0.2015 - acc: 0.9501Epoch 00498: val_loss improved from 0.50129 to 0.50108, saving model to saved_models/weights.best.Res.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.2011 - acc: 0.9501 - val_loss: 0.5011 - val_acc: 0.8563\n",
      "Epoch 500/500\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.1909 - acc: 0.9556Epoch 00499: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.1916 - acc: 0.9554 - val_loss: 0.5012 - val_acc: 0.8551\n"
     ]
    }
   ],
   "source": [
    "#######################################       RESNET50 Implementation w/500 epochs    ###########################################\n",
    "\n",
    "### TODO: Obtain bottleneck features from another pre-trained CNN.\n",
    "bottleneck_features = np.load('bottleneck_features/DogResnet50Data.npz')\n",
    "train_Res = bottleneck_features['train']\n",
    "valid_Res = bottleneck_features['valid']\n",
    "test_Res = bottleneck_features['test']\n",
    "\n",
    "### TODO: Define your architecture.\n",
    "Res_model = Sequential()\n",
    "Res_model.add(GlobalAveragePooling2D(input_shape=train_Res.shape[1:]))\n",
    "Res_model.add(Dense(512, activation='relu'))\n",
    "Res_model.add(Dropout(0.5))\n",
    "Res_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "Res_model.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=1e-4, momentum=0.9), metrics=['accuracy'])\n",
    "\n",
    " \n",
    "### TODO: Train the model.\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.Res.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "Res_model.fit(train_Res, train_targets, \n",
    "          validation_data=(valid_Res, valid_targets),\n",
    "          epochs=500, batch_size=50, callbacks=[checkpointer], verbose=1)\n",
    "### TODO: Load the model weights with the best validation loss.\n",
    "Res_model.load_weights('saved_models/weights.best.Res.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "nbpresent": {
     "id": "4461ff4b-ca00-439e-a84f-395416506b61"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 85.1675%\n"
     ]
    }
   ],
   "source": [
    "### TODO: Calculate classification accuracy on the test dataset.\n",
    "# get index of predicted dog breed for each image in test set\n",
    "Res_predictions = [np.argmax(Res_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Res]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(Res_predictions)==np.argmax(test_targets, axis=1))/len(Res_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "120cbf0d-37d8-4a5e-8386-bd223f3e21d0"
    }
   },
   "source": [
    "### (IMPLEMENTATION) Predict Dog Breed with the Model\n",
    "\n",
    "Write a function that takes an image path as input and returns the dog breed (`Affenpinscher`, `Afghan_hound`, etc) that is predicted by your model.  \n",
    "\n",
    "Similar to the analogous function in Step 5, your function should have three steps:\n",
    "1. Extract the bottleneck features corresponding to the chosen CNN model.\n",
    "2. Supply the bottleneck features as input to the model to return the predicted vector.  Note that the argmax of this prediction vector gives the index of the predicted dog breed.\n",
    "3. Use the `dog_names` array defined in Step 0 of this notebook to return the corresponding breed.\n",
    "\n",
    "The functions to extract the bottleneck features can be found in `extract_bottleneck_features.py`, and they have been imported in an earlier code cell.  To obtain the bottleneck features corresponding to your chosen CNN architecture, you need to use the function\n",
    "\n",
    "    extract_{network}\n",
    "    \n",
    "where `{network}`, in the above filename, should be one of `VGG19`, `Resnet50`, `InceptionV3`, or `Xception`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "40e9688d-1d8e-4ff1-b457-f1d03ae0b6ab"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### TODO: Write a function that takes a path to an image as input\n",
    "### and returns the dog breed that is predicted by the model.\n",
    "\n",
    "def Resnet50_predict_breed(img_path):\n",
    "    # extract bottleneck features\n",
    "    bottleneck_feature = extract_Resnet50(path_to_tensor(img_path))\n",
    "    # obtain predicted vector\n",
    "    predicted_vector = Res_model.predict(bottleneck_feature)\n",
    "    # return dog breed that is predicted by the model\n",
    "    return dog_names[np.argmax(predicted_vector)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c2a38cc2-2065-4183-9504-d3cac6741f6a"
    }
   },
   "source": [
    "---\n",
    "<a id='step6'></a>\n",
    "## Step 6: Write your Algorithm\n",
    "\n",
    "Write an algorithm that accepts a file path to an image and first determines whether the image contains a human, dog, or neither.  Then,\n",
    "- if a __dog__ is detected in the image, return the predicted breed.\n",
    "- if a __human__ is detected in the image, return the resembling dog breed.\n",
    "- if __neither__ is detected in the image, provide output that indicates an error.\n",
    "\n",
    "You are welcome to write your own functions for detecting humans and dogs in images, but feel free to use the `face_detector` and `dog_detector` functions developed above.  You are __required__ to use your CNN from Step 5 to predict dog breed.  \n",
    "\n",
    "Some sample output for our algorithm is provided below, but feel free to design your own user experience!\n",
    "\n",
    "![Sample Human Output](images/sample_human_output.png)\n",
    "\n",
    "\n",
    "### (IMPLEMENTATION) Write your Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "dc3e2803-27ed-4e49-922f-3438493b2309"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def dog_app(img_path):\n",
    "    if face_detector(img_path):\n",
    "        print(\"Hello, human!\")\n",
    "        print(\"You look like a ...\")\n",
    "        print(Resnet50_predict_breed(img_path))\n",
    "    elif dog_detector(img_path):\n",
    "        print(\"Hello, Dog!\")\n",
    "        print(\"It looks like you own a ...\")\n",
    "        print(Resnet50_predict_breed(img_path))\n",
    "    else:\n",
    "        print(\"Sorry, you need to use a picture of a human or a dog!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "09e8ed52-897f-471d-b71f-9f636c9d7359"
    }
   },
   "source": [
    "---\n",
    "<a id='step7'></a>\n",
    "## Step 7: Test Your Algorithm\n",
    "\n",
    "In this section, you will take your new algorithm for a spin!  What kind of dog does the algorithm think that __you__ look like?  If you have a dog, does it predict your dog's breed accurately?  If you have a cat, does it mistakenly think that your cat is a dog?\n",
    "\n",
    "### (IMPLEMENTATION) Test Your Algorithm on Sample Images!\n",
    "\n",
    "Test your algorithm at least six images on your computer.  Feel free to use any images you like.  Use at least two human and two dog images.  \n",
    "\n",
    "__Question 6:__ Is the output better than you expected :) ?  Or worse :( ?  Provide at least three possible points of improvement for your algorithm.\n",
    "\n",
    "__Answer:__\n",
    "\n",
    "Both humans were detected as humans and recieved a dog that they most resemble. One dog was detected as a human, but was properly labeled as a labrador. The second dog was detected as a dog and properly labeled as a German Shepard. The two cat images successfully displayed the error since they weren't detected as dogs or humans.\n",
    "\n",
    "The output was about what I exptected for the dogs. Since the final model I used had 85% accuracy, I assumed the two samples would be properly identified. I thought there was a chance the cats would get labeled as dogs, but that didn't happen. I don't know what to make of the output for the humans other than it would make for a funny app. \n",
    "\n",
    "Improvement areas could include: \n",
    "- Better detection, since a dog was labeled as a human.\n",
    "- In phone app form, the algorithm should accept images straight from a camera input.\n",
    "- Speed improvements would be nice as well, since it took several seconds to get a prediction.\n",
    "- A GUI that included a picture of the type of dog that the human resembles, right next to the human image would make it more entertaining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "30e14e33-bbde-4da7-a340-09c0547f81c1"
    }
   },
   "source": [
    "<img src='images/human1.jpg' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "nbpresent": {
     "id": "ff0504be-0abb-4dcc-974b-a3f252f76e20"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, human!\n",
      "You look like a ...\n",
      "Chihuahua\n"
     ]
    }
   ],
   "source": [
    "dog_app('images/human1.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "dd4378a9-eb98-4df1-bd2d-d1f5fce478b4"
    }
   },
   "source": [
    "<img src='images/human2.jpg' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "nbpresent": {
     "id": "2fe2e683-78cc-49d4-b244-7ee42cc93137"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, human!\n",
      "You look like a ...\n",
      "American_water_spaniel\n"
     ]
    }
   ],
   "source": [
    "dog_app('images/human2.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "54aa0105-588c-428b-8c36-2bfd59861215"
    }
   },
   "source": [
    "<img src='images/dog1.jpg' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "nbpresent": {
     "id": "d4e83688-3df4-4735-8d88-6241d909c7ab"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, human!\n",
      "You look like a ...\n",
      "Labrador_retriever\n"
     ]
    }
   ],
   "source": [
    "dog_app('images/dog1.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ef7200f0-ab61-4479-a21e-ecb6f611095f"
    }
   },
   "source": [
    "<img src='images/dog2.jpg' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "nbpresent": {
     "id": "28a1382b-f749-4fe9-8e0c-7157bc7eecd1"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Dog!\n",
      "It looks like you own a ...\n",
      "German_shepherd_dog\n"
     ]
    }
   ],
   "source": [
    "dog_app('images/dog2.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "47010ded-9dab-4203-834f-dccefd973bad"
    }
   },
   "source": [
    "<img src='images/cat1.JPG' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "nbpresent": {
     "id": "002b2684-6c10-4655-803b-2fdc3dd54029"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, you need to use a picture of a human or a dog!!!\n"
     ]
    }
   ],
   "source": [
    "dog_app('images/cat1.JPG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "da2c191c-1305-4d05-bba8-6df87ccda6b4"
    }
   },
   "source": [
    "<img src='images/cat2.JPG' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "nbpresent": {
     "id": "701d9f3f-3340-42ab-ad30-1467bb22e55a"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, you need to use a picture of a human or a dog!!!\n"
     ]
    }
   ],
   "source": [
    "dog_app('images/cat2.JPG')\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "nbpresent": {
   "slides": {
    "063b8d73-1d0d-42df-b5ae-2ad0d14adfa8": {
     "id": "063b8d73-1d0d-42df-b5ae-2ad0d14adfa8",
     "prev": "e1610ddc-7b05-42e5-9eda-c92324cf6767",
     "regions": {
      "f63bc5f9-66d1-44c5-af72-baa38a3947ce": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "8a856d1b-97f7-42a3-8f4b-fb1020d47ef0",
        "part": "whole"
       },
       "id": "f63bc5f9-66d1-44c5-af72-baa38a3947ce"
      }
     }
    },
    "09a60639-85e9-473f-9bd5-e5e733764c6b": {
     "id": "09a60639-85e9-473f-9bd5-e5e733764c6b",
     "prev": "bf06ac61-8173-4b21-a826-d94d7091bc2c",
     "regions": {
      "326f2cd9-239e-4752-b7ee-8e71c08e45a8": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "76dd4630-e672-44fc-b2c9-694de96229be",
        "part": "whole"
       },
       "id": "326f2cd9-239e-4752-b7ee-8e71c08e45a8"
      }
     }
    },
    "0a7d1789-7072-4bd4-b403-342be9e85977": {
     "id": "0a7d1789-7072-4bd4-b403-342be9e85977",
     "prev": "beae7539-ec0c-49e5-afd9-b46a6e8f5fe5",
     "regions": {
      "9af61cc3-897e-4590-b143-1cd45e7c7873": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "120cbf0d-37d8-4a5e-8386-bd223f3e21d0",
        "part": "whole"
       },
       "id": "9af61cc3-897e-4590-b143-1cd45e7c7873"
      }
     }
    },
    "0c022e7f-c6aa-4846-a272-c4ee94937001": {
     "id": "0c022e7f-c6aa-4846-a272-c4ee94937001",
     "prev": "e8fa51b7-4cb4-44b6-b3a2-7eccf1f3fdad",
     "regions": {
      "1aed19c7-8319-44c1-ac52-a3e64265fb86": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "d6ff3093-83d7-4596-a317-0eb378733877",
        "part": "whole"
       },
       "id": "1aed19c7-8319-44c1-ac52-a3e64265fb86"
      }
     }
    },
    "11b702f1-4923-4b26-9136-ed2dd3e6cfb6": {
     "id": "11b702f1-4923-4b26-9136-ed2dd3e6cfb6",
     "prev": "eff26920-54cb-45f3-9630-0e9783e72161",
     "regions": {
      "da585311-5785-435f-b360-d6553e4d15b3": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "ff0504be-0abb-4dcc-974b-a3f252f76e20",
        "part": "whole"
       },
       "id": "da585311-5785-435f-b360-d6553e4d15b3"
      }
     }
    },
    "148f527a-dc77-4f08-bca5-a7acc1232233": {
     "id": "148f527a-dc77-4f08-bca5-a7acc1232233",
     "prev": "11b702f1-4923-4b26-9136-ed2dd3e6cfb6",
     "regions": {
      "b7a7b868-87d0-4d50-81fe-2cf6383a3331": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "dd4378a9-eb98-4df1-bd2d-d1f5fce478b4",
        "part": "whole"
       },
       "id": "b7a7b868-87d0-4d50-81fe-2cf6383a3331"
      }
     }
    },
    "1648b69e-b184-48b7-ac54-ef09d63d1d6e": {
     "id": "1648b69e-b184-48b7-ac54-ef09d63d1d6e",
     "prev": "6a37c4b0-2126-4762-83eb-1d56e9c38b51",
     "regions": {
      "610185b9-c08f-4ed1-b4f2-6e24d58f8d6d": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "fe585b02-0648-4bed-9a10-d7fbd5c084ff",
        "part": "whole"
       },
       "id": "610185b9-c08f-4ed1-b4f2-6e24d58f8d6d"
      }
     }
    },
    "19103e1e-a12f-4f7a-a82f-0cccca34dd81": {
     "id": "19103e1e-a12f-4f7a-a82f-0cccca34dd81",
     "prev": "6692e094-aecf-4872-bfab-7553befcf406",
     "regions": {
      "6954f01a-488a-4322-b537-cea113eaef1a": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "bb35f6e3-d2c1-4e70-ba54-938ee9eaefe1",
        "part": "whole"
       },
       "id": "6954f01a-488a-4322-b537-cea113eaef1a"
      }
     }
    },
    "197b2088-9c5a-47d6-9864-297b2fa9d50a": {
     "id": "197b2088-9c5a-47d6-9864-297b2fa9d50a",
     "prev": "dc6001fe-4c26-4c6d-9f3b-b60b4481e130",
     "regions": {
      "77f62c73-be90-4093-a9ca-b7616625f94f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "8a6b0228-99a0-4c33-b0ce-535c5b56102d",
        "part": "whole"
       },
       "id": "77f62c73-be90-4093-a9ca-b7616625f94f"
      }
     }
    },
    "1caa9c5b-2f1c-4315-9ef3-2a051aef5c6b": {
     "id": "1caa9c5b-2f1c-4315-9ef3-2a051aef5c6b",
     "prev": "839b2a00-8ec0-482f-8513-9c274e15c779",
     "regions": {
      "2c045016-e595-4b7d-b238-9f88d2146800": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "40bfc0ac-b4b1-43ff-8b6f-c4094c24b437",
        "part": "whole"
       },
       "id": "2c045016-e595-4b7d-b238-9f88d2146800"
      }
     }
    },
    "2219e122-2e40-467c-8e42-2ae65fdea26a": {
     "id": "2219e122-2e40-467c-8e42-2ae65fdea26a",
     "prev": "2591f272-4e98-42b4-af15-40f7713be161",
     "regions": {
      "a3e62837-f34e-4c8f-90d4-8cc5a3959fc6": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "5a55d640-1bda-43cb-926c-3cfcd7c461c4",
        "part": "whole"
       },
       "id": "a3e62837-f34e-4c8f-90d4-8cc5a3959fc6"
      }
     }
    },
    "25518dfd-da10-47c8-bc29-0c62da3ee2b1": {
     "id": "25518dfd-da10-47c8-bc29-0c62da3ee2b1",
     "prev": "44fb29e3-71fb-49de-a457-119e6288749d",
     "regions": {
      "d4a2877d-fcaa-4e9e-9148-90969717cb36": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "efa4dcdf-25c5-4f39-b482-2b20c04bd396",
        "part": "whole"
       },
       "id": "d4a2877d-fcaa-4e9e-9148-90969717cb36"
      }
     }
    },
    "2591f272-4e98-42b4-af15-40f7713be161": {
     "id": "2591f272-4e98-42b4-af15-40f7713be161",
     "prev": "4ac338f9-32a4-43b5-93a3-2ba0e5fa8ffb",
     "regions": {
      "7a0b5ead-6108-4824-a5de-a7ec0b002e4f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "93e6e66b-1c4d-4078-b5b5-5f20c4a6d696",
        "part": "whole"
       },
       "id": "7a0b5ead-6108-4824-a5de-a7ec0b002e4f"
      }
     }
    },
    "2720fa78-e935-45df-9c7f-5731de106d38": {
     "id": "2720fa78-e935-45df-9c7f-5731de106d38",
     "prev": "b6778ccf-8225-48bd-a92a-7e6ca651c4a8",
     "regions": {
      "722ed3c6-c7e7-451f-8827-cd5c6ca2039e": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "002b2684-6c10-4655-803b-2fdc3dd54029",
        "part": "whole"
       },
       "id": "722ed3c6-c7e7-451f-8827-cd5c6ca2039e"
      }
     }
    },
    "2cd8f86e-84b6-465b-b029-cfa9dc318f35": {
     "id": "2cd8f86e-84b6-465b-b029-cfa9dc318f35",
     "prev": "1648b69e-b184-48b7-ac54-ef09d63d1d6e",
     "regions": {
      "76ab4ec6-1755-4361-9c38-a91566337e91": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "b134ae7c-d30b-413a-b5e9-8eb8fc5a5332",
        "part": "whole"
       },
       "id": "76ab4ec6-1755-4361-9c38-a91566337e91"
      }
     }
    },
    "2e46586f-a736-44d1-9064-3989edd2dea4": {
     "id": "2e46586f-a736-44d1-9064-3989edd2dea4",
     "prev": "7a0c0a54-5b11-44dc-bbf1-a7a92602b77e",
     "regions": {
      "c3afd09b-7748-466a-b7d0-4aa4ccf52c90": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "d1cab638-b77b-4d26-ac61-2d085f8b7d9b",
        "part": "whole"
       },
       "id": "c3afd09b-7748-466a-b7d0-4aa4ccf52c90"
      }
     }
    },
    "3436101c-f064-41af-9b7f-82577417a935": {
     "id": "3436101c-f064-41af-9b7f-82577417a935",
     "prev": "7e761591-dbb0-4fef-b047-5956e09dd416",
     "regions": {
      "fab3373f-5991-422b-a8af-786bcbf10247": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "5f2f0863-3365-4d0f-b188-bfb017d7c9c2",
        "part": "whole"
       },
       "id": "fab3373f-5991-422b-a8af-786bcbf10247"
      }
     }
    },
    "357bf8d4-6ab6-4114-b0e0-ccac6df23d37": {
     "id": "357bf8d4-6ab6-4114-b0e0-ccac6df23d37",
     "prev": "feb711ef-e8f0-443b-a9e0-2e6a794614d6",
     "regions": {
      "9c271a2e-06d3-4bc6-8fe1-056a71d08500": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "93e448c9-cba4-4132-8123-fcc29eecf5cd",
        "part": "whole"
       },
       "id": "9c271a2e-06d3-4bc6-8fe1-056a71d08500"
      }
     }
    },
    "3e3382c6-47ad-44de-96dd-6b73e92abb01": {
     "id": "3e3382c6-47ad-44de-96dd-6b73e92abb01",
     "prev": "2720fa78-e935-45df-9c7f-5731de106d38",
     "regions": {
      "29c618f2-5445-4713-9c84-ee60e0cb116c": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "da2c191c-1305-4d05-bba8-6df87ccda6b4",
        "part": "whole"
       },
       "id": "29c618f2-5445-4713-9c84-ee60e0cb116c"
      }
     }
    },
    "3e4b83b0-61ea-4010-8e15-41de2046165d": {
     "id": "3e4b83b0-61ea-4010-8e15-41de2046165d",
     "prev": null,
     "regions": {
      "56274a9b-f5d4-4537-a426-355c40396d9d": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "0b1d1371-e9f4-4dda-8dc6-e7882e362b20",
        "part": "whole"
       },
       "id": "56274a9b-f5d4-4537-a426-355c40396d9d"
      }
     }
    },
    "448ab863-dabc-4842-a841-c205d61d3c53": {
     "id": "448ab863-dabc-4842-a841-c205d61d3c53",
     "prev": "4aff62ca-c4a1-49cb-86a7-e690494ab9ed",
     "regions": {
      "f4e42a24-52eb-416f-8c40-a9061f2c9d02": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "4e23d757-aa9f-423f-98c7-c8183e23cda4",
        "part": "whole"
       },
       "id": "f4e42a24-52eb-416f-8c40-a9061f2c9d02"
      }
     }
    },
    "44fb29e3-71fb-49de-a457-119e6288749d": {
     "id": "44fb29e3-71fb-49de-a457-119e6288749d",
     "prev": "9b05a789-caf2-45f9-8dd9-fadd1f1a8363",
     "regions": {
      "5f52dcf3-bbc8-455b-a810-143db39e953d": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "285d10bf-cef5-4093-9e21-808a09f3fceb",
        "part": "whole"
       },
       "id": "5f52dcf3-bbc8-455b-a810-143db39e953d"
      }
     }
    },
    "46b475a3-ebce-4bfc-af38-b50c79f2f0a7": {
     "id": "46b475a3-ebce-4bfc-af38-b50c79f2f0a7",
     "prev": "197b2088-9c5a-47d6-9864-297b2fa9d50a",
     "regions": {
      "738329b4-b648-4588-8e3c-ea037e74b536": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "298d0910-21a9-4261-962c-ee93bb6ac592",
        "part": "whole"
       },
       "id": "738329b4-b648-4588-8e3c-ea037e74b536"
      }
     }
    },
    "4ac338f9-32a4-43b5-93a3-2ba0e5fa8ffb": {
     "id": "4ac338f9-32a4-43b5-93a3-2ba0e5fa8ffb",
     "prev": "e1e9e11e-c16d-4abb-afef-e2ba94c06575",
     "regions": {
      "8703ddd0-0a23-4075-b052-69746a257148": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "8ce78060-7823-4180-b1de-7480070928cc",
        "part": "whole"
       },
       "id": "8703ddd0-0a23-4075-b052-69746a257148"
      }
     }
    },
    "4aff62ca-c4a1-49cb-86a7-e690494ab9ed": {
     "id": "4aff62ca-c4a1-49cb-86a7-e690494ab9ed",
     "prev": "9a6d4202-0a09-4a03-a785-a1d959de9ef1",
     "regions": {
      "bd6abb23-fe35-49a5-a704-86b6c9998101": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "164067d0-394b-4d40-8d5e-029e6affe38c",
        "part": "whole"
       },
       "id": "bd6abb23-fe35-49a5-a704-86b6c9998101"
      }
     }
    },
    "4fc8716e-1884-4fcf-a782-a7b15bc6301f": {
     "id": "4fc8716e-1884-4fcf-a782-a7b15bc6301f",
     "prev": "69633901-628a-42f6-8300-423f4045f964",
     "regions": {
      "85520271-f265-4f3d-80fb-e725975adba0": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "a0e528df-c5d3-4f4f-ac21-22c6cd102e56",
        "part": "whole"
       },
       "id": "85520271-f265-4f3d-80fb-e725975adba0"
      }
     }
    },
    "5191e236-795a-4ca6-b892-0cff57b3cc4c": {
     "id": "5191e236-795a-4ca6-b892-0cff57b3cc4c",
     "prev": "357bf8d4-6ab6-4114-b0e0-ccac6df23d37",
     "regions": {
      "c368ea5b-a2be-4336-b934-73910906b0d6": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "1e1c5169-867f-4c11-b9cd-8257f4dc3d5a",
        "part": "whole"
       },
       "id": "c368ea5b-a2be-4336-b934-73910906b0d6"
      }
     }
    },
    "5292315b-2d4d-465e-b840-bb1dc7e261c9": {
     "id": "5292315b-2d4d-465e-b840-bb1dc7e261c9",
     "prev": "8f4bf026-eea7-4a6e-8654-01b23c341653",
     "regions": {
      "d0617aca-a190-48aa-9c60-f994e1d837be": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "b0ec57f9-5d06-4e12-b598-3366a1bd4a53",
        "part": "whole"
       },
       "id": "d0617aca-a190-48aa-9c60-f994e1d837be"
      }
     }
    },
    "57f16d19-fa26-463c-800f-49fef5339b9f": {
     "id": "57f16d19-fa26-463c-800f-49fef5339b9f",
     "prev": "714f93cb-c025-42f4-afb9-0178ad65baa6",
     "regions": {
      "63fc4232-3be8-4e32-a993-d646c360d859": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "95ca45c9-4659-4be3-9fc7-a674b91ae7db",
        "part": "whole"
       },
       "id": "63fc4232-3be8-4e32-a993-d646c360d859"
      }
     }
    },
    "668b2555-e18f-489f-a6d1-26d19de44dc8": {
     "id": "668b2555-e18f-489f-a6d1-26d19de44dc8",
     "prev": "f8b3f977-9620-45b6-b0a2-b6d7d443441f",
     "regions": {
      "088f47dd-ee38-430d-9102-59deb6974861": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "9e2f1553-8581-4fac-9402-ce0510458c7d",
        "part": "whole"
       },
       "id": "088f47dd-ee38-430d-9102-59deb6974861"
      }
     }
    },
    "6692e094-aecf-4872-bfab-7553befcf406": {
     "id": "6692e094-aecf-4872-bfab-7553befcf406",
     "prev": "d90a49e5-f19b-44da-a475-9937bbfd6809",
     "regions": {
      "50523eb4-0a57-4115-862c-b7f83421aa39": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "fe9034c9-227f-4ec2-8e1a-2201ed102401",
        "part": "whole"
       },
       "id": "50523eb4-0a57-4115-862c-b7f83421aa39"
      }
     }
    },
    "69199d30-5e47-4438-bce5-4fbd1c77314f": {
     "id": "69199d30-5e47-4438-bce5-4fbd1c77314f",
     "prev": "063b8d73-1d0d-42df-b5ae-2ad0d14adfa8",
     "regions": {
      "2d214a84-758b-41ae-ba73-62971465b71d": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "6a5d04a8-c85c-4573-8da8-1671348f5182",
        "part": "whole"
       },
       "id": "2d214a84-758b-41ae-ba73-62971465b71d"
      }
     }
    },
    "69633901-628a-42f6-8300-423f4045f964": {
     "id": "69633901-628a-42f6-8300-423f4045f964",
     "prev": "9405fa02-109c-4fbf-9608-94788d39ffdc",
     "regions": {
      "c6832faf-fbf1-4379-b00c-d01a2c2536be": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "617cc09a-6a6f-4746-8e28-d9c627135a64",
        "part": "whole"
       },
       "id": "c6832faf-fbf1-4379-b00c-d01a2c2536be"
      }
     }
    },
    "6a37c4b0-2126-4762-83eb-1d56e9c38b51": {
     "id": "6a37c4b0-2126-4762-83eb-1d56e9c38b51",
     "prev": "19103e1e-a12f-4f7a-a82f-0cccca34dd81",
     "regions": {
      "133ffe54-22d2-41bc-ac30-a0397d1ce29b": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "bbc8a30c-aa10-4579-973f-f991c9014dc4",
        "part": "whole"
       },
       "id": "133ffe54-22d2-41bc-ac30-a0397d1ce29b"
      }
     }
    },
    "6ae72de6-791a-4f62-aa15-0b89ccebf1f2": {
     "id": "6ae72de6-791a-4f62-aa15-0b89ccebf1f2",
     "prev": "e48dacd6-27a1-4b4a-9e07-32e3c3c23d4d",
     "regions": {
      "c2638ccd-3c08-4714-9d5d-91d3aa735ea0": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "9d7fcb7a-fc73-48e1-825a-ebd5af3ed167",
        "part": "whole"
       },
       "id": "c2638ccd-3c08-4714-9d5d-91d3aa735ea0"
      }
     }
    },
    "714f93cb-c025-42f4-afb9-0178ad65baa6": {
     "id": "714f93cb-c025-42f4-afb9-0178ad65baa6",
     "prev": "6ae72de6-791a-4f62-aa15-0b89ccebf1f2",
     "regions": {
      "eeaa7ffa-8699-4892-9c08-138f5c10752b": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "197df124-a496-4dc9-a10b-80de67d20ded",
        "part": "whole"
       },
       "id": "eeaa7ffa-8699-4892-9c08-138f5c10752b"
      }
     }
    },
    "7733b30c-61d7-45e2-8bce-cfa6f3b02e6d": {
     "id": "7733b30c-61d7-45e2-8bce-cfa6f3b02e6d",
     "prev": "3e3382c6-47ad-44de-96dd-6b73e92abb01",
     "regions": {
      "a5e0da9c-f760-4b0a-a386-37d9fb6088bf": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "701d9f3f-3340-42ab-ad30-1467bb22e55a",
        "part": "whole"
       },
       "id": "a5e0da9c-f760-4b0a-a386-37d9fb6088bf"
      }
     }
    },
    "7a0c0a54-5b11-44dc-bbf1-a7a92602b77e": {
     "id": "7a0c0a54-5b11-44dc-bbf1-a7a92602b77e",
     "prev": "09a60639-85e9-473f-9bd5-e5e733764c6b",
     "regions": {
      "6fa05236-12c8-4350-bbcc-7181813f6c7a": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "e3421c6c-f71c-4a21-9c46-2e1485e97d36",
        "part": "whole"
       },
       "id": "6fa05236-12c8-4350-bbcc-7181813f6c7a"
      }
     }
    },
    "7d9bc2d6-b5c2-465c-8f45-9fabce6b6a9f": {
     "id": "7d9bc2d6-b5c2-465c-8f45-9fabce6b6a9f",
     "prev": "2e46586f-a736-44d1-9064-3989edd2dea4",
     "regions": {
      "4f5c10d7-40f6-4671-90d4-7302936517f6": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "96880e8c-d72e-49f9-8cde-132a57731919",
        "part": "whole"
       },
       "id": "4f5c10d7-40f6-4671-90d4-7302936517f6"
      }
     }
    },
    "7e761591-dbb0-4fef-b047-5956e09dd416": {
     "id": "7e761591-dbb0-4fef-b047-5956e09dd416",
     "prev": "3e4b83b0-61ea-4010-8e15-41de2046165d",
     "regions": {
      "cf15c1d3-084f-4df0-857f-96535e77f6db": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "be414f28-6d27-4e52-8786-1d0d51575532",
        "part": "whole"
       },
       "id": "cf15c1d3-084f-4df0-857f-96535e77f6db"
      }
     }
    },
    "839b2a00-8ec0-482f-8513-9c274e15c779": {
     "id": "839b2a00-8ec0-482f-8513-9c274e15c779",
     "prev": "b29ad2cc-cde8-40e8-82e2-beb583db0092",
     "regions": {
      "a2687f8c-fbbb-4a6c-86c7-60be0b8172bd": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "14326f0d-2f7f-4ca0-9609-37d3d1311666",
        "part": "whole"
       },
       "id": "a2687f8c-fbbb-4a6c-86c7-60be0b8172bd"
      }
     }
    },
    "84f8be36-5679-49aa-b20f-3886f24803f6": {
     "id": "84f8be36-5679-49aa-b20f-3886f24803f6",
     "prev": "fdc4d227-082b-4cb0-8e44-c47ffb8af9ac",
     "regions": {
      "dfc5c841-d6b9-44ee-8e89-4f330d9806e1": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "4dcd32d1-22c0-4c4c-a175-18dbd73de3ca",
        "part": "whole"
       },
       "id": "dfc5c841-d6b9-44ee-8e89-4f330d9806e1"
      }
     }
    },
    "8a05beb4-584d-4396-a559-3f89e29e7ef7": {
     "id": "8a05beb4-584d-4396-a559-3f89e29e7ef7",
     "prev": "5292315b-2d4d-465e-b840-bb1dc7e261c9",
     "regions": {
      "8c1d9fe2-09f5-4630-972e-01e0b113b836": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "11f53b71-e368-45e5-a900-fd3eb2e13587",
        "part": "whole"
       },
       "id": "8c1d9fe2-09f5-4630-972e-01e0b113b836"
      }
     }
    },
    "8f4bf026-eea7-4a6e-8654-01b23c341653": {
     "id": "8f4bf026-eea7-4a6e-8654-01b23c341653",
     "prev": "c9d67d68-6d15-4640-b06f-02e7f8e607af",
     "regions": {
      "06a49088-3735-4775-bf80-1317a9a89e6c": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "2f21f4bf-ca21-41b5-b2e8-177d2c85a319",
        "part": "whole"
       },
       "id": "06a49088-3735-4775-bf80-1317a9a89e6c"
      }
     }
    },
    "90278a6d-d99b-4911-be66-193ff8f98a3c": {
     "id": "90278a6d-d99b-4911-be66-193ff8f98a3c",
     "prev": "148f527a-dc77-4f08-bca5-a7acc1232233",
     "regions": {
      "d86db07a-89d3-4a01-aabf-84f647639724": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "2fe2e683-78cc-49d4-b244-7ee42cc93137",
        "part": "whole"
       },
       "id": "d86db07a-89d3-4a01-aabf-84f647639724"
      }
     }
    },
    "93542281-2e00-40c5-bce0-37a24bd4dae5": {
     "id": "93542281-2e00-40c5-bce0-37a24bd4dae5",
     "prev": "0c022e7f-c6aa-4846-a272-c4ee94937001",
     "regions": {
      "dfbd5f68-90ba-4cab-80b4-cc33a601e3c2": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "68a866d5-b1dd-4fd1-a515-1fcf15c15172",
        "part": "whole"
       },
       "id": "dfbd5f68-90ba-4cab-80b4-cc33a601e3c2"
      }
     }
    },
    "9405fa02-109c-4fbf-9608-94788d39ffdc": {
     "id": "9405fa02-109c-4fbf-9608-94788d39ffdc",
     "prev": "97a52e63-b3cc-4a71-acb4-5a8d75116e2d",
     "regions": {
      "c4c6c836-2b13-4c6b-a964-5a6f97fab453": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "1d337b93-57f5-498c-8ba2-fc350be7eae8",
        "part": "whole"
       },
       "id": "c4c6c836-2b13-4c6b-a964-5a6f97fab453"
      }
     }
    },
    "97a52e63-b3cc-4a71-acb4-5a8d75116e2d": {
     "id": "97a52e63-b3cc-4a71-acb4-5a8d75116e2d",
     "prev": "46b475a3-ebce-4bfc-af38-b50c79f2f0a7",
     "regions": {
      "7cffe9a8-dc26-42b5-a06c-2ec0a7a64b08": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "6a23eac4-242b-4e8e-9d94-ef7fe32824cb",
        "part": "whole"
       },
       "id": "7cffe9a8-dc26-42b5-a06c-2ec0a7a64b08"
      }
     }
    },
    "9862dc19-2cc6-48f4-a681-4a4b4ae9ed11": {
     "id": "9862dc19-2cc6-48f4-a681-4a4b4ae9ed11",
     "prev": "d91b56f1-55b2-4274-a34c-a65421d96a1e",
     "regions": {
      "cf857511-74b4-4b93-bfb6-f0fbd926054f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "9785470a-3226-4b28-bb31-b408e246f471",
        "part": "whole"
       },
       "id": "cf857511-74b4-4b93-bfb6-f0fbd926054f"
      }
     }
    },
    "9a6d4202-0a09-4a03-a785-a1d959de9ef1": {
     "id": "9a6d4202-0a09-4a03-a785-a1d959de9ef1",
     "prev": "5191e236-795a-4ca6-b892-0cff57b3cc4c",
     "regions": {
      "db70b143-fce3-4042-b5ae-e1240b2711a3": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "79ac0fae-69ea-4386-a3a3-2f5a104fe452",
        "part": "whole"
       },
       "id": "db70b143-fce3-4042-b5ae-e1240b2711a3"
      }
     }
    },
    "9b05a789-caf2-45f9-8dd9-fadd1f1a8363": {
     "id": "9b05a789-caf2-45f9-8dd9-fadd1f1a8363",
     "prev": "e9a6515c-4e8d-4168-a2f9-146a08d9a904",
     "regions": {
      "46e1897d-20f4-47a8-9b6f-ccb531d03c4d": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "9e57ec93-eaa0-4f5a-803f-fb2c3cc34496",
        "part": "whole"
       },
       "id": "46e1897d-20f4-47a8-9b6f-ccb531d03c4d"
      }
     }
    },
    "9b9c32a5-30ad-430c-a5da-1cbef4016c80": {
     "id": "9b9c32a5-30ad-430c-a5da-1cbef4016c80",
     "prev": "668b2555-e18f-489f-a6d1-26d19de44dc8",
     "regions": {
      "c2a56d62-751b-44c3-89e1-996483329568": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "fd8fd949-ab08-4b19-83e4-5058d6494b44",
        "part": "whole"
       },
       "id": "c2a56d62-751b-44c3-89e1-996483329568"
      }
     }
    },
    "9d78ebf8-7429-4ab1-8d3c-da04414ee0ab": {
     "id": "9d78ebf8-7429-4ab1-8d3c-da04414ee0ab",
     "prev": "efe362d0-32de-4432-b127-df8520fc340a",
     "regions": {
      "fecd2937-2d26-49f1-a9a2-bd97b4fb3983": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "d4e83688-3df4-4735-8d88-6241d909c7ab",
        "part": "whole"
       },
       "id": "fecd2937-2d26-49f1-a9a2-bd97b4fb3983"
      }
     }
    },
    "a0c01e2b-ee56-4233-96b3-9f7875c3659d": {
     "id": "a0c01e2b-ee56-4233-96b3-9f7875c3659d",
     "prev": "448ab863-dabc-4842-a841-c205d61d3c53",
     "regions": {
      "523ab98f-d87e-4962-ab36-696080cad4d7": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "6e6e0bc3-bfbd-4612-92c5-d0a931482eda",
        "part": "whole"
       },
       "id": "523ab98f-d87e-4962-ab36-696080cad4d7"
      }
     }
    },
    "ac166adc-2f50-49e6-b35b-b685536b98c9": {
     "id": "ac166adc-2f50-49e6-b35b-b685536b98c9",
     "prev": "25518dfd-da10-47c8-bc29-0c62da3ee2b1",
     "regions": {
      "8e7f0b71-a5b8-46c7-a348-7b9f9bfdc9fe": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "69c0df3d-d0f3-4020-8c67-40a24bd8e97b",
        "part": "whole"
       },
       "id": "8e7f0b71-a5b8-46c7-a348-7b9f9bfdc9fe"
      }
     }
    },
    "acf9b3ee-832a-4af4-a4c1-b23114227d8e": {
     "id": "acf9b3ee-832a-4af4-a4c1-b23114227d8e",
     "prev": "9d78ebf8-7429-4ab1-8d3c-da04414ee0ab",
     "regions": {
      "58863da5-b8c7-46ec-adb0-e40085b078db": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "ef7200f0-ab61-4479-a21e-ecb6f611095f",
        "part": "whole"
       },
       "id": "58863da5-b8c7-46ec-adb0-e40085b078db"
      }
     }
    },
    "b29ad2cc-cde8-40e8-82e2-beb583db0092": {
     "id": "b29ad2cc-cde8-40e8-82e2-beb583db0092",
     "prev": "8a05beb4-584d-4396-a559-3f89e29e7ef7",
     "regions": {
      "f14b59d7-cac8-4211-80d1-fb442f44f860": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "b17f501c-85c8-48c1-8bb1-01673ef453de",
        "part": "whole"
       },
       "id": "f14b59d7-cac8-4211-80d1-fb442f44f860"
      }
     }
    },
    "b6778ccf-8225-48bd-a92a-7e6ca651c4a8": {
     "id": "b6778ccf-8225-48bd-a92a-7e6ca651c4a8",
     "prev": "be4244dd-e838-4af5-b51f-f5d6169fdb59",
     "regions": {
      "e9632125-ba6e-459e-9825-ae2f0ccd5e63": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "47010ded-9dab-4203-834f-dccefd973bad",
        "part": "whole"
       },
       "id": "e9632125-ba6e-459e-9825-ae2f0ccd5e63"
      }
     }
    },
    "b963492a-8022-4c4d-adaa-12f95ac91652": {
     "id": "b963492a-8022-4c4d-adaa-12f95ac91652",
     "prev": "0a7d1789-7072-4bd4-b403-342be9e85977",
     "regions": {
      "5f5d4758-1b83-4293-8bf1-ace0fe2be254": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "40e9688d-1d8e-4ff1-b457-f1d03ae0b6ab",
        "part": "whole"
       },
       "id": "5f5d4758-1b83-4293-8bf1-ace0fe2be254"
      }
     }
    },
    "be4244dd-e838-4af5-b51f-f5d6169fdb59": {
     "id": "be4244dd-e838-4af5-b51f-f5d6169fdb59",
     "prev": "acf9b3ee-832a-4af4-a4c1-b23114227d8e",
     "regions": {
      "6c95f61b-28de-4f70-a049-ace6e57fcda2": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "28a1382b-f749-4fe9-8e0c-7157bc7eecd1",
        "part": "whole"
       },
       "id": "6c95f61b-28de-4f70-a049-ace6e57fcda2"
      }
     }
    },
    "be4c524a-2108-4b58-ae89-867353e96fe2": {
     "id": "be4c524a-2108-4b58-ae89-867353e96fe2",
     "prev": "e9e9510e-8b44-4e01-8066-89cfa6e9970b",
     "regions": {
      "b0c5ad6c-0070-49b2-bf5c-cef76c035f24": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "09e8ed52-897f-471d-b71f-9f636c9d7359",
        "part": "whole"
       },
       "id": "b0c5ad6c-0070-49b2-bf5c-cef76c035f24"
      }
     }
    },
    "beae7539-ec0c-49e5-afd9-b46a6e8f5fe5": {
     "id": "beae7539-ec0c-49e5-afd9-b46a6e8f5fe5",
     "prev": "a0c01e2b-ee56-4233-96b3-9f7875c3659d",
     "regions": {
      "84f18edf-9ffa-4759-98d8-bd147eee837f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "4461ff4b-ca00-439e-a84f-395416506b61",
        "part": "whole"
       },
       "id": "84f18edf-9ffa-4759-98d8-bd147eee837f"
      }
     }
    },
    "bf06ac61-8173-4b21-a826-d94d7091bc2c": {
     "id": "bf06ac61-8173-4b21-a826-d94d7091bc2c",
     "prev": "c9d06323-1802-4c59-9c24-1483d8b4ead6",
     "regions": {
      "d84bf8c7-6f30-42f5-92cc-af1ebc84ebe2": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "5aded55d-a7be-4955-9622-3a8c84f13340",
        "part": "whole"
       },
       "id": "d84bf8c7-6f30-42f5-92cc-af1ebc84ebe2"
      }
     }
    },
    "c9d06323-1802-4c59-9c24-1483d8b4ead6": {
     "id": "c9d06323-1802-4c59-9c24-1483d8b4ead6",
     "prev": "1caa9c5b-2f1c-4315-9ef3-2a051aef5c6b",
     "regions": {
      "38b005d5-f84e-4e47-a1a2-5ba337173017": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "c1ae29c7-3b8b-47e1-b6cc-b3855b660fc0",
        "part": "whole"
       },
       "id": "38b005d5-f84e-4e47-a1a2-5ba337173017"
      }
     }
    },
    "c9d67d68-6d15-4640-b06f-02e7f8e607af": {
     "id": "c9d67d68-6d15-4640-b06f-02e7f8e607af",
     "prev": "69199d30-5e47-4438-bce5-4fbd1c77314f",
     "regions": {
      "98e4f34c-b486-4f53-8739-e387d6b839e0": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "0d5079c3-d8b4-47c0-86ea-2f30433c2556",
        "part": "whole"
       },
       "id": "98e4f34c-b486-4f53-8739-e387d6b839e0"
      }
     }
    },
    "cd36e68d-d76e-4101-92d0-3b83268e8917": {
     "id": "cd36e68d-d76e-4101-92d0-3b83268e8917",
     "prev": "2219e122-2e40-467c-8e42-2ae65fdea26a",
     "regions": {
      "a660ba3e-724c-4549-a238-95c62f98b968": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "4a10d074-285c-41c8-9878-4b0ddba05d03",
        "part": "whole"
       },
       "id": "a660ba3e-724c-4549-a238-95c62f98b968"
      }
     }
    },
    "d90a49e5-f19b-44da-a475-9937bbfd6809": {
     "id": "d90a49e5-f19b-44da-a475-9937bbfd6809",
     "prev": "ac166adc-2f50-49e6-b35b-b685536b98c9",
     "regions": {
      "79e64f1a-1dae-4003-9267-b0b437008acf": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "eb16b6c2-d84d-4b5c-b7fa-1fb637df84d9",
        "part": "whole"
       },
       "id": "79e64f1a-1dae-4003-9267-b0b437008acf"
      }
     }
    },
    "d91b56f1-55b2-4274-a34c-a65421d96a1e": {
     "id": "d91b56f1-55b2-4274-a34c-a65421d96a1e",
     "prev": "9b9c32a5-30ad-430c-a5da-1cbef4016c80",
     "regions": {
      "61c4867b-579f-4110-a8ee-b3337fe4e755": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "82321456-bdf7-429f-87a5-80bbb875ed4b",
        "part": "whole"
       },
       "id": "61c4867b-579f-4110-a8ee-b3337fe4e755"
      }
     }
    },
    "dc6001fe-4c26-4c6d-9f3b-b60b4481e130": {
     "id": "dc6001fe-4c26-4c6d-9f3b-b60b4481e130",
     "prev": "7d9bc2d6-b5c2-465c-8f45-9fabce6b6a9f",
     "regions": {
      "a52909bf-40ce-4be2-b463-6f34fad739cb": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "5b6391e4-183c-4e6f-b556-073ea09dadf7",
        "part": "whole"
       },
       "id": "a52909bf-40ce-4be2-b463-6f34fad739cb"
      }
     }
    },
    "dd3c2564-19df-40f5-806f-6a995ebef5b1": {
     "id": "dd3c2564-19df-40f5-806f-6a995ebef5b1",
     "prev": "b963492a-8022-4c4d-adaa-12f95ac91652",
     "regions": {
      "737d16dc-b1bc-4efb-8a30-89fb404b55d3": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "c2a38cc2-2065-4183-9504-d3cac6741f6a",
        "part": "whole"
       },
       "id": "737d16dc-b1bc-4efb-8a30-89fb404b55d3"
      }
     }
    },
    "e1610ddc-7b05-42e5-9eda-c92324cf6767": {
     "id": "e1610ddc-7b05-42e5-9eda-c92324cf6767",
     "prev": "57f16d19-fa26-463c-800f-49fef5339b9f",
     "regions": {
      "f50df8f0-45e2-4130-be20-b104be33a43d": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "a7e03bb9-f910-403a-89be-ccb580273907",
        "part": "whole"
       },
       "id": "f50df8f0-45e2-4130-be20-b104be33a43d"
      }
     }
    },
    "e1e9e11e-c16d-4abb-afef-e2ba94c06575": {
     "id": "e1e9e11e-c16d-4abb-afef-e2ba94c06575",
     "prev": "93542281-2e00-40c5-bce0-37a24bd4dae5",
     "regions": {
      "8ee36d58-0a85-446a-bd35-6bf1e06ca909": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f9d05385-5750-493f-a8a6-676a2a2a29f5",
        "part": "whole"
       },
       "id": "8ee36d58-0a85-446a-bd35-6bf1e06ca909"
      }
     }
    },
    "e3c1960e-4d9c-43ca-bfee-7e5748e6ef51": {
     "id": "e3c1960e-4d9c-43ca-bfee-7e5748e6ef51",
     "prev": "84f8be36-5679-49aa-b20f-3886f24803f6",
     "regions": {
      "5ac15aa1-ef15-4674-a428-554fcb75eb65": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "6369770f-aac6-4921-80e6-947191449268",
        "part": "whole"
       },
       "id": "5ac15aa1-ef15-4674-a428-554fcb75eb65"
      }
     }
    },
    "e48dacd6-27a1-4b4a-9e07-32e3c3c23d4d": {
     "id": "e48dacd6-27a1-4b4a-9e07-32e3c3c23d4d",
     "prev": "9862dc19-2cc6-48f4-a681-4a4b4ae9ed11",
     "regions": {
      "ae4ac55f-0370-4faf-9c4e-87eaa1863e4f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "922e2623-1236-4d28-8bdc-df63b756ab9a",
        "part": "whole"
       },
       "id": "ae4ac55f-0370-4faf-9c4e-87eaa1863e4f"
      }
     }
    },
    "e8fa51b7-4cb4-44b6-b3a2-7eccf1f3fdad": {
     "id": "e8fa51b7-4cb4-44b6-b3a2-7eccf1f3fdad",
     "prev": "e3c1960e-4d9c-43ca-bfee-7e5748e6ef51",
     "regions": {
      "42e8c3b7-ee1b-45a2-a727-b949bbeadbc3": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "c792a73e-81cc-475e-be24-8f429a75b5b3",
        "part": "whole"
       },
       "id": "42e8c3b7-ee1b-45a2-a727-b949bbeadbc3"
      }
     }
    },
    "e9a6515c-4e8d-4168-a2f9-146a08d9a904": {
     "id": "e9a6515c-4e8d-4168-a2f9-146a08d9a904",
     "prev": "cd36e68d-d76e-4101-92d0-3b83268e8917",
     "regions": {
      "97923c44-8a57-4b0d-a84e-128d5762e1da": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "29c791f6-bf10-4a84-9230-8e73ce1e5d56",
        "part": "whole"
       },
       "id": "97923c44-8a57-4b0d-a84e-128d5762e1da"
      }
     }
    },
    "e9e9510e-8b44-4e01-8066-89cfa6e9970b": {
     "id": "e9e9510e-8b44-4e01-8066-89cfa6e9970b",
     "prev": "dd3c2564-19df-40f5-806f-6a995ebef5b1",
     "regions": {
      "1df3afcd-7fc7-4cfe-8830-76872b65f107": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "dc3e2803-27ed-4e49-922f-3438493b2309",
        "part": "whole"
       },
       "id": "1df3afcd-7fc7-4cfe-8830-76872b65f107"
      }
     }
    },
    "ed0049a0-1c3f-47f2-a301-442a8acb765e": {
     "id": "ed0049a0-1c3f-47f2-a301-442a8acb765e",
     "prev": "2cd8f86e-84b6-465b-b029-cfa9dc318f35",
     "regions": {
      "866d9128-d338-4ae9-80bc-f316155fbc7a": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "1c02170f-4661-4741-98fb-2e73fae3c15e",
        "part": "whole"
       },
       "id": "866d9128-d338-4ae9-80bc-f316155fbc7a"
      }
     }
    },
    "efe362d0-32de-4432-b127-df8520fc340a": {
     "id": "efe362d0-32de-4432-b127-df8520fc340a",
     "prev": "90278a6d-d99b-4911-be66-193ff8f98a3c",
     "regions": {
      "3dc71ee4-ec95-4a5b-9cff-c664faf7e294": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "54aa0105-588c-428b-8c36-2bfd59861215",
        "part": "whole"
       },
       "id": "3dc71ee4-ec95-4a5b-9cff-c664faf7e294"
      }
     }
    },
    "eff26920-54cb-45f3-9630-0e9783e72161": {
     "id": "eff26920-54cb-45f3-9630-0e9783e72161",
     "prev": "be4c524a-2108-4b58-ae89-867353e96fe2",
     "regions": {
      "e912e061-9b3b-4805-a288-573b16a790ea": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "30e14e33-bbde-4da7-a340-09c0547f81c1",
        "part": "whole"
       },
       "id": "e912e061-9b3b-4805-a288-573b16a790ea"
      }
     }
    },
    "f8b3f977-9620-45b6-b0a2-b6d7d443441f": {
     "id": "f8b3f977-9620-45b6-b0a2-b6d7d443441f",
     "prev": "3436101c-f064-41af-9b7f-82577417a935",
     "regions": {
      "c68eb1db-2d31-46cb-b8a0-ab309fc44a66": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "aa49ee33-9985-43e4-b7bd-e502ac28ab64",
        "part": "whole"
       },
       "id": "c68eb1db-2d31-46cb-b8a0-ab309fc44a66"
      }
     }
    },
    "fdc4d227-082b-4cb0-8e44-c47ffb8af9ac": {
     "id": "fdc4d227-082b-4cb0-8e44-c47ffb8af9ac",
     "prev": "4fc8716e-1884-4fcf-a782-a7b15bc6301f",
     "regions": {
      "ce4253cd-4151-4450-a25e-afa1e77bd9b2": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "c71e9ce4-4a39-4979-b695-dfbcf05e465a",
        "part": "whole"
       },
       "id": "ce4253cd-4151-4450-a25e-afa1e77bd9b2"
      }
     }
    },
    "feb711ef-e8f0-443b-a9e0-2e6a794614d6": {
     "id": "feb711ef-e8f0-443b-a9e0-2e6a794614d6",
     "prev": "ed0049a0-1c3f-47f2-a301-442a8acb765e",
     "regions": {
      "844e293f-de04-4f01-b9a9-e767f78d3294": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "82f89478-4ac7-4e36-bdcf-4190c8ddcfc9",
        "part": "whole"
       },
       "id": "844e293f-de04-4f01-b9a9-e767f78d3294"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
